# 三、评估和验证

为了拥有可持续的、负责任的机器学习工作流，并开发能够产生真正价值的机器学习应用程序，我们需要能够衡量机器学习模型的性能。我们还需要确保我们的机器学习模型能够推广到他们将在生产中看到的数据。如果我们不做这些事情，我们基本上是在黑暗中拍摄。我们将不了解模型的预期行为，并且随着时间的推移，我们将无法改进它们。

测量模型执行情况（关于特定数据）的过程称为**评估**。确保我们的模型概括到我们可能会遇到的数据的过程称为**验证**。这两个过程都需要出现在每个机器学习工作流和应用程序中，我们将在本章中介绍这两个过程。

# 评价

科学的一个基本原则是测量，机器学习科学也不例外。我们需要能够测量或评估我们的模型的性能，以便我们能够继续改进它们，比较一个模型与另一个模型，并检测我们的模型何时表现不佳。

只有一个问题。我们如何评估模型的性能？我们应该衡量他们接受培训的速度，还是做出推断？我们应该衡量他们得到正确答案的次数吗？我们如何知道正确答案是什么？我们是否应该测量我们与观察值的偏离程度？我们如何测量这个距离？

正如你所看到的，关于我们如何评估我们的模型，有很多决策需要做出。真正重要的是背景。在某些情况下，效率肯定很重要，但每个机器学习环境都要求我们衡量我们的预测、推论或结果与理想预测、推论或结果的匹配程度。因此，测量计算结果和理想结果之间的比较应始终优先于速度优化。

通常，我们需要评估某些类型的结果：

*   **连续**：销售总额、股价、温度等结果，可以取任意连续数值（$12102.21、92 度等）
*   **分类**：结果，如欺诈/非欺诈、活动和名称，可作为有限数量类别之一（欺诈、站立、坦率等）

每种类型的结果都有相应的评估指标，这里将介绍这些指标。但是，请记住，您选择的评估指标取决于您试图通过机器学习模型实现的目标。没有一刀切的度量标准，在某些情况下，您甚至需要创建自己的度量标准。

# 连续度量

假设我们有一个模型，可以预测一些连续的价值，比如股票价格。假设我们已经积累了一些预测值，可以与实际观察值进行比较：

```go
observation,prediction
22.1,17.9
10.4,9.1
9.3,7.8
18.5,14.2
12.9,15.6
7.2,7.4
11.8,9.7
...
```

现在，我们如何衡量这个模型的性能？好的，第一步是取观测值和预测值之间的差值，得到一个`error`：

```go
observation,prediction,error
22.1,17.9,4.2
10.4,9.1,1.3
9.3,7.8,1.5
18.5,14.2,4.3
12.9,15.6,-2.7
7.2,7.4,-0.2
11.8,9.7,2.1
...
```

这个错误让我们大致了解了*距离我们应该预测的数值*有多远。然而，单独查看所有的误差值并不切实可行，尤其是在有大量数据的情况下。这些错误值可能有一百万或更多。因此，我们需要一种方法来理解总体上的错误。

**均方误差**（**MSE**）和**均方绝对误差**（**MAE**）为我们提供了总体误差的观点：

*   MSE 或**均方偏差**（**MSD**）是所有误差平方的平均值
*   MAE 是所有误差绝对值的平均值

MSE 和 MAE 都为我们的预测提供了一个很好的整体图景，但它们确实存在差异。由于均方误差取误差的平方，大误差值（例如，对应于异常值）比平均误差更为重要。换句话说，MSE 对异常值更敏感。另一方面，MAE 与我们试图预测的变量保持相同的单位，因此与这些值直接可比。

对于该数据集，我们可以解析观测值和预测值，并计算 MAE 和 MSE，如下所示：

```go
// Open the continuous observations and predictions.
f, err := os.Open("continuous_data.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)

// observed and predicted will hold the parsed observed and predicted values
// form the continuous data file.
var observed []float64
var predicted []float64

// line will track row numbers for logging.
line := 1
// Read in the records looking for unexpected types in the columns.
for {

    // Read in a row. Check if we are at the end of the file.
    record, err := reader.Read()
    if err == io.EOF {
        break
    }

    // Skip the header.
    if line == 1 {
        line++
        continue
    }

    // Read in the observed and predicted values.
    observedVal, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Printf("Parsing line %d failed, unexpected type\n", line)
        continue
    }

    predictedVal, err := strconv.ParseFloat(record[1], 64)
    if err != nil {
        log.Printf("Parsing line %d failed, unexpected type\n", line)
        continue
    }

    // Append the record to our slice, if it has the expected type.
    observed = append(observed, observedVal)
    predicted = append(predicted, predictedVal)
    line++
}

// Calculate the mean absolute error and mean squared error.
var mAE float64
var mSE float64
for idx, oVal := range observed {
    mAE += math.Abs(oVal-predicted[idx]) / float64(len(observed))
    mSE += math.Pow(oVal-predicted[idx], 2) / float64(len(observed))
}

// Output the MAE and MSE value to standard out.
fmt.Printf("\nMAE = %0.2f\n", mAE)
fmt.Printf("\nMSE = %0.2f\n\n", mSE)
```

对于我们的示例数据，这将导致以下结果：

```go
$ go build
$ ./myprogram 

MAE = 2.55

MSE = 10.51
```

为了判断这些值是否良好，我们需要将它们与观察数据中的值进行比较。特别是，MAE 为`2.55`，我们观测值的平均值为 14.0，因此我们的 MAE 约为我们平均值的 20%。不太好，视上下文而定。

与 MSE 和 MAE 一起，您可能会看到**R 平方**（也称为**R<sup class="calibre29">2</sup>**或**R2**）或**确定系数**，用作连续变量模型的评估指标。R 平方也给了我们一个关于预测偏差的一般概念，但 R 平方的概念略有不同。

R 平方衡量我们在预测值中捕获的观察值中方差的比例。请记住，我们试图预测的值具有一定的可变性。例如，我们可能试图预测股票价格、利率或疾病的进展，但从本质上讲，这些并不完全相同。我们正试图创建一个模型来预测观测值的这种变化，我们捕获的变化百分比用 R 平方表示。

方便的是，`gonum.org/v1/gonum/stat`有一个内置函数来计算 R 平方：

```go
// Calculate the R^2 value.
rSquared := stat.RSquaredFrom(observed, predicted, nil)

// Output the R^2 value to standard out.
fmt.Printf("\nR^2 = %0.2f\n\n", rSquared)
```

为示例数据集运行前面的代码会产生以下结果：

```go
$ go build
$ ./myprogram     

R^2 = 0.37
```

这是一个好的还是坏的 R 平方？请记住，R 平方是一个百分比，百分比越高越好。在这里，我们捕获了我们试图预测的变量中约 37%的方差。不太好。

# 分类度量

假设我们有一个模型，用来预测一些离散值，比如欺诈/不欺诈、站着/坐着/走路、批准/未批准，等等。我们的数据可能如下所示：

```go
observed,predicted
0,0
0,1
2,2
1,1
1,1
0,0
2,0
0,0
...
```

观察到的值可以取有限个值中的任意一个（在本例中为 1、2 或 3）。这些值中的每一个都代表我们数据中的一个离散类别（例如，类别 1 可能对应于欺诈性交易，类别 2 可能对应于非欺诈性交易，类别 3 可能对应于无效交易）。预测值也可以取这些离散值中的一个。在评估我们的预测时，我们希望以某种方式衡量我们做出这些离散预测的正确性。

# 分类变量的个体评估指标

事实上，有很多方法可以使用度量来评估离散预测，包括准确性、精确性、召回率、特异性、敏感性、影响、错误遗漏率等等。与连续变量一样，评估也没有一刀切的标准。每次处理问题时，都需要确定适合问题并与项目目标相匹配的度量标准。您不希望为错误的事情进行优化，然后浪费大量时间基于其他指标重新实现您的模型。

为了理解这些指标并确定哪一个适合我们的用例，我们需要认识到，当我们进行离散预测时，可能会出现许多不同的场景：

*   **真阳性**（**TP**）：我们预测了某个类别，观察结果实际上就是该类别（例如，我们预测了欺诈，观察结果是欺诈）
*   **假阳性**（**FP**）：我们预测了某个类别，但观察结果实际上是另一个类别（例如，我们预测了欺诈，但观察结果不是欺诈）
*   **真阴性**（**TN**）：我们预测观察不是某个类别，观察也不是那个类别（例如，我们预测不是欺诈，观察也不是欺诈）
*   **假阴性**（**FN**）：我们预测观察不是某一类别，但观察实际上是该类别（例如，我们预测不是欺诈，但观察是欺诈）

您可以看到，我们可以通过多种方式组合、聚合和度量这些场景。事实上，我们甚至可以以某种与我们的具体问题相关的独特方式对它们进行聚合/度量。但是，有一些相当标准的方法可以聚合和测量这些场景，从而得出以下通用指标：

*   **准确度**：预测正确的百分比，或*（TP+TN）/（TP+TN+FP+FN）*
*   **精度**：实际为阳性的阳性预测百分比，或*TP/（TP+FP）*
*   **回忆**：确定为阳性的阳性预测的百分比，或*TP/（TP+FN）*

尽管我将在这里强调这些，但您应该看看其他常用指标及其含义。在[可以找到一个很好的概述 https://en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall) 。

下面是一个解析数据并计算精度的示例。首先，我们读取我们的`labeled.csv`文件，创建一个 CSV 读取器，并初始化两个将保存解析的观察值/预测值的切片：

```go
// Open the binary observations and predictions.
f, err := os.Open("labeled.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)

// observed and predicted will hold the parsed observed and predicted values
// form the labeled data file.
var observed []int
var predicted []int
```

然后，我们将迭代 CSV 中的记录，解析值，并比较观察值和预测值，以计算精度：

```go
// line will track row numbers for logging.
line := 1

// Read in the records looking for unexpected types in the columns.
for {

    // Read in a row. Check if we are at the end of the file.
    record, err := reader.Read()
    if err == io.EOF {
        break
    }

    // Skip the header.
    if line == 1 {
        line++
        continue
    }

    // Read in the observed and predicted values.
    observedVal, err := strconv.Atoi(record[0])
    if err != nil {
        log.Printf("Parsing line %d failed, unexpected type\n", line)
        continue
    }

    predictedVal, err := strconv.Atoi(record[1])
    if err != nil {
        log.Printf("Parsing line %d failed, unexpected type\n", line)
        continue
    }

    // Append the record to our slice, if it has the expected type.
    observed = append(observed, observedVal)
    predicted = append(predicted, predictedVal)
    line++
}

// This variable will hold our count of true positive and
// true negative values.
var truePosNeg int

// Accumulate the true positive/negative count.
for idx, oVal := range observed {
    if oVal == predicted[idx] {
        truePosNeg++
    }
}

// Calculate the accuracy (subset accuracy).
accuracy := float64(truePosNeg) / float64(len(observed))

// Output the Accuracy value to standard out.
fmt.Printf("\nAccuracy = %0.2f\n\n", accuracy)
```

运行此操作将导致以下结果：

```go
$ go build
$ ./myprogram

Accuracy = 0.97
```

97%! 那很好。这意味着我们 97%的时候是对的。

我们同样可以计算精确性和召回率。但是，您可能已经注意到，当我们有两个以上的类别或类时，有几种方法可以做到这一点。我们可以认为 1 班是积极的，其他班级是消极的，2 班是积极的，其他班级是消极的，等等。也就是说，我们可以计算每个类的精度或召回率，如以下代码示例所示：

```go
// classes contains the three possible classes in the labeled data.
classes := []int{0, 1, 2}

// Loop over each class.
for _, class := range classes {

    // These variables will hold our count of true positives and
    // our count of false positives.
    var truePos int
    var falsePos int
    var falseNeg int

    // Accumulate the true positive and false positive counts.
    for idx, oVal := range observed {

        switch oVal {

        // If the observed value is the relevant class, we should check to
        // see if we predicted that class.
        case class:
            if predicted[idx] == class {
                truePos++
                continue
            }

            falseNeg++

        // If the observed value is a different class, we should 
        // check to see if we predicted a false positive.
        default:
            if predicted[idx] == class {
                falsePos++
            }
        }
    }

    // Calculate the precision.
    precision := float64(truePos) / float64(truePos+falsePos)

    // Calculate the recall.
    recall := float64(truePos) / float64(truePos+falseNeg)

    // Output the precision value to standard out.
    fmt.Printf("\nPrecision (class %d) = %0.2f", class, precision)
    fmt.Printf("\nRecall (class %d) = %0.2f\n\n", class, recall)
}
```

运行此代码将导致以下结果：

```go
$ go build
$ ./myprogram

Precision (class 0) = 1.00
Recall (class 0) = 1.00

Precision (class 1) = 0.96
Recall (class 1) = 0.94

Precision (class 2) = 0.94
Recall (class 2) = 0.96
```

请注意，精确度和召回率是略有不同的度量标准，具有不同的含义。如果我们想得到整体精度或召回率，我们可以平均每类精度和召回率。事实上，如果某些类比其他类更重要，我们可以取这些类的加权平均值，并将其用作我们的评估指标。

您可以看到，有几个指标是 100%。这看起来不错，但实际上可能表明存在问题，我们将进一步讨论。

在某些情况下，如金融和银行业，误报或其他情况可能会对某些类别造成非常高的成本。例如，将交易错误标记为欺诈可能导致重大损失。另一方面，其他类的某些结果可能可以忽略不计。这些场景可能保证使用自定义度量或成本函数，将某些类、某些结果或结果的某些组合作为比其他更重要的权重。

# 混淆矩阵、AUC 和 ROC

除了为我们的模型计算单个数值指标外，还有多种技术可以将各种指标组合成一种形式，从而为您提供更完整的模型性能表示。这些包括但不限于曲线下的**混淆矩阵**和**区域**（**AUC**/**接收机工作特性**（**ROC**）**曲线**。

混淆矩阵允许我们以二维格式可视化我们预测的各种**TP**、**TN**、**FP**和**FN**值。混淆矩阵具有与您应该预测的类别对应的行，以及与预测的类别对应的列。然后，每个元素的值就是相应的计数：

![](img/00015.jpeg)

如您所见，理想情况是您的混淆矩阵只有对角线上的条目（**TP**、**TN**）。对角线元素表示预测某一类别以及实际存在于该类别中的观察结果。非对角元素包括不正确预测的计数。

这种类型的混淆矩阵对于两类以上的问题特别有用。例如，您可能试图根据移动加速器和位置数据预测各种活动。这些活动可能包括两个以上的类别，如站立、坐下、跑步、驾驶等。此问题的混淆矩阵应大于 2 x 2，并允许您快速评估模型在所有类别上的总体性能，并确定模型性能较差的类别。

除了混淆矩阵外，ROC 曲线通常用于获得二元分类器（或训练用于预测两类分类器之一的模型）性能的总体图。ROC 曲线绘制了每个可能分类阈值的召回率与假阳性率（*FP/（FP+TN）*。

ROC 曲线中使用的阈值表示将分类的两个类别分开的各种边界或排名。也就是说，由 ROC 曲线评估的模型必须基于概率、排名或分数（在下图中称为分数）对这两个类别进行预测。在前面提到的每个示例中，分数都是按一种方式分类的，反之亦然：

![](img/00016.jpeg)

为了生成 ROC 曲线，我们在测试示例中为每个分数或等级绘制一个点（回忆、假阳性率）。然后我们可以把这些连接起来形成一条曲线。在许多情况下，您将看到沿 ROC 曲线图对角线绘制的直线。该直线是分类器的参考线，具有近似随机的预测能力：

![](img/00017.jpeg)

好的 ROC 曲线位于图的左上角，这意味着我们的模型比随机预测能力更好。ROC 曲线越靠近绘图的左上侧，效果越好。这意味着良好的 ROC 曲线有更多的 AUC；ROC 曲线的 AUC 也用作评估指标。请参阅下图：

![](img/00018.jpeg)

`gonum.org/v1/gonum/stat`具有一些内置函数和类型，可帮助您构建 ROC 曲线和 AUC 指标：

```go
func ROC(n int, y []float64, classes []bool, weights []float64) (tpr, fpr []float64)
```

下面是一个快速示例，用于计算具有 gonum 的 ROC 曲线的 AUC：

```go
// Define our scores and classes.
scores := []float64{0.1, 0.35, 0.4, 0.8}
classes := []bool{true, false, true, false}

// Calculate the true positive rates (recalls) and
// false positive rates.
tpr, fpr := stat.ROC(0, scores, classes, nil)
// Compute the Area Under Curve.
auc := integrate.Trapezoidal(fpr, tpr)

// Output the results to standard out.
fmt.Printf("true positive rate: %v\n", tpr)
fmt.Printf("false positive rate: %v\n", fpr)
fmt.Printf("auc: %v\n", auc)
```

运行此代码将导致以下结果：

```go
$ go build
$ ./myprogram 
true positive rate: [0 0.5 0.5 1 1]
false positive rate: [0 0 0.5 0.5 1]
auc: 0.75
```

# 验证

所以，现在我们知道了一些方法来衡量我们的模型的性能。事实上，如果我们愿意，我们可以创建一个超级复杂的模型，可以毫无误差地预测每一次观测。例如，我们可以创建一个模型，该模型将获取观察行的索引，并返回每一行的确切答案。这可能是一个非常大的函数，有很多参数，但它会返回正确的答案。

那么，这有什么问题？问题是它不能推广到新数据。我们的复杂模型可以很好地预测我们将要暴露的数据，但一旦我们尝试一些新的输入数据（这不是我们训练数据集的一部分），模型的性能可能会很差。

我们称这种类型的模型（不能概括）为**过度拟合**的模型。也就是说，我们根据现有数据使模型变得越来越复杂的过程过度拟合了模型。

预测连续值或离散/分类值时可能发生过拟合：

![](img/00019.jpeg)

为了防止过度拟合，我们需要验证我们的模型。执行验证有多种方法，我们将在这里介绍其中的两种。

每次生产模型时，您都需要确保您已经验证了您的模型，并了解它将如何推广到新数据。

# 培训和测试集

帮助防止过度拟合的第一种方法是在数据集的一部分上训练或拟合模型，然后在数据集的另一部分上测试或评估模型。训练模型通常包括对组成模型的一个或多个函数进行参数化，以使这些函数能够预测您试图预测的内容。然后，您可以使用我们前面讨论的一个或多个评估指标来评估这个经过培训的模型。这里重要的一点是，您不希望在用于训练模型的相同数据上测试/评估模型。

通过保留部分数据以供测试，您可以模拟模型看到新数据的场景。也就是说，模型正在根据未用于参数化模型的数据进行预测。

![](img/00020.jpeg)

许多人一开始会将 80%的数据分割成一个训练数据集，将 20%的数据分割成一个测试集（80/20 分割）。但是，您将看到不同的人以不同的比例将其数据集拆分。测试与训练数据的比例在一定程度上取决于您拥有的数据类型和数量以及您尝试训练的模型。通常，您希望确保您的训练数据和测试数据在很大程度上都是数据的相当准确的表示。

例如，如果您试图预测几个不同类别（a、B 和 C）中的一个，您不希望您的培训数据包含仅对应于 a 和 B 的观察值。在此类数据集上培训的模型可能只能预测 a 和 B 类别。同样，您也不希望测试集包含某些类别的子集，或者人为地加权类别的比例。这很容易发生，具体取决于数据的生成方式。

此外，您希望确保有足够的训练数据，以减少反复计算确定参数时的可变性。如果训练数据点太少，或者训练数据点的采样率太低，则模型训练可能会生成具有大量可变性的参数，或者甚至可能无法在数值上收敛。这些都表明您的模型缺乏预测能力。

通常，随着模型复杂性的增加，您将能够改进用于训练数据的评估指标，但在某个时候，测试数据的评估指标将开始变得更差。当测试数据的评估指标开始变得更糟时，您就开始过度拟合模型。理想的场景是当您能够将模型复杂性增加到拐点时，此时测试评估指标开始降低。另一种方法是，我们需要能够产生有价值结果的最可解释的模型（或简化模型），这非常符合本书中关于模型构建的一般哲学。

![](img/00021.jpeg)

一种将数据集快速拆分为训练集和测试集的方法是使用`github.com/kniren/gota/dataframe`。让我们使用一个数据集来演示这一点，该数据集包括一组关于医疗患者的匿名信息以及疾病和糖尿病进展的相应指示：

```go
age,sex,bmi,map,tc,ldl,hdl,tch,ltg,glu,y
0.0380759064334,0.0506801187398,0.0616962065187,0.021872354995,-0.0442234984244,-0.0348207628377,-0.043400845652,-0.00259226199818,0.0199084208763,-0.0176461251598,151.0
-0.00188201652779,-0.044641636507,-0.0514740612388,-0.0263278347174,-0.00844872411122,-0.0191633397482,0.0744115640788,-0.0394933828741,-0.0683297436244,-0.0922040496268,75.0
0.0852989062967,0.0506801187398,0.0444512133366,-0.00567061055493,-0.0455994512826,-0.0341944659141,-0.0323559322398,-0.00259226199818,0.00286377051894,-0.0259303389895,141.0
-0.0890629393523,-0.044641636507,-0.0115950145052,-0.0366564467986,0.0121905687618,0.0249905933641,-0.0360375700439,0.0343088588777,0.0226920225667,-0.00936191133014,206.0
0.00538306037425,-0.044641636507,-0.0363846922045,0.021872354995,0.00393485161259,0.0155961395104,0.00814208360519,-0.00259226199818,-0.0319914449414,-0.0466408735636,135.0
...
```

您可以在此处检索此数据集：[https://archive.ics.uci.edu/ml/datasets/diabetes](https://archive.ics.uci.edu/ml/datasets/diabetes) 。

要使用`github.com/kniren/gota/dataframe`拆分此数据，我们可以执行以下操作（将培训和测试拆分保存到相应的 CSV 文件中）：

```go
// Open the diabetes dataset file.
f, err := os.Open("diabetes.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
// The types of the columns will be inferred.
diabetesDF := dataframe.ReadCSV(f)

// Calculate the number of elements in each set.
trainingNum := (4 * diabetesDF.Nrow()) / 5
testNum := diabetesDF.Nrow() / 5
if trainingNum+testNum < diabetesDF.Nrow() {
    trainingNum++
}

// Create the subset indices.
trainingIdx := make([]int, trainingNum)
testIdx := make([]int, testNum)

// Enumerate the training indices.
for i := 0; i < trainingNum; i++ {
    trainingIdx[i] = i
}

// Enumerate the test indices.
for i := 0; i < testNum; i++ {
    testIdx[i] = trainingNum + i
}

// Create the subset dataframes.
trainingDF := diabetesDF.Subset(trainingIdx)
testDF := diabetesDF.Subset(testIdx)

// Create a map that will be used in writing the data
// to files.
setMap := map[int]dataframe.DataFrame{
    0: trainingDF,
    1: testDF,
}

// Create the respective files.
for idx, setName := range []string{"training.csv", "test.csv"} {

    // Save the filtered dataset file.
    f, err := os.Create(setName)
    if err != nil {
        log.Fatal(err)
    }

    // Create a buffered writer.
    w := bufio.NewWriter(f)

    // Write the dataframe out as a CSV.
    if err := setMap[idx].WriteCSV(w); err != nil {
        log.Fatal(err)
    }
}
```

运行此操作将导致以下结果：

```go
$ go build
$ ./myprogram 
$ wc -l *.csv
   443 diabetes.csv
    89 test.csv
   355 training.csv
   887 total
```

# 抗冲套

我们正在取得进展，以确保我们的模型使用训练集和测试集进行推广。但是，想象一下以下场景：

1.  我们根据我们的培训集开发了模型的第一个版本。
2.  我们在测试集上测试模型的第一个版本。
3.  我们对测试集的结果不满意，所以我们返回到步骤 1 并重复。

这个过程似乎合乎逻辑，但您可能已经看到了这个过程可能导致的问题。通过迭代地将模型暴露于测试集，我们实际上可以在测试数据上过度拟合模型。

有几种方法可以处理这种额外的过度装配。第一种方法是简单地创建另一组数据，称为**保留集**（也称为**验证集**。现在我们有了一个训练集，测试集和坚持集。出于显而易见的原因，这有时被称为三数据集验证。

![](img/00022.jpeg)

请记住，要真正了解模型的总体性能，绝不能在培训和测试中使用坚持集。在完成模型培训、模型调整和测试数据集上获得可接受性能的过程后，应保留此数据集以供验证。

您可能想知道如何随着时间的推移管理这种数据分割，并恢复用于训练或测试某些模型的不同数据集。在试图维护机器学习工作流的完整性时，数据的“来源”至关重要。这也正是 Pachyderm 的数据版本控制（在第 1 章*收集和组织数据*中介绍）创建的目的。我们将在第 9 章*部署和分发分析和模型*中详细介绍这一点。

# 交叉验证

除了保留用于验证的保持集外，交叉验证是验证模型通用性的常用技术。在交叉验证或 k-fold 交叉验证中，您实际上将数据集的*k*随机拆分为不同的训练和测试组合。把这些想象成*k*实验。

执行每个分割后，根据该分割的训练数据训练模型，然后根据该分割的测试数据对其进行评估。此过程将为数据的每个随机分割生成一个评估度量结果。然后，您可以对这些评估指标进行平均，以获得一个总体评估指标，该指标比任何一个单独的评估指标本身都更通用地表示模型性能。您还可以查看评估指标中的差异，以了解各种实验的稳定性。下图说明了该过程：

![](img/00023.jpeg)

与数据集验证相比，使用交叉验证的一些优点如下：

*   您正在使用整个数据集，因此，实际上正在将您的模型暴露给更多的培训示例和测试示例
*   已经为交叉验证编写了一些方便的函数和打包
*   它有助于防止因选择单个验证集而产生的偏差

`github.com/sjwhitworth/golearn`是一个 Go 包，为交叉验证提供了一些方便的功能。实际上，`github.com/sjwhitworth/golearn`包含了一系列机器学习功能，我们将在本书后面介绍这些功能，但现在，让我们看看哪些功能可用于交叉验证。

如果您查看`github.com/sjwhitworth/golearn/evaluation`包 Godocs，您将看到以下可用于交叉验证的功能：

```go
func GenerateCrossFoldValidationConfusionMatrices(data base.FixedDataGrid, cls base.Classifier, folds int) ([]ConfusionMatrix, error)
```

该函数实际上可以用于各种模型，但下面是一个使用决策树模型的示例（不要担心模型的细节）：

```go
// Define the decision tree model.
tree := trees.NewID3DecisionTree(param)

// Perform the cross validation.
cfs, err := evaluation.GenerateCrossFoldValidationConfusionMatrices(myData, tree, 5)
if err != nil {
    panic(err)
}

// Calculate the metrics.
mean, variance := evaluation.GetCrossValidatedMetric(cfs, evaluation.GetAccuracy)
stdev := math.Sqrt(variance)

// Output the results to standard out.
fmt.Printf("%0.2f\t\t%.2f (+/- %.2f)\n", param, mean, stdev*2)
```

# 工具书类

评价：

*   1.Essay on overfitting:【t0]http://scott.fortmann-roe.com/docs/MeasuringError.html
*   关于偏差-方差权衡的论文：[http://scott.fortmann-roe.com/docs/BiasVariance.html](http://scott.fortmann-roe.com/docs/BiasVariance.html)
*   分类评估指标比较：[https://en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall)
*   T0 文件：T1https://godoc.org/gonum.org/v1/gonum/stat T2
*   `github.com/sjwhitworth/golearn/evaluation`文件：[https://godoc.org/github.com/sjwhitworth/golearn/evaluation](https://godoc.org/github.com/sjwhitworth/golearn/evaluation)

验证：

*   \12304；T0】文件：\12304；T1]https://godoc.org/github.com/kniren/gota/dataframe \12304；T2]
*   `github.com/sjwhitworth/golearn/evaluation`文件：[https://godoc.org/github.com/sjwhitworth/golearn/evaluation](https://godoc.org/github.com/sjwhitworth/golearn/evaluation)

# 总结

选择合适的评估指标并制定评估/确认程序是任何机器学习项目的重要组成部分。您已经了解了各种相关的评估指标，以及如何使用保持集和/或交叉验证避免过度拟合。在下一章中，我们将开始研究机器学习模型，并将使用线性回归建立我们的第一个模型！