# 六、聚类

通常，一组数据可以组织成一组集群。例如，您可以将数据组织到与某些基本属性（如人口统计属性，包括年龄、性别、地理位置、就业状况等）或某些基本过程（如浏览、购物、机器人交互以及网站上的其他此类行为）相对应的集群中。检测和标记这些簇的机器学习技术自然被称为**聚类**技术。

到目前为止，我们探索的机器学习算法已经被**监督**。也就是说，我们有一组特征或属性与我们试图预测的相应标签或编号配对。我们使用这些带标签的数据使我们的模型符合我们在训练模型之前已经知道的行为。

大多数聚类技术是无监督的。与回归和分类的监督技术不同，在使用聚类模型找到数据集中的聚类之前，我们通常不知道这些聚类。因此，我们使用未标记的数据集和算法来研究聚类问题，并使用聚类算法为数据生成聚类标签。

此外，聚类技术与其他机器学习技术的区别在于，很难说出给定数据集的*正确*或*准确*聚类是什么。根据要查找的集群数量和数据点之间的相似性度量，最终可能会得到各种集群集，每个集群都有一些潜在的含义。这并不意味着聚类技术无法评估或验证，但它确实意味着我们需要了解我们的局限性，并在量化结果时小心。

# 理解聚类模型术语

集群非常独特，它有自己的术语集，如下所示。请记住，以下列表只是一个部分列表，因为有许多不同类型的集群使用相应的术语：

*   **集群**或**组**：这些集群或组中的每一个都是数据点的集合，我们的集群技术将数据点组织到其中。
*   **簇内****-组**或**簇内**：可以使用数据点与同一结果簇中其他数据点之间的相似性度量来评估由聚类产生的簇。这称为组内或簇内评估和相似性。
*   **Inter****-组**或**Inter cluster**：可以使用数据点与其他结果聚类中其他数据点之间的差异性度量来评估聚类结果。这称为组间或组间评估和差异性。
*   **内部标准**：通常情况下，我们没有一套黄金标准的聚类标签，可以用来评估结果聚类。在这些情况下，我们利用集群间和集群内的相似性来衡量集群技术的性能。
*   **外部标准**：在其他情况下，我们可能会有一个用于集群标签或分组的金标准，例如由人类法官生成的标准。这些场景允许我们使用标准或外部标准来评估集群技术。
*   **距离**或**相似性**：这是衡量两个数据点之间的距离。这可能是特征空间中的欧几里德距离，或者其他某种接近度度量。

# 测量距离或相似性

为了将数据点聚集在一起，我们需要定义和利用一些距离或相似性，这些距离或相似性定量地定义了数据点之间的接近度。选择此度量是每个集群项目的重要组成部分，因为它直接影响集群的生成方式。使用一个相似性度量产生的聚类可能与使用另一个相似性度量产生的聚类非常不同。

这些距离度量中最常见和最简单的是**欧几里德距离**或**平方欧几里德距离**。这只是特征空间中两个数据点之间的直线距离（您可能还记得这个距离，因为它也在[第 5 章](05.html#2TEN40-a5604e9f48ea4e63828d369cca8f0939)、*分类*中的 kNN 示例中使用）或数量平方。然而，还有许多其他的，有时更复杂的距离度量。下图显示了其中的一些：

![](img/00048.jpeg)

例如，**曼哈顿**距离是点之间的绝对*x*距离加上*y*距离，**闵可夫斯基**距离是欧几里德距离和曼哈顿距离之间的广义距离。与欧几里德距离相比，这些距离度量对于数据中的异常值（或异常值）更为稳健。

其他距离度量，例如**汉明**距离，适用于某些类型的数据，例如字符串。在所示示例中，**Golang**和**Gopher**之间的汉明距离为 4，因为字符串中有四个位置不同。因此，如果您正在处理文本数据，例如新闻文章或 tweet，则汉明距离可能是距离度量的一个好选择。

在这里，我们主要使用欧几里德距离。此距离通过`Distance()`功能在`gonum.org/v1/gonum/floats`中实现。作为说明，假设我们要计算`(1, 2)`点和`(3, 4)`点之间的距离。我们可以这样做：

```go
// Calculate the Euclidean distance, specified here via
// the last argument in the Distance function.
distance := floats.Distance([]float64{1, 2}, []float64{3, 4}, 2)

fmt.Printf("\nDistance: %0.2f\n\n", distance)
```

# 评价聚类技术

由于我们不试图预测一个数字或类别，我们之前讨论的连续变量和离散变量的评估指标并不真正适用于聚类技术。这并不意味着我们将避免测量聚类算法的性能。我们需要知道集群的性能如何。我们只需要介绍一些特定于集群的评估指标。

# 内部聚类评价

如果我们没有一套用于集群比较的黄金标准标签，那么我们就必须使用内部标准来评估集群技术的性能。换句话说，我们仍然可以通过在集群内部进行相似性和相异性度量来评估集群。

我们将在这里介绍的第一个内部度量称为**轮廓系数**。可按如下方式计算每个聚集数据点的轮廓系数：

![](img/00049.jpeg)

这里，*a*是一个数据点与同一簇中所有其他点之间的平均距离（例如欧几里德距离），*b*是一个数据点与该数据点所在簇最近的簇中所有其他点之间的平均距离。所有数据点的轮廓系数的平均值表示每个簇中的点有多紧密。此平均值可以针对每个群集或所有群集中的数据点进行计算。

让我们试着为虹膜数据集计算这个值，它可以被看作是一组三个簇，对应于三个虹膜物种中的每一个。首先，为了计算轮廓系数，我们需要知道三个簇的**质心**。这些质心只是三个簇的中心点（在我们的四维特征空间中），它们将允许我们确定哪个簇最接近某个数据点的簇。

为此，我们需要解析我们的 iris 数据集文件（最初在[第 1 章](01.html#KVCC0-a5604e9f48ea4e63828d369cca8f0939)、*收集和组织数据*）中介绍），通过聚类标签分离我们的记录，平均每个聚类中的特征，然后计算相应的质心。首先，我们将为质心定义一个`type`：

```go
type centroid []float64
```

然后，我们可以使用`github.com/kniren/gota/dataframe`创建包含每个鸢尾花物种质心的地图：

```go
// Pull in the CSV file.
irisFile, err := os.Open("iris.csv")
if err != nil {
    log.Fatal(err)
}
defer irisFile.Close()

// Create a dataframe from the CSV file.
irisDF := dataframe.ReadCSV(irisFile)

// Define the names of the three separate species contained in the CSV file.
speciesNames := []string{
    "Iris-setosa",
    "Iris-versicolor",
    "Iris-virginica",
}

// Create a map to hold our centroid information.
centroids := make(map[string]centroid)

// Filter the dataset into three separate dataframes,
// each corresponding to one of the Iris species.
for _, species := range speciesNames {

    // Filter the original dataset.
    filter := dataframe.F{
        Colname:    "species",
        Comparator: "==",
        Comparando: species,
    }
    filtered := irisDF.Filter(filter)

    // Calculate the mean of features.
    summaryDF := filtered.Describe()

    // Put each dimension's mean into the corresponding centroid.
    var c centroid
    for _, feature := range summaryDF.Names() {

        // Skip the irrelevant columns.
        if feature == "column" || feature == "species" {
            continue
        }
        c = append(c, summaryDF.Col(feature).Float()[0])
     }

     // Add this centroid to our map.
     centroids[species] = c
}

// As a sanity check, output our centroids.
for _, species := range speciesNames {
    fmt.Printf("%s centroid: %v\n", species, centroids[species])
}
```

编译并运行它可以为我们提供质心：

```go
$ go build
$ ./myprogram 
Iris-setosa centroid: [5.005999999999999 3.4180000000000006 1.464 0.2439999999999999]
Iris-versicolor centroid: [5.936 2.7700000000000005 4.26 1.3259999999999998]
Iris-virginica centroid: [6.587999999999998 2.9739999999999998 5.552 2.026]
```

接下来，我们需要实际计算每个数据点的轮廓系数。为此，让我们修改前面的代码，以便能够访问`for loop`之外的每一组过滤数据点：

```go
// Create a map to hold the filtered dataframe for each cluster.
clusters := make(map[string]dataframe.DataFrame)

// Filter the dataset into three separate dataframes,
// each corresponding to one of the Iris species.
for _, species := range speciesNames {

    ...

    // Add the filtered dataframe to the map of clusters.
    clusters[species] = filtered

    ...
}
```

我们还将创建一个方便的函数，用于从[T0]中的行检索浮点值：

```go
// dfFloatRow retrieves a slice of float values from a DataFrame
// at the given index and for the given column names.
func dfFloatRow(df dataframe.DataFrame, names []string, idx int) []float64 {
        var row []float64
        for _, name := range names {
                row = append(row, df.Col(name).Float()[idx])
        }
        return row
}
```

现在，我们可以循环计算轮廓系数所需的*a*和*b*的记录。我们还将平均轮廓系数，以获得集群的总体评估指标，如以下代码所示：

```go
// Convert our labels into a slice of strings and create a slice
// of float column names for convenience.
labels := irisDF.Col("species").Records()
floatColumns := []string{
    "sepal_length",
    "sepal_width",
    "petal_length",
    "petal_width",
}

// Loop over the records accumulating the average silhouette coefficient.
var silhouette float64

for idx, label := range labels {

    // a will store our accumulated value for a.
    var a float64

    // Loop over the data points in the same cluster.
    for i := 0; i < clusters[label].Nrow(); i++ {

        // Get the data point for comparison.
        current := dfFloatRow(irisDF, floatColumns, idx)
        other := dfFloatRow(clusters[label], floatColumns, i)

        // Add to a.
        a += floats.Distance(current, other, 2) / float64(clusters[label].Nrow())
    }

    // Determine the nearest other cluster.
    var otherCluster string
    var distanceToCluster float64
    for _, species := range speciesNames {

        // Skip the cluster containing the data point.
        if species == label {
            continue
        }

        // Calculate the distance to the cluster from the current cluster.
        distanceForThisCluster := floats.Distance(centroids[label], centroids[species], 2)

        // Replace the current cluster if relevant.
        if distanceToCluster == 0.0 || distanceForThisCluster < distanceToCluster {
            otherCluster = species
            distanceToCluster = distanceForThisCluster
        }
    }

    // b will store our accumulated value for b.
    var b float64

    // Loop over the data points in the nearest other cluster.
    for i := 0; i < clusters[otherCluster].Nrow(); i++ {

        // Get the data point for comparison.
        current := dfFloatRow(irisDF, floatColumns, idx)
        other := dfFloatRow(clusters[otherCluster], floatColumns, i)

        // Add to b.
        b += floats.Distance(current, other, 2) / float64(clusters[otherCluster].Nrow())
    }

    // Add to the average silhouette coefficient.
    if a > b {
        silhouette += ((b - a) / a) / float64(len(labels))
    }
    silhouette += ((b - a) / b) / float64(len(labels))
}

// Output the final average silhouette coeffcient to stdout.
fmt.Printf("\nAverage Silhouette Coefficient: %0.2f\n\n", silhouette)
```

编译并运行此示例评估将产生以下结果：

```go
$ go build
$ ./myprogram

Average Silhouette Coefficient: 0.51
```

我们如何知道`0.51`是一个好的还是坏的平均轮廓系数？好的，记住轮廓系数与平均簇内距离和平均簇间距离之间的差值成正比，它总是在*0.0*和*1.0*之间。因此，更高的值（那些接近于*1.0*的值）意味着更紧密的集群，并且与其他集群更为不同。

通常，我们可能需要调整集群数量和/或集群技术，以优化轮廓分数（即使其更大）。在这里，我们使用的数据实际上是手工标注的，所以`0.51`必须是这个数据集的一个好分数。对于其他数据集，它可能更高或更低，这取决于数据中是否存在集群以及您选择的相似性度量。

剪影评分绝不是内部评估集群的唯一方法。实际上，我们可以使用剪影评分中的数量*a*或*b*来分别评估我们的簇的同质性，或每个簇与其他簇的差异性。此外，我们还可以使用簇中各点之间的平均距离和簇的质心来测量紧密堆积的簇。此外，我们还可以使用各种其他评估指标，这些指标在这里不会详细介绍，例如**卡林斯基-哈拉巴兹指数**（此处将进一步讨论：[http://datamining.rutgers.edu/publication/internalmeasures.pdf [T7]。](http://datamining.rutgers.edu/publication/internalmeasures.pdf)

# 外部聚类评价

如果我们对集群有一个基本事实或黄金标准，那么我们可以利用各种外部集群评估技术。这一基本事实或黄金标准意味着我们可以访问或（通过人工注释）获得一组数据点，在这些数据点上对真实或所需的集群标签进行了注释。

通常，我们无法使用这种聚类黄金标准，因此，我们将不在这里详细介绍这些评估技术。但是，如果感兴趣或相关，您可以查看**调整后的兰德指数**、**互信息**、**福克斯-马洛分数**、**完整性**和**V-测度**，这些都是相关的外部聚类评估指标（请在此阅读更多有关这些的详细信息：[https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html) 。

# k-均值聚类

我们将在这里介绍的第一种聚类技术，可能是最著名的聚类技术，称为**k-means**c**lustering**，或者只是**k-means**。k-means 是一种迭代方法，其中数据点围绕在每次迭代期间调整的簇质心周围。这项技术相对容易掌握，但也有一些相关的微妙之处很容易被忽略。在探索这项技术的过程中，我们将确保强调这些。

由于 k-means 聚类非常容易实现，因此在 Go 中有许多算法的概念验证实现。您可以通过在此链接（[上搜索 k-means）找到这些 https://golanglibs.com/top?q=kmeans [T2]。但是，我们将使用一个最新的、使用起来相当简单的实现，`github.com/mash/gokmeans`。](https://golanglibs.com/top?q=kmeans)

# k-均值聚类综述

假设我们有一组由两个变量定义的数据点，*x<sub class="calibre25">1</sub>*和*x<sub class="calibre25">2</sub>*。这些数据点自然地表现出一些分组，如下图所示：

![](img/00050.jpeg)

要使用 k-means 对这些点进行自动聚类，我们首先需要选择聚类将产生多少个聚类。这是参数*k*，它给出了 k 的名称。在本例中，让我们使用*k=3*。

然后我们将随机选择*k*质心的*x<sub class="calibre25">1</sub>*和*x<sub class="calibre25">2</sub>*位置。这些随机质心将作为算法的起点。此类随机质心通过 Xs 显示在下图中：

![](img/00051.jpeg)

为了优化这些质心并对我们的点进行聚类，我们迭代执行以下操作：

1.  将每个数据点指定给与最近的质心对应的簇（通过我们选择的距离度量进行测量，如欧几里德距离）。
2.  计算每个簇内的平均*x<sub class="calibre25">1</sub>*和*x<sub class="calibre25">2</sub>*位置。
3.  将每个质心的位置更新到计算出的*x<sub class="calibre25">1</sub>*和*x<sub class="calibre25">2</sub>*位置。

重复步骤 1 至步骤 3，直到步骤 1 中的分配不再更改。此过程如下图所示：

![](img/00052.jpeg)

只有这么多，你可以用一个静态图来说明，但希望这是有帮助的。如果您想直观地逐步了解 k-means 流程以更好地了解更新，您应该查看[http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html](http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html) ，包括 k-均值聚类过程的交互式动画。

# k-均值假设和陷阱

k-means 似乎是一个非常简单的算法，事实就是如此。但是，它确实对您的数据做出了一些基本假设，这些假设很容易被忽略：

*   **球形**或**空间分组簇**：k-means 基本上在我们的特征空间中绘制球形或空间上紧密的区域来发现簇。这意味着对于非球形簇（本质上，簇在我们的特征空间中看起来不像分组的水滴），k-means 很可能失败。为了使这个想法更具体，非球形簇（k-means 可能表现不佳）可能如下所示：

![](img/00053.jpeg)

*   **相似大小**：k-means 还假设您的集群都具有相似的大小。小的外围聚类会导致简单的 k-means 算法偏离轨道，产生奇怪的分组。

此外，在使用 k-means 对数据进行聚类时，我们可能会遇到几个陷阱：

*   *k*的选择取决于我们。这意味着我们可以选择一个不合逻辑的*k*，但也意味着我们可以继续增加*k*，直到我们的每个点都有一个聚类（这将是非常好的聚类，因为每个点都与自身完全相同）。为了帮助您选择*k*，您应该使用**肘部图**方法。在这种方法中，在计算评估指标时增加*k*。随着*k*的增加，您的评估指标应该会不断提高，但最终会出现一个拐点，表明回报递减。理想的*k*就在这个拐弯处。
*   不能保证 k-means 总是收敛到相同的簇。当您从随机质心开始时，您的 k-means 算法可以在不同的运行中收敛到不同的局部极小值。您应该意识到这一点，并从各种初始化中运行 k-means 算法以确保稳定性。

# k-均值聚类实例

我们将用来说明集群技术的数据集是关于交付驱动因素的。数据集如下所示：

```go
$ head fleet_data.csv 
Driver_ID,Distance_Feature,Speeding_Feature
3423311935,71.24,28.0
3423313212,52.53,25.0
3423313724,64.54,27.0
3423311373,55.69,22.0
3423310999,54.58,25.0
3423313857,41.91,10.0
3423312432,58.64,20.0
3423311434,52.02,8.0
3423311328,31.25,34.0
```

第一列`Driver_ID`包括特定驾驶员的各种匿名身份。第二列和第三列是我们将在集群中使用的属性。`Distance_Feature`列是根据数据行驶的平均距离，`Speeding_Feature`是驾驶员以每小时超过限速 5 英里的速度行驶的平均时间百分比。

集群的目标是根据`Distance_Feature`和`Speeding_Feature`将交付驱动因素分组。请记住，这是一种无监督的学习技术，因此，我们并不真正知道数据中应该或可以形成哪些集群。希望我们能了解一些我们在练习开始时不知道的驾驶者的情况。

# 分析数据

是的，你猜对了！我们有了一个新的数据集，我们需要对这个数据集进行分析，以进一步了解它。让我们首先使用`github.com/kniren/dataframe`计算汇总统计数据，并使用`gonum.org/v1/plot`创建每个特征的直方图。我们已经在[第 4 章](04.html#2F4UM0-a5604e9f48ea4e63828d369cca8f0939)、*回归*和[第 5 章](05.html#2TEN40-a5604e9f48ea4e63828d369cca8f0939)、*分类*中多次这样做，所以我们不会在这里重复代码。让我们看看结果：

```go
$ go build
$ ./myprogram
[7x4] DataFrame

    column   Driver_ID         Distance_Feature Speeding_Feature
 0: mean     3423312447.500000 76.041523        10.721000       
 1: stddev   1154.844867       53.469563        13.708543       
 2: min      3423310448.000000 15.520000        0.000000        
 3: 25%      3423311447.000000 45.240000        4.000000        
 4: 50%      3423312447.000000 53.330000        6.000000        
 5: 75%      3423313447.000000 65.610000        9.000000        
 6: max      3423314447.000000 244.790000       100.000000      
    <string> <float>           <float>          <float> 
```

哇！看起来大多数司机的速度都在 10%左右，这有点吓人。甚至有一个司机似乎在 100%的时间里超速行驶。我希望我不是在他的路线上。

直方图特征如下图所示：

![](img/00054.jpeg)

在`Distance_Feature`数据中似乎有一个有趣的结构。这实际上很快就会影响到我们的聚类，但我们可以通过创建特征空间的散点图来获得该结构的另一个视图：

```go
// Open the driver dataset file.
f, err := os.Open("fleet_data.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
driverDF := dataframe.ReadCSV(f)

// Extract the distance column.
yVals := driverDF.Col("Distance_Feature").Float()

// pts will hold the values for plotting
pts := make(plotter.XYs, driverDF.Nrow())

// Fill pts with data.
for i, floatVal := range driverDF.Col("Speeding_Feature").Float() {
    pts[i].X = floatVal
    pts[i].Y = yVals[i]
}

// Create the plot.
p, err := plot.New()
if err != nil {
    log.Fatal(err)
}
p.X.Label.Text = "Speeding"
p.Y.Label.Text = "Distance"
p.Add(plotter.NewGrid())

s, err := plotter.NewScatter(pts)
if err != nil {
    log.Fatal(err)
}
s.GlyphStyle.Color = color.RGBA{R: 255, B: 128, A: 255}
s.GlyphStyle.Radius = vg.Points(3)

// Save the plot to a PNG file.
p.Add(s)
if err := p.Save(4*vg.Inch, 4*vg.Inch, "fleet_data_scatter.png"); err != nil {
    log.Fatal(err)
} 
```

编译并运行此命令将创建以下散点图：

![](img/00055.gif)

在这里，我们可以看到更多我们在直方图中看到的结构。这里似乎至少有两组清晰的数据。这种对数据的直觉可以作为正式应用聚类技术时的一种心理检查，它可以为我们实验[T0]k 值提供一个起点。

# 用 k-均值生成聚类

现在，让我们通过对交付驱动程序数据实际应用 k-means 聚类来解决问题。为了利用`github.com/mash/gokmeans`，我们首先需要创建`gokmeans.Node`值的切片，该切片将被输入到集群中：

```go
// Open the driver dataset file.
f, err := os.Open("fleet_data.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()        

// Create a new CSV reader.
r := csv.NewReader(f)
r.FieldsPerRecord = 3

// Initialize a slice of gokmeans.Node's to
// hold our input data.
var data []gokmeans.Node

// Loop over the records creating our slice of
// gokmeans.Node's.
for {

    // Read in our record and check for errors.
    record, err := r.Read()
    if err == io.EOF {
        break
    }
    if err != nil {
        log.Fatal(err)
    }

    // Skip the header.
    if record[0] == "Driver_ID" {
        continue
    }

    // Initialize a point.
    var point []float64

    // Fill in our point.
    for i := 1; i < 3; i++ {

        // Parse the float value.
        val, err := strconv.ParseFloat(record[i], 64)
        if err != nil {
            log.Fatal(err)
        }

        // Append this value to our point.
        point = append(point, val)
    }

    // Append our point to the data.
    data = append(data, gokmeans.Node{point[0], point[1]})
}
```

然后，生成集群就像调用`gomeans.Train(...)`函数一样简单。具体来说，我们将以*k=2*和最多`50`次迭代调用此函数：

```go
// Generate our clusters with k-means.
success, centroids := gokmeans.Train(data, 2, 50)
if !success {
    log.Fatal("Could not generate clusters")
}

// Output the centroids to stdout.
fmt.Println("The centroids for our clusters are:")
for _, centroid := range centroids {
    fmt.Println(centroid)
}
```

同时运行所有这些操作将为生成的簇提供以下质心：

```go
$ go build
$ ./myprogram 
The centroids for our clusters are:
[50.04763437499999 8.82875]
[180.01707499999992 18.29]
```

美好的我们已经生成了第一批集群。现在，我们需要继续评估这些集群的合法性。

我刚刚在这里输出了星团的质心，因为这就是我们需要知道的所有群点。如果我们想知道数据点是在第一个簇中还是在第二个簇中，我们只需要计算到这些质心的距离。质心的较近者对应于包含数据点的组。

# 评估生成的簇

我们评估刚刚生成的集群的第一种方法是可视化。让我们创建另一个散点图。但是，这次让我们为每个组使用不同的形状：

```go
// Open the driver dataset file.
f, err := os.Open("fleet_data.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
driverDF := dataframe.ReadCSV(f)

// Extract the distance column.
yVals := driverDF.Col("Distance_Feature").Float()        

// clusterOne and clusterTwo will hold the values for plotting.
var clusterOne [][]float64
var clusterTwo [][]float64

// Fill the clusters with data.
for i, xVal := range driverDF.Col("Speeding_Feature").Float() {
    distanceOne := floats.Distance([]float64{yVals[i], xVal}, []float64{50.05, 8.83}, 2)
    distanceTwo := floats.Distance([]float64{yVals[i], xVal}, []float64{180.02, 18.29}, 2)
    if distanceOne < distanceTwo {
        clusterOne = append(clusterOne, []float64{xVal, yVals[i]})
        continue
    }
    clusterTwo = append(clusterTwo, []float64{xVal, yVals[i]})
}

// pts* will hold the values for plotting
ptsOne := make(plotter.XYs, len(clusterOne))
ptsTwo := make(plotter.XYs, len(clusterTwo))

// Fill pts with data.
for i, point := range clusterOne {
    ptsOne[i].X = point[0]
    ptsOne[i].Y = point[1]
}

for i, point := range clusterTwo {
    ptsTwo[i].X = point[0]
    ptsTwo[i].Y = point[1]
}

// Create the plot.
p, err := plot.New()
if err != nil {
    log.Fatal(err)
}
p.X.Label.Text = "Speeding"
p.Y.Label.Text = "Distance"
p.Add(plotter.NewGrid())

sOne, err := plotter.NewScatter(ptsOne)
if err != nil {
    log.Fatal(err)
}
sOne.GlyphStyle.Radius = vg.Points(3)
sOne.GlyphStyle.Shape = draw.PyramidGlyph{}

sTwo, err := plotter.NewScatter(ptsTwo)
if err != nil {
    log.Fatal(err)
}
sTwo.GlyphStyle.Radius = vg.Points(3)

// Save the plot to a PNG file.
p.Add(sOne, sTwo)
if err := p.Save(4*vg.Inch, 4*vg.Inch, "fleet_data_clusters.png"); err != nil {
    log.Fatal(err)
}
```

此代码生成以下散点图，清楚地显示了我们成功的群集：

![](img/00056.gif)

定性地说，我们可以看到，有一组驾驶员主要是短途驾驶，还有一组驾驶员主要是长途驾驶。这实际上分别对应于农村和城市配送司机（或短途和长途司机）。

为了更定量地评估我们的簇，我们可以计算簇内点与簇质心之间的平均距离。为了帮助我们完成这项工作，让我们创建一个函数，使事情变得更简单：

```go
// withinClusterMean calculates the mean distance between
// points in a cluster and the centroid of the cluster.
func withinClusterMean(cluster [][]float64, centroid []float64) float64 {

    // meanDistance will hold our result.
    var meanDistance float64

    // Loop over the points in the cluster.
    for _, point := range cluster {
        meanDistance += floats.Distance(point, centroid, 2) / float64(len(cluster))
    }

    return meanDistance
}
```

现在，为了评估集群，我们只需为每个集群调用此函数：

```go
// Output our within cluster metrics.
fmt.Printf("\nCluster 1 Metric: %0.2f\n", withinClusterMean(clusterOne, []float64{50.05, 8.83}))
fmt.Printf("\nCluster 2 Metric: %0.2f\n", withinClusterMean(clusterTwo, []float64{180.02, 18.29}))
```

运行此命令可为我们提供以下指标：

```go
$ go build
$ ./myprogram 

Cluster 1 Metric: 11.68

Cluster 2 Metric: 23.52

```

正如我们所看到的，第一簇（散点图中的粉红色簇）是第二簇的两倍。这在没有图的情况下是一致的，并且为我们提供了更多关于集群的定量信息。

注意，这里很明显我们在寻找两个集群。但是，在其他情况下，簇的数量可能事先不清楚，特别是在您拥有的功能超过您所能想象的情况下。在这些场景中，使用一种方法（如`elbow`方法）来确定适当的*k*非常重要。有关此方法的更多信息，请访问[https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/](https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/) 。

# 其他聚类技术

这里没有讨论许多其他集群技术。这些包括 DBSCAN 和分层集群。不幸的是，Go 中的当前实现对于这些其他集群选项是有限的。DBSCAN 是在`https://github.com/sjwhitworth/golearn`中实现的，但据我所知，目前还没有其他集群技术的实现。

这创造了一个为社区做出贡献的好机会！集群技术通常并不复杂，创建另一种集群技术的实现可能是回馈 Go 数据科学社区的一个好方法。如果您想讨论实现、提出问题或获得帮助，请随时联系 Gophers Slack（`@dwhitena`中的作者或 Gophers Slack 上的`#data-science`中的其他数据科学 Gophers！

# 工具书类

距离度量和评估集群：

*   聚类评价综述：[https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)
*   各种距离/相似性度量的比较：[http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144059](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144059)
*   可视化 k-均值聚类：[https://www.naftaliharris.com/blog/visualizing-k-means-clustering/](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)
*   T0 文件：T1https://godoc.org/github.com/mash/gokmeans T2

# 总结

在本章中，我们介绍了聚类的一般原理，学习了如何评估生成的聚类，以及如何使用 k-means 聚类的 Go 实现。您现在应该处于良好状态，可以检测数据集中的分组结构。

接下来，我们将讨论时间序列数据的建模，如股票价格、传感器数据等。