# 五、分类

当许多人想到机器学习或人工智能时，他们可能首先想到机器学习来解决分类问题。在这些问题中，我们需要训练一个模型来预测有限个不同类别中的一个。例如，我们可能想要预测一个金融交易是否欺诈，或者我们可能想要预测一个图像是否包含热狗、飞机、猫等等，或者没有这些东西。

我们试图预测的类别可能从两个到数百或数千个。此外，我们可能仅基于几个或多个属性进行预测。这些组合产生的所有场景都会产生一系列模型，其中包含相应的假设、优点和缺点。

我们将在本章和本书后面部分介绍其中的一些模型，但为了简洁起见，我们将跳过许多模型。然而，正如我们在本书中处理的任何问题一样，在我们为用例选择一种模型时，简单性和完整性应该是一个主要关注点。有非常复杂的模型可以很好地解决某些问题，但是这些模型对于许多用例来说是不必要的。应用简单且可解释的分类模型应该继续是我们的目标之一。

# 理解分类模型术语

与回归一样，分类问题也有自己的一套术语。与回归中使用的术语有一些重叠，但也有一些特定于分类的新术语：

*   **类别**、**标签**或**类别**：这些术语可以互换使用，以表示我们预测的各种不同选择。例如，我们可以有欺诈类和非欺诈类，或者我们可以有坐、站、跑和走路类。
*   **二元分类**：这类分类只有两个类别或类别，如是/否或欺诈/非欺诈。
*   **多类分类**：这类分类有两个以上的类别，例如试图将热狗、飞机、猫等中的一个类别分配给图像的分类。
*   **标记数据**或**注释数据**：与相应类别配对的真实世界观察或记录。例如，如果我们通过交易时间预测欺诈，那么该数据将包括一组测量的交易时间以及一个相应的标签，指示它们是否欺诈。

# 逻辑回归

我们将要探索的第一个分类模型称为**逻辑回归**。从名称可以看出，此方法基于回归，我们在上一章中详细讨论了回归。然而，这个特殊的回归使用了一个特别适合分类问题的函数。

这也是一个简单且可解释的模型，这使得它成为解决分类问题的最佳首选。有多种现有的 Go 包为您实现逻辑回归，包括`github.com/xlvector/hector`、`github.com/cdipaolo/goml`和`github.com/sjwhitworth/golearn`。然而，在我们的示例中，我们将从头开始实施逻辑回归，这样您既可以对训练模型的内容形成全面的理解，也可以理解逻辑回归的简单性。此外，在某些情况下，您可能希望利用下一节中说明的从头开始的实现来避免代码库中的额外依赖项。

# logistic 回归综述

假设我们有两个类，*A*和*B*，我们正在尝试预测。我们还假设我们正试图根据变量*x*预测*A*或*B*。类*A*和*B*在与*x*绘制时可能看起来像这样：

![](img/00036.jpeg)

虽然我们可以画一条线来模拟这种行为，但这显然不是线性行为，不符合线性回归的假设。作为*x*的函数，数据的形状更像是从一个类到另一个类的一个步骤。我们真正需要的是一些函数，对于较低的*x 值，*会进入并停留在*A*，对于较高的*x 值，会进入并停留在*B*。*

好吧，我们很幸运！有这样一个功能。该函数称为**逻辑函数**，它为逻辑回归起了名字。其形式如下：

![](img/00037.jpeg)

在 Go 中实现，如下所示：

```go
// logistic implements the logistic function, which
// is used in logistic regression.
func logistic(x float64) float64 {
        return 1 / (1 + math.Exp(-x))
}
```

让我们用`gonum.org/v1/plot`绘制逻辑函数，看看它是什么样子：

```go
// Create a new plot.
p, err := plot.New()
if err != nil {
    log.Fatal(err)
}
p.Title.Text = "Logistic Function"
p.X.Label.Text = "x"
p.Y.Label.Text = "f(x)"

// Create the plotter function.
logisticPlotter := plotter.NewFunction(func(x float64) float64 { return logistic(x) })
logisticPlotter.Color = color.RGBA{B: 255, A: 255}

// Add the plotter function to the plot.
p.Add(logisticPlotter)

// Set the axis ranges.  Unlike other data sets,
// functions don't set the axis ranges automatically
// since functions don't necessarily have a
// finite range of x and y values.
p.X.Min = -10
p.X.Max = 10
p.Y.Min = -0.1
p.Y.Max = 1.1

// Save the plot to a PNG file.
if err := p.Save(4*vg.Inch, 4*vg.Inch, "logistic.png"); err != nil {
    log.Fatal(err)
}
```

编译并运行此绘图代码将创建以下图形：

![](img/00038.jpeg)

如您所见，该函数具有我们正在寻找的阶跃行为，用于对类*A*和*B*之间的步骤进行建模（假设*A*对应于*0.0*和*B*对应于*1.0*。

不仅如此，逻辑函数还有一些非常方便的性质，我们可以在进行分类时加以利用。为了看到这一点，让我们后退一步，考虑一下我们如何对 Po.To1 T1 进行建模。一种方法是对**几率****比率**的*对数*进行线性建模，*对数（**p/（1-p）】*，其中，几率比告诉我们*A*类的存在或不存在如何影响*类的存在或不存在 B*。使用这个奇怪的*日志*（称为**logit**）的原因很快就会有意义，但现在，让我们假设我们要对其进行线性建模，如下所示：

![](img/00039.jpeg)

现在，如果我们取这个优势比的指数，我们得到以下结果：

![](img/00040.jpeg)

这就是我们在简化前面的等式时得到的结果：

![](img/00041.jpeg)

如果你看这个等式的右边，你会看到我们的逻辑函数出现了。然后，该方程为我们的假设提供了一些形式上的支持，即逻辑函数有助于对两类之间的分离进行建模：*A*和*B*。例如，如果我们将*p*作为观察*B*的概率，并将逻辑函数拟合到我们的数据中，我们可以得到一个模型，预测*B*的概率作为*x*的函数（因此减去预测*a*的概率）。这如下图所示，在下图中，我们对*A*和*B*的原始图进行了形式化，并覆盖了建模概率的逻辑函数：

![](img/00042.jpeg)

因此，创建逻辑回归模型需要找到一个逻辑函数，该函数使我们可以用逻辑函数预测的观测值数量最大化。

注意，逻辑回归的一个优点是它保持简单和可解释性。然而，模型中的系数*m*和*b*与线性回归中的解释不同。如果我们有多个自变量，系数*m*（或系数*m<sub class="calibre42">1</sub>*、*m<sub class="calibre42">2</sub>*等）与优势比呈指数关系。因此，如果你有一个系数*m*为*0.5*，这与通过*exp（0.5 x）*的优势比有关。如果我们有两个系数*exp（0.5 x<sub class="calibre42">1</sub>+1.0 x<sub class="calibre42">2</sub>*，我们可以得出结论，与*x*的*exp（1.0）=2.72*相比，*x<sub class="calibre42">1</sub>1*的模型类概率为*exp（0.5）=1.65*。换句话说，我们不能直接比较系数。我们需要将它们保存在指数的上下文中。

# 逻辑回归假设和陷阱

还记得应用于线性回归的一长串假设吗？逻辑回归不受这些假设的限制。然而，在使用逻辑回归时，我们仍然做出一些重要假设：

*   **与对数优势率的线性关系**：正如我们前面讨论的，潜在的逻辑回归是一种假设，即我们可以用一条线来模拟优势率的对数。
*   **因变量**的编码：当我们之前建立模型时，我们假设我们试图预测*B*的概率，其中概率为 1.0 对应一个正*B*示例。因此，我们需要用这种编码来准备数据。这将在下面的示例中演示。
*   **观察的独立性**：我们数据中*x*的每个例子都必须是独立的。也就是说，我们必须避免多次包含相同的示例。

此外，要记住逻辑回归的一些常见陷阱如下：

*   逻辑回归比其他分类技术对异常值更敏感。记住这一点，并尝试相应地分析您的数据。
*   由于逻辑回归依赖于一个指数函数，该函数永远不会真正达到*0.0*或*1.0*（除了+/-无穷大），因此您的评估指标可能会有非常小的退化。

总而言之，逻辑回归是一种相当稳健的方法，仍然可以解释。在考虑如何解决分类问题时，它是一个灵活的模型，应该放在列表的顶部。

# Logistic 回归实例

我们将用来说明逻辑回归的数据集是对应于 LendingClub 贷款问题的数据。LendingClub 每季度发布一次该数据，其原始格式见[https://www.lendingclub.com/info/download-data.action](https://www.lendingclub.com/info/download-data.action) 。我们将使用该数据的精简版本（可在本书附带的代码包中获得），其中仅包括两列，`FICO.Range`（表示贷款申请人的信用评分，如 Fair、Isaac 和 Company 或 FICO 给出的）和`Interest.Rate`（表示贷款申请人的贷款利率）。数据如下所示：

```go
$ head loan_data.csv 
FICO.Range,Interest.Rate
735-739,8.90%
715-719,12.12%
690-694,21.98%
695-699,9.99%
695-699,11.71%
670-674,15.31%
720-724,7.90%
705-709,17.14%
685-689,14.33%
```

我们这项工作的目标是创建一个逻辑回归模型，该模型将告诉我们，对于给定的信用评分，我们是否能够以或低于某一利率获得贷款。例如，假设我们希望利率低于 12%。我们的模型会告诉我们，给定一定的信用分数，我们可以（是的，或者一级）或者不能（不是，二级）获得贷款。

# 清理和分析数据

查看前面的贷款数据样本，我们可以看到它并不完全符合我们分类所需的格式。具体而言，我们需要做以下工作：

1.  从“利率”和“FICO 分数”列中删除非数字字符。
2.  根据给定的利率阈值，将我们的利率编码为两类。我们将使用*1.0*代表我们的一级（是的，我们可以获得该利率的贷款）和*0.0*代表我们的二级（不，我们不能获得该利率的贷款）。
3.  为 FICO 信用评分选择一个值。我们得到了一系列的信用分数，但我们需要一个单一的值。平均、最小或最大分数是自然选择，在我们的示例中，我们将使用最小值（保守）。
4.  在这种情况下，我们将**标准化**我们的 FICO 分数（从每个分数中减去最低分数值，然后除以分数范围）。这将使我们的得分值分布在*0.0*和*1.0*之间。我们需要有理由这样做，因为这会降低数据的可读性。然而，有一个很好的理由。我们将使用梯度下降法来训练逻辑回归，该方法可以更好地处理标准化数据。事实上，在使用非规范化数据运行同一示例时，存在收敛问题。

让我们编写一个 Go 程序，该程序将清除给定利率下的数据（我们的示例为 12%）。我们将读取给定文件中的数据，使用`encoding/csv`解析值，并将清理后的数据放入名为`clean_loan_data.csv`的输出文件中。在数据清理过程中，我们将使用以下最小值和最大值，我们将其定义为常量：

```go
const (
    scoreMax = 830.0
    scoreMin = 640.0
)
```

然后，实际的清洁功能显示在以下代码中：

```go
// Open the loan dataset file.
f, err := os.Open("loan_data.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)
reader.FieldsPerRecord = 2

// Read in all of the CSV records
rawCSVData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// Create the output file.
f, err = os.Create("clean_loan_data.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a CSV writer.
w := csv.NewWriter(f)

// Sequentially move the rows writing out the parsed values.
for idx, record := range rawCSVData {

    // Skip the header row.
    if idx == 0 {

        // Write the header to the output file.
        if err := w.Write(record); err != nil {
            log.Fatal(err)
        }
        continue
    }

    // Initialize a slice to hold our parsed values.
    outRecord := make([]string, 2)

    // Parse and standardize the FICO score.
    score, err := strconv.ParseFloat(strings.Split(record[0], "-")[0], 64)
    if err != nil {
        log.Fatal(err)
    }

    outRecord[0] = strconv.FormatFloat((score-scoreMin)/(scoreMax-scoreMin), 'f', 4, 64)

    // Parse the Interest rate class.
    rate, err := strconv.ParseFloat(strings.TrimSuffix(record[1], "%"), 64)
    if err != nil {
        log.Fatal(err)
    }

    if rate <= 12.0 {
        outRecord[1] = "1.0"

        // Write the record to the output file.
        if err := w.Write(outRecord); err != nil {
            log.Fatal(err)
        }
        continue
    }

    outRecord[1] = "0.0"

    // Write the record to the output file.
    if err := w.Write(outRecord); err != nil {
        log.Fatal(err)
    }
}

// Write any buffered data to the underlying writer (standard output).
w.Flush()

if err := w.Error(); err != nil {
    log.Fatal(err)
}
```

编译并运行它确认了我们所需的输出：

```go
$ go build
$ ./example3 
$ head clean_loan_data.csv 
FICO_score,class
0.5000,1.0
0.3947,0.0
0.2632,0.0
0.2895,1.0
0.2895,1.0
0.1579,0.0
0.4211,1.0
0.3421,0.0
0.2368,0.0
```

伟大的我们拥有所需格式的数据。现在，让我们通过为 FICO 分数和利率数据创建直方图并计算汇总统计数据，来获得更多关于数据的直觉。我们将利用`github.com/kniren/gota/dataframe`计算汇总统计，利用`gonum.org/v1/plot`生成直方图：

```go
// Open the CSV file.
loanDataFile, err := os.Open("clean_loan_data.csv")
if err != nil {
    log.Fatal(err)
}
defer loanDataFile.Close()

// Create a dataframe from the CSV file.
loanDF := dataframe.ReadCSV(loanDataFile)

// Use the Describe method to calculate summary statistics
// for all of the columns in one shot.
loanSummary := loanDF.Describe()

// Output the summary statistics to stdout.
fmt.Println(loanSummary)

// Create a histogram for each of the columns in the dataset.
for _, colName := range loanDF.Names() {

    // Create a plotter.Values value and fill it with the
    // values from the respective column of the dataframe.
    plotVals := make(plotter.Values, loanDF.Nrow())
    for i, floatVal := range loanDF.Col(colName).Float() {
        plotVals[i] = floatVal
    }

    // Make a plot and set its title.
    p, err := plot.New()
    if err != nil {
        log.Fatal(err)
    }
    p.Title.Text = fmt.Sprintf("Histogram of a %s", colName)

    // Create a histogram of our values.
    h, err := plotter.NewHist(plotVals, 16)
    if err != nil {
        log.Fatal(err)
    }

    // Normalize the histogram.
    h.Normalize(1)

    // Add the histogram to the plot.
    p.Add(h)

    // Save the plot to a PNG file.
    if err := p.Save(4*vg.Inch, 4*vg.Inch, colName+"_hist.png"); err != nil {
        log.Fatal(err)
    }
}
```

运行此操作将产生以下输出：

```go
$ go build
$ ./myprogram
[7x3] DataFrame

 column FICO_score class 
 0: mean 0.346782 0.396800
 1: stddev 0.184383 0.489332
 2: min 0.000000 0.000000
 3: 25% 0.210500 0.000000
 4: 50% 0.315800 0.000000
 5: 75% 0.447400 1.000000
 6: max 1.000000 1.000000
 <string> <float> <float>     

$ ls *.png
class_hist.png FICO_score_hist.png
```

我们可以看到，平均信用分数非常高，达到了*706.1*，一级和零级之间有着非常好的平衡，这一点可以从*0.5*附近的平均值看出。然而，似乎有更多零级贷款的例子（这相当于没有收到利率为 12%或更低的贷款）。此外，`*.png`直方图如下所示：

![](img/00043.jpeg)

这证实了我们对类之间平衡的怀疑，并表明 FICO 分数有点偏向较低的值。

# 创建我们的培训和测试集

与前一章中的示例类似，我们需要将数据拆分为训练集和测试集。我们将再次使用`github.com/kniren/gota/dataframe`来执行此操作：

```go
// Open the clean loan dataset file.
f, err := os.Open("clean_loan_data.csv")        
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
// The types of the columns will be inferred.
loanDF := dataframe.ReadCSV(f)

// Calculate the number of elements in each set.
trainingNum := (4 * loanDF.Nrow()) / 5
testNum := loanDF.Nrow() / 5
if trainingNum+testNum < loanDF.Nrow() {
    trainingNum++
}

// Create the subset indices.
trainingIdx := make([]int, trainingNum)
testIdx := make([]int, testNum)

// Enumerate the training indices.
for i := 0; i < trainingNum; i++ {
    trainingIdx[i] = i
}

// Enumerate the test indices.
for i := 0; i < testNum; i++ {
    testIdx[i] = trainingNum + i
}

// Create the subset dataframes.
trainingDF := loanDF.Subset(trainingIdx)
testDF := loanDF.Subset(testIdx)

// Create a map that will be used in writing the data
// to files.
setMap := map[int]dataframe.DataFrame{
    0: trainingDF,
    1: testDF,
}

// Create the respective files.
for idx, setName := range []string{"training.csv", "test.csv"} {

    // Save the filtered dataset file.
    f, err := os.Create(setName)
    if err != nil {
        log.Fatal(err)
    }

    // Create a buffered writer.
    w := bufio.NewWriter(f)

    // Write the dataframe out as a CSV.
    if err := setMap[idx].WriteCSV(w); err != nil {
        log.Fatal(err)
    }
}
```

使用我们的培训和测试示例在两个文件中编译和运行此结果：

```go
$ go build
$ ./myprogram 
$ wc -l *.csv
 2046 clean_loan_data.csv
  410 test.csv
 1638 training.csv
 4094 total
```

# logistic 回归模型的训练与检验

现在，让我们创建一个函数来训练逻辑回归模型。此功能需要执行以下操作：

1.  接受我们的 FICO 分数数据作为自变量。
2.  在我们的模型中添加一个截距。
3.  初始化并优化逻辑回归模型的系数（或权重）。
4.  返回定义训练模型的优化权重。

为了优化系数/权重，我们将使用一种称为**随机梯度下降**的技术。该技术将在附录*机器学习相关算法/技术中详细介绍。*就目前而言，我们正在尝试使用一些非优化权重进行预测，计算这些权重的误差，然后迭代更新，以最大限度地提高做出正确预测的可能性。

此优化的实现如下所示。该函数将以下内容作为输入：

*   `features`：指向 gonum`mat64.Dense`矩阵的指针。该矩阵包括我们正在使用的任何自变量的一列（在本例中为 FICO 分数），以及表示截距的 1.0 列。
*   `labels`：一片浮动，包括我们`features`对应的所有类标签。
*   `numSteps`：优化的最大迭代次数。
*   `learningRate`：有助于优化收敛的可调参数。

然后，该函数输出逻辑回归模型的优化权重：

```go
// logisticRegression fits a logistic regression model
// for the given data.
func logisticRegression(features *mat64.Dense, labels []float64, numSteps int, learningRate float64) []float64 {

        // Initialize random weights.
        _, numWeights := features.Dims()
        weights := make([]float64, numWeights)

        s := rand.NewSource(time.Now().UnixNano())
        r := rand.New(s)

        for idx, _ := range weights {
                weights[idx] = r.Float64()
        }

        // Iteratively optimize the weights.
        for i := 0; i < numSteps; i++ {

        // Initialize a variable to accumulate error for this iteration.
        var sumError float64

        // Make predictions for each label and accumulate error.
        for idx, label := range labels {

            // Get the features corresponding to this label.
            featureRow := mat64.Row(nil, idx, features)

            // Calculate the error for this iteration's weights.
            pred := logistic(featureRow[0]*weights[0]
            featureRow[1]*weights[1])
            predError := label - pred
            sumError += math.Pow(predError, 2)

            // Update the feature weights.
            for j := 0; j < len(featureRow); j++ {
                weights[j] += learningRate * predError * pred * (1 - pred) * featureRow[j]
            }
        }
    }

    return weights
}
```

如您所见，此函数相对紧凑且简单。这将使我们的代码保持可读性，并允许我们团队中的人员快速理解模型中发生的事情，而无需将内容隐藏在黑盒子中。

尽管 R 和 Python 在机器学习中很流行，但您可以看到机器学习算法可以在 Go 中快速而紧凑地实现。此外，这些实现立即实现了远远超过其他语言中天真实现的完整性级别。

为了在我们的训练数据集上训练我们的逻辑回归模型，我们将用`encoding/csv`解析我们的训练文件，然后为我们的`logisticRegression`函数提供必要的参数。此过程如下所示，并附带一些代码将我们经过培训的逻辑公式输出到`stdout`：

```go
// Open the training dataset file.
f, err := os.Open("training.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)
reader.FieldsPerRecord = 2

// Read in all of the CSV records
rawCSVData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// featureData and labels will hold all the float values that
// will eventually be used in our training.
featureData := make([]float64, 2*len(rawCSVData))
labels := make([]float64, len(rawCSVData))

// featureIndex will track the current index of the features
// matrix values.
var featureIndex int

// Sequentially move the rows into the slices of floats.
for idx, record := range rawCSVData {

    // Skip the header row.
    if idx == 0 {
        continue
    }

    // Add the FICO score feature.
    featureVal, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Fatal(err)
    }

    featureData[featureIndex] = featureVal

    // Add an intercept.
    featureData[featureIndex+1] = 1.0

    // Increment our feature row.
    featureIndex += 2

    // Add the class label.
    labelVal, err := strconv.ParseFloat(record[1], 64)
    if err != nil {
        log.Fatal(err)
    }

    labels[idx] = labelVal
}        

// Form a matrix from the features.
features := mat64.NewDense(len(rawCSVData), 2, featureData)

// Train the logistic regression model.
weights := logisticRegression(features, labels, 100, 0.3)

// Output the Logistic Regression model formula to stdout.
formula := "p = 1 / ( 1 + exp(- m1 * FICO.score - m2) )"
fmt.Printf("\n%s\n\nm1 = %0.2f\nm2 = %0.2f\n\n", formula, weights[0], weights[1])
```

编译和运行此培训功能将产生以下经过培训的逻辑回归公式：

```go
$ go build
$ ./myprogram

p = 1 / ( 1 + exp(- m1 * FICO.score - m2) )

m1 = 13.65
m2 = -4.89

```

然后我们可以直接利用这个公式进行预测。但是，请记住，此模型预测获得贷款的概率（利率为 12%）。因此，我们需要在进行预测时利用概率阈值。例如，我们可以说，*0.5+*中的任何*p*将被视为正（一级，或获得贷款），任何较低的*p*值将被视为负。此类预测通过以下功能实现：

```go
// predict makes a prediction based on our
// trained logistic regression model.
func predict(score float64) float64 {

    // Calculate the predicted probability.
    p := 1 / (1 + math.Exp(-13.65*score+4.89))

    // Output the corresponding class.
    if p >= 0.5 {
        return 1.0
    }

    return 0.0
}
```

使用这个`predict`函数，我们可以使用本书前面介绍的一个评估指标来评估经过训练的逻辑回归模型。在这种情况下，让我们使用精度，如下代码所示：

```go
// Open the test examples.
f, err := os.Open("test.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)

// observed and predicted will hold the parsed observed and predicted values
// form the labeled data file.
var observed []float64
var predicted []float64

// line will track row numbers for logging.
line := 1

// Read in the records looking for unexpected types in the columns.
for {

    // Read in a row. Check if we are at the end of the file.
    record, err := reader.Read()
    if err == io.EOF {
        break
    }

    // Skip the header.
    if line == 1 {
        line++
        continue
    }

    // Read in the observed value.
    observedVal, err := strconv.ParseFloat(record[1], 64)
    if err != nil {
        log.Printf("Parsing line %d failed, unexpected type\n", line)
        continue
    }

    // Make the corresponding prediction.
    score, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Printf("Parsing line %d failed, unexpected type\n", line)
        continue
    }

    predictedVal := predict(score)

    // Append the record to our slice, if it has the expected type.
    observed = append(observed, observedVal)
    predicted = append(predicted, predictedVal)
    line++
}

// This variable will hold our count of true positive and
// true negative values.
var truePosNeg int

// Accumulate the true positive/negative count.
for idx, oVal := range observed {
    if oVal == predicted[idx] {
        truePosNeg++
    }
}

// Calculate the accuracy (subset accuracy).
accuracy := float64(truePosNeg) / float64(len(observed))

// Output the Accuracy value to standard out.
fmt.Printf("\nAccuracy = %0.2f\n\n", accuracy)

```

在我们的数据上运行此测试可获得以下精度：

```go
$ go build
$ ./myprogram

Accuracy = 0.83
```

美好的 83%的准确率对于我们在大约 30 行 Go 中实现的机器学习模型来说并不坏。通过这个简单的模型，我们能够预测，给定一定的信用评分，贷款申请人是否会接受利息低于或等于 12%的贷款。不仅如此，我们还利用了一家真实公司的真实数据。

# K 近邻

从逻辑回归开始，让我们尝试我们的第一个非回归模型，**k-最近邻**（**kNN**。kNN 也是一个简单的分类模型，也是最容易掌握的模型算法之一。它的基本前提是，如果我想分类一个记录，我应该考虑其他类似的记录。

kNN 在多个现有 Go 包中实现，包括`github.com/sjwhitworth/golearn`、`github.com/rikonor/go-ann`、`github.com/akreal/knn`和`github.com/cdipaolo/goml`。我们将使用`github.com/sjwhitworth/golearn`实现，这将作为一般使用`github.com/sjwhitworth/golearn`的重要介绍。

# kNN 概述

如上所述，kNN 的工作原理是，我们应根据类似记录对记录进行分类。在定义类似规则时，需要处理一些细节。然而，kNN 没有许多模型所具有的参数和选项的复杂性。

再想象一下，我们有两个类*A*和*B*。但是，这次假设我们想要基于两个特征进行分类，*x<sub class="calibre25">1</sub>*和*x<sub class="calibre25">2</sub>*。从视觉上看，这类似于以下内容：

![](img/00044.jpeg)

现在，假设我们有一个未知类的新数据点。这个新的数据点将位于这个空间的某个地方。kNN 表示，要对这一新点进行分类，我们应执行以下操作：

1.  根据某种近距度量（例如，*x<sub class="calibre25">1</sub>*和*x<sub class="calibre25">2</sub>*在该空间中的直线距离），找到距离新点最近的*k*点。
2.  确定这些*k*近邻中有多少属于*A*类，有多少属于*B 类。*
3.  将新点划分为*k*近邻中的主导类。

例如，如果我们选择*k*为 4，则如下所示：

![](img/00045.jpeg)

我们的神秘点有三个*A*近邻，只有一个*B*近邻。因此，我们将新的神秘点归类为*类*点。

有许多相似性度量可以用来确定*k*最近邻。其中最常见的是欧几里德距离*，*就是由你的特征组成的空间中从一点到下一点的直线距离（*x<sub class="calibre25">1</sub>*和*x<sub class="calibre25">2</sub>*在我们的示例中）。其他包括**曼哈顿距离**、**闵可夫斯基****距离**、**余弦相似性**和**贾卡相似性**。

与评估指标一样，有许多方法可以测量距离或相似性。在使用 kNN 时，您应该研究这些度量的优点和缺点，并选择一个适合您的用例和数据的度量。然而，当有疑问时，可以尝试从欧几里德距离开始。

# kNN 假设和陷阱

由于其简单性，kNN 没有太多的假设。但是，在应用 kNN 时，您应该注意一些常见的陷阱：

*   kNN 被惰性地评估。我们的意思是，当我们需要做出预测时，计算距离或相似性。在做出预测之前，实际上没有什么需要训练或适应的。这有一些优点，但是当您有许多数据点时，对点的计算和搜索可能会很慢。
*   *k*的选择取决于您，但您应该在选择*k*时采取一些形式主义，并为您选择的*k*提供理由。选择*k*的一种常见技术就是搜索*k*值的范围。例如，您可以从[T10]k=2[T11]开始。然后，您可以开始增加*k*，并对每个*k*在测试集上进行评估。
*   kNN 没有考虑哪些特征比其他特征更重要。此外，如果您的特征的确定性规模比其他特征大得多，这可能会不自然地加重这些较大特征的重要性。

# kNN 示例

为此，以及本章剩余的示例，我们将使用一个关于**鸢尾花**的数据集来解决一个经典的分类问题。数据集如下所示：

```go
$ head iris.csv 
sepal_length,sepal_width,petal_length,petal_width,species
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa
5.0,3.4,1.5,0.2,Iris-setosa
4.4,2.9,1.4,0.2,Iris-setosa
```

前四列是对鸢尾花的各种测量，最后一列是相应的物种标签。本例的目标是创建一个 kNN 分类器，该分类器能够根据一组测量值预测鸢尾花的种类。有三种花，或三个类，这使得这是一个多类分类（与我们用逻辑回归进行的二元分类相反）。

您可能还记得，我们已经在[第 2 章](02.html#1DOR00-a5604e9f48ea4e63828d369cca8f0939)、*矩阵、概率和统计中详细分析了 iris 数据集。*我们不会在这里重新归档数据。然而，在开发 kNN 模型时，我们对数据的直觉仍然很重要。确保您回到[第 2 章](02.html#1DOR00-a5604e9f48ea4e63828d369cca8f0939)、*矩阵、概率和统计*，以提醒自己该数据集中变量的分布。

在本例中，我们将使用`github.com/sjwhitworth/golearn`。`github.com/sjwhitworth/golearn`实现了多种机器学习模型，包括 kNN 和其他一些我们将很快探索的模型。`github.com/sjwhitworth/golearn`还实现了交叉验证。在这里，我们将利用交叉验证来执行培训、测试和验证，这很方便，并且可以避免在培训集和测试集之间执行手动拆分。

要使用`github.com/sjwhitworth/golearn`的任何模型，我们必须首先将数据转换为一种称为**实例**的`github.com/sjwhitworth/golearn`内部格式。对于 iris 数据，我们可以按如下方式执行：

```go
// Read in the iris data set into golearn "instances".
irisData, err := base.ParseCSVToInstances("iris.csv", true)
if err != nil {
    log.Fatal(err)
}
```

然后，初始化 kNN 模型并执行交叉验证既快速又简单：

```go
// Initialize a new KNN classifier.  We will use a simple
// Euclidean distance measure and k=2.
knn := knn.NewKnnClassifier("euclidean", "linear", 2)

// Use cross-fold validation to successively train and evaluate the model
// on 5 folds of the data set.
cv, err := evaluation.GenerateCrossFoldValidationConfusionMatrices(irisData, knn, 5)
if err != nil {
    log.Fatal(err)
}
```

最后，我们可以得到交叉验证的五倍平均精度，并将该精度输出到`stdout`：

```go
// Get the mean, variance and standard deviation of the accuracy for the
// cross validation.
mean, variance := evaluation.GetCrossValidatedMetric(cv, evaluation.GetAccuracy)
stdev := math.Sqrt(variance)

// Output the cross metrics to standard out.
fmt.Printf("\nAccuracy\n%.2f (+/- %.2f)\n\n", mean, stdev*2)
```

同时编译和运行所有这些将产生以下输出：

```go
$ go build
$ ./myprogram 
Optimisations are switched off
Optimisations are switched off
Optimisations are switched off
Optimisations are switched off
Optimisations are switched off
KNN: 95.00 % done
Accuracy
0.95 (+/- 0.05)
```

在交叉验证期间，从软件包中进行了一些良性记录后，我们可以看到 kNN（*k=2*）能够以 95%的准确率预测鸢尾花物种！

下一步是用各种不同的*k*值来尝试这个模型。事实上，绘制精度与*k*值的对比图是一个很好的练习，以查看哪个*k*值为您提供了最佳性能。

# 决策树与随机森林

基于树的模型与我们之前讨论过的模型类型非常不同，但是它们被广泛使用并且非常强大。你可以考虑一个**决策树**模型，就像一系列应用于数据的`if-then`语句。当您训练这种类型的模型时，您正在构造一系列控制流语句，这些语句最终允许您对记录进行分类。

决策树在`github.com/sjwhitworth/golearn`和`github.com/xlvector/hector`中实现，随机林在`github.com/sjwhitworth/golearn`、`github.com/xlvector/hector`和`github.com/ryanbressler/CloudForest`中实现。我们将在下一节所示的示例中再次使用`github.com/sjwhitworth/golearn`。

# 决策树和随机森林概述

再一次，考虑我们的类 TytT0 和 Ty2 T2。在这种情况下，假设我们有一个特征，*x<sub class="calibre25">1</sub>，*的范围从*0.0*到*1.0*，我们还有另一个特征，*x<sub class="calibre25">2</sub>*是分类的，可以采用两个值中的一个，【T16 a<sub class="calibre25">1</sub>和*a<sub class="calibre25">2</sub>*（可能是男性/女性或红色/蓝色）。对新数据点进行分类的决策树可能如下所示：

![](img/00046.jpeg)

有多种方法可以选择如何构建、拆分决策树，等等。确定如何构建决策树的最常用方法之一是使用称为**熵**的量。这种基于熵的方法在*附录*中进行了更详细的讨论，但是，基本上，我们构建了树并根据哪些特征为我们提供了有关我们正在解决的问题的最多信息进行了拆分。更重要的功能在树上的优先级更高。

重要特征的优先顺序和自然外观结构使得决策树非常易于解释。这使得决策树对于可能需要解释推论的应用程序非常重要（例如，出于法规遵从性原因）。

但是，单个决策树可能会因训练数据的更改而不稳定。换言之，即使训练数据发生微小变化，树的结构也可能发生显著变化。这在操作和认知上都具有挑战性，这也是创建**随机森林**模型的原因之一。

一个随机森林是一个决策树的集合，这些决策树一起工作以做出预测。与单决策树相比，随机森林更稳定，而且它们对过度拟合更为鲁棒。事实上，这种在**集合**中组合模型的想法在整个机器学习中都很流行，既可以提高简单分类器（如决策树）的性能，也可以帮助防止过度拟合。

为了构建一个随机森林，我们选择*N*个随机特征子集，并基于这些子集构建*N*个独立的决策树。在进行预测时，我们可以让这些*N*决策树中的每一个都进行预测。为了获得最终预测，我们可以对这些*N*预测进行多数投票。

# 决策树与随机森林假设和陷阱

基于树的方法是非统计方法，没有回归之类的假设。但是，要记住一些陷阱：

*   单个决策树模型很容易过度拟合数据，尤其是在不限制树的深度的情况下。大多数实现允许您通过参数（或**修剪**您的决策树）来限制此深度。修剪参数通常允许您删除对预测几乎没有影响的树部分，从而降低模型的总体复杂性。
*   当我们开始讨论集合模型时，比如随机森林，我们进入的模型有些不透明。很难获得关于模型集合的直觉，在某种程度上，你必须将其视为一个黑匣子。只有在必要时才应采用此类不易解释的模型。
*   尽管决策树本身在计算上非常高效，但随机林在计算上可能非常低效，这取决于您拥有的特征数量和随机林中的树数量。

# 决策树示例

在本例中，我们将再次使用 iris 数据集。您已经在`github.com/sjwhitworth/golearn`中学习了如何处理此数据集，我们可以再次遵循类似的模式。我们将再次使用交叉验证。但是，这次我们将拟合一个决策树模型：

```go
// Read in the iris data set into golearn "instances".
irisData, err := base.ParseCSVToInstances("iris.csv", true)
if err != nil {
    log.Fatal(err)
}

// This is to seed the random processes involved in building the
// decision tree.
rand.Seed(44111342)

// We will use the ID3 algorithm to build our decision tree.  Also, we
// will start with a parameter of 0.6 that controls the train-prune split.
tree := trees.NewID3DecisionTree(0.6)

// Use cross-fold validation to successively train and evaluate the model
// on 5 folds of the data set.
cv, err := evaluation.GenerateCrossFoldValidationConfusionMatrices(irisData, tree, 5)
if err != nil {
    log.Fatal(err)
}

// Get the mean, variance and standard deviation of the accuracy for the
// cross validation.
mean, variance := evaluation.GetCrossValidatedMetric(cv, evaluation.GetAccuracy)
stdev := math.Sqrt(variance)

// Output the cross metrics to standard out.
fmt.Printf("\nAccuracy\n%.2f (+/- %.2f)\n\n", mean, stdev*2)
```

编译并运行此决策树模型可提供以下信息：

```go
$ go build
$ ./myprogram 

Accuracy
0.94 (+/- 0.06)
```

这次的准确率是 94%。比我们的 kNN 模型稍微差一点，但仍然很体面。

# 随机森林示例

`github.com/sjwhitworth/golearn`还实现了随机森林。为了利用随机森林解决 iris 问题，我们只需将我们的决策树模型替换为随机森林。我们需要告诉软件包我们要构建多少棵树，以及每棵树有多少随机选择的特性。

每个树的特征数量的一个合理默认值是总特征数量的平方根，在我们的例子中是两个。我们将看到，对于我们的小数据集，这种选择不会产生好的结果，因为我们需要所有的特性来做出好的预测。但是，我们将用 sane 默认值演示随机林，以了解其工作原理：

```go
// Assemble a random forest with 10 trees and 2 features per tree,
// which is a sane default (number of features per tree is normally set
// to sqrt(number of features)).
rf := ensemble.NewRandomForest(10, 2)

// Use cross-fold validation to successively train and evaluate the model
// on 5 folds of the data set.
cv, err := evaluation.GenerateCrossFoldValidationConfusionMatrices(irisData, rf, 5)
if err != nil {
    log.Fatal(err)
}
```

运行此操作的准确性比单个决策树差。如果我们将每棵树的特征数改回 4 个，我们将重新创建单个决策树的准确性。这意味着，每棵树都使用与单个决策树相同的信息进行训练，从而产生相同的结果。

Random forest 在这里可能会杀伤力过大，并且不能用任何性能提升来证明它的合理性，因此最好坚持使用单一决策树。这种单一的决策树也更具解释性和效率。

# 朴素贝叶斯

我们将在这里介绍的最后一个分类模型称为**朴素贝叶斯**。在[第 2 章](02.html#1DOR00-a5604e9f48ea4e63828d369cca8f0939)*矩阵、概率和统计学*中，我们讨论了构成该技术基础的贝叶斯规则。朴素贝叶斯是一种基于概率的方法，类似于 logistic 回归，但其基本思想和假设不同。

NaiveBayes 也在`github.com/sjwhitworth/golearn`中实现，这将允许我们轻松地尝试它。然而，还有多种其他 Go 实现，包括`github.com/jbrukh/bayesian`、`github.com/lytics/multibayes`和`github.com/cdipaolo/goml`。

# 朴素贝叶斯及其大假设综述

朴素贝叶斯在一个大的假设下运行。该假设表示，类的概率以及数据集中某个特征的存在或不存在与数据集中其他特征的存在或不存在无关。这使我们能够为某一类的概率编写一个非常简单的公式，给定是否存在某些特征。

让我们举个例子，让它更具体。再次，假设我们正试图根据电子邮件中的单词预测两类电子邮件，*A*和*B*（例如，垃圾邮件和非垃圾邮件）。朴素贝叶斯假设某个词的存在/不存在与其他词无关。如果我们做这个假设，某个类包含某个单词的概率与所有单独的条件概率相乘成正比。利用这一点、贝叶斯规则、一些链式规则和我们的独立假设，我们可以将某一类的条件概率写成如下：

![](img/00047.jpeg)

右侧的所有内容都可以通过计算训练集中特征和标签的出现次数来计算，这就是训练模型时所做的。然后，可以通过将这些概率链串在一起进行预测。

实际上，在实践中，使用了一个小技巧来避免将一组接近零的数字串在一起。我们可以取概率的对数，把它们相加，然后取表达式的指数。这一过程在实践中通常更好。

# 朴素贝叶斯例子

最后一次回到我们的贷款数据集，我们将使用`github.com/sjwhitworth/golearn`来解决相同的贷款接受问题。我们将使用我们在逻辑回归示例中使用的相同训练集和测试集。但是，我们需要将数据集中的标签转换为`github.com/sjwhitworth/golearn`中使用的二进制分类器格式。我们可以编写一个简单的函数来执行此转换：

```go
// convertToBinary utilizes built in golearn functionality to
// convert our labels to a binary label format.
func convertToBinary(src base.FixedDataGrid) base.FixedDataGrid {
    b := filters.NewBinaryConvertFilter()
    attrs := base.NonClassAttributes(src)
    for _, a := range attrs {
        b.AddAttribute(a)
    }
    b.Train()
    ret := base.NewLazilyFilteredInstances(src, b)
    return ret
}
```

一旦我们有了这些，我们就可以训练和测试我们的朴素贝叶斯模型，如下代码所示：

```go
// Read in the loan training data set into golearn "instances".
trainingData, err := base.ParseCSVToInstances("training.csv", true)
if err != nil {
    log.Fatal(err)
}

// Initialize a new Naive Bayes classifier.
nb := naive.NewBernoulliNBClassifier()

// Fit the Naive Bayes classifier.
nb.Fit(convertToBinary(trainingData))

// Read in the loan test data set into golearn "instances".
// This time we will utilize a template of the previous set
// of instances to validate the format of the test set.
testData, err := base.ParseCSVToTemplatedInstances("test.csv", true, trainingData)
if err != nil {
    log.Fatal(err)
}

// Make our predictions.
predictions := nb.Predict(convertToBinary(testData))

// Generate a Confusion Matrix.
cm, err := evaluation.GetConfusionMatrix(testData, predictions)
if err != nil {
    log.Fatal(err)
}

// Retrieve the accuracy.
accuracy := evaluation.GetAccuracy(cm)
fmt.Printf("\nAccuracy: %0.2f\n\n", accuracy)
```

编译并运行此命令可获得以下精度：

```go
$ go build
$ ./myprogram 

Accuracy: 0.63
```

这不如我们从头开始的逻辑回归那么好。然而，这里仍然有一些预测能力。一个好的练习是将 LendingClub 数据集中的一些其他特性添加到此模型，特别是一些分类变量。这可能会改善朴素贝叶斯结果。

# 工具书类

一般分类：

*   `github.com/sjwhitworth/golearn`文件：[https://godoc.org/github.com/sjwhitworth/golearn](https://godoc.org/github.com/sjwhitworth/golearn)

# 总结

我们涵盖了各种分类模型，包括逻辑回归、k 近邻、决策树、随机森林和朴素贝叶斯。事实上，我们甚至从头开始实施逻辑回归。所有这些模型都有各自的优缺点，我们已经讨论过了。但是，它们应该为您提供一套好的工具，以开始使用 Go 进行分类。

在下一章中，我们将讨论另一种称为**聚类**的机器学习。这是我们将讨论的第一种无监督技术，我们将尝试几种不同的方法。