# 四、回归

我们将探索的第一组机器学习技术通常被称为**回归**。回归是一个过程，通过这个过程，我们可以了解一个变量（例如，销售额）相对于另一个变量（例如，用户数量）是如何变化的。这些技术本身是有用的。然而，它们也是讨论机器学习技术的一个很好的起点，因为它们构成了我们将在本书后面讨论的其他更复杂技术的基础。

一般来说，机器学习中的回归技术与预测连续值有关（例如，股价、温度或疾病进展）。**分类**（我们将在下一章中介绍）涉及预测离散变量，或一组离散类别中的一个（例如，欺诈/非欺诈、坐/站/跑或热狗/非热狗）。如前所述，回归技术作为分类算法的一部分在整个机器学习中使用，但在本章中，我们将重点介绍它们在预测连续值方面的基本应用。

# 理解回归模型行话

如前所述，回归本身是一个分析一个变量和另一个变量之间关系的过程，但机器学习中使用了一些术语来描述这些变量以及各种类型的回归和与回归相关的过程：

*   **响应**或**因变量**：这些术语将互换用于我们试图基于一个或多个其他变量预测的变量。此变量通常标记为*y*。
*   **解释变量**、**自变量**、**特征**、**属性**或**回归系数**：这些术语将互换用于我们用于预测反应的变量。这些变量通常被标记为*x*或*x<sub class="calibre25">1</sub>、x<sub class="calibre25">2</sub>、*等。
*   **线性回归**：这类回归假设因变量线性依赖于自变量（即遵循直线方程）。
*   **非线性回归**：这类回归假设因变量依赖于非线性关系中的自变量（例如多项式或指数）。
*   **多元回归**：包含多个自变量的回归。
*   **拟合**或**训练**：将模型参数化的过程，如回归模型，以便预测某个因变量。
*   **预测**：使用参数化模型（如回归模型）预测某个因变量的过程。

这些术语中的一些将在本书其余部分的回归上下文和其他上下文中使用。

# 线性回归

线性回归是最简单的机器学习模型之一。但是，您不应以任何方式忽略此模型。如前所述，它是在其他模型中使用的基本构建块，并且具有一些非常重要的优点。

正如本书所讨论的，机器学习应用程序中的完整性是至关重要的，模型越简单、解释性越强，维护完整性就越容易。此外，由于模型简单且可解释，它允许您理解变量之间的推断关系，并在开发过程中检查您的工作。用快进实验室的迈克·李·威廉姆斯（Mike Lee Williams）的话来说（在[中）http://blog.fastforwardlabs.com/2017/08/02/interpretability.html](http://blog.fastforwardlabs.com/2017/08/02/interpretability.html) ：

未来是算法的。可解释模型在人类和智能机器之间提供了一种更安全、更高效、最终更具协作性的关系。

线性回归模型是可解释的，因此，它们可以为数据科学家提供安全和高效的选择。当你在寻找一个模型来预测一个连续变量时，如果你的数据和问题允许你使用它，你应该考虑并尝试线性回归（甚至多元线性回归）。

# 线性回归综述

在线性回归中，我们尝试通过自变量*x*对因变量*y*进行建模，使用直线方程：

![](img/00024.jpeg)

此处，*m*为线路坡度，*b*为截距。例如，假设我们想通过我们网站上每天的*用户数量*来模拟每日*销售额*。为了通过线性回归实现这一点，我们需要确定一个*m*和*b*，它允许我们通过以下公式预测销售额：

![](img/00025.jpeg)

因此，我们训练的模型实际上就是这个参数化函数。我们输入**用户数**，得到**预计销量**，如下所示：

![](img/00026.jpeg)

线性回归模型的训练或拟合包括确定*m*和*b、*的值，从而得出的公式对我们的反应具有预测能力。确定*m*和*b*的方法有多种，但最常见和最简单的方法称为**普通最小二乘法**（**OLS**。

要使用 OLS 查找*m*和*b*，我们首先为*m*和*b*选择一个值来创建第一个示例行。然后，我们测量每个已知点（例如，从我们的训练集）和示例线之间的垂直距离。这些距离被称为**误差**或**残差**，类似于我们在[第 3 章](03.html#23MNU0-a5604e9f48ea4e63828d369cca8f0939)、*评估和验证*中讨论的误差，如下图所示：

![](img/00027.jpeg)

接下来，我们将这些误差的平方和相加：

![](img/00028.jpeg)

我们调整*m*和*b*，直到我们最小化误差平方和。换句话说，我们训练的线性回归线就是使平方和最小化的线。

有多种方法可以找到使平方误差之和最小的直线，对于 OLS，可以通过分析找到直线。然而，一种用于最小化平方误差之和的非常流行且通用的优化技术被称为**梯度下降**。这种方法在实现上更有效，在计算上更有利（例如在内存方面），并且比解析解更灵活。

在[附录](04.html)、*与机器学习相关的算法/技术*中对梯度下降进行了更详细的讨论，因此我们将避免在这里进行冗长的讨论。可以说，线性回归和其他回归的许多实现都利用梯度下降来拟合或训练线性回归线。事实上，梯度下降在机器学习中是普遍存在的，并且也支持更复杂的建模技术，如深度学习。

# 线性回归假设和陷阱

与所有机器学习模型一样，线性回归并不适用于所有情况，它确实对数据和数据中的关系做出了某些假设。线性回归的假设如下：

*   **线性关系**：这似乎很明显，但线性回归假设因变量线性依赖于自变量（通过直线方程）。如果这种关系不是线性的，线性回归很可能表现不佳。
*   **正态性**：这个假设意味着你的变量应该按照正态分布（看起来像钟形）分布。我们将在本章后面回到这个属性，并讨论遇到非正态分布变量时的一些权衡和选择。
*   **无多重共线性**：多重共线性是一个奇特的术语，意味着自变量并非真正独立。他们以某种方式相互依赖。
*   **无自相关**：自相关是另一个奇特的术语，意味着变量依赖于自身或自身的某个移位版本（如在某些可预测的时间序列中）。
*   **同质性**：这可能是这组术语中最花哨的一个词，但它的意思相对简单，实际上并不需要经常担心。线性回归假设，对于自变量的所有值，回归线周围的数据方差大致相同。

从技术上讲，我们需要满足所有这些假设才能使用线性回归。我们知道数据是如何分布的以及数据的行为是非常重要的。当我们在线性回归的示例中分析数据时，我们将研究这些假设。

作为数据科学家或分析师，在应用线性回归时，您希望牢记以下陷阱：

*   您正在为自变量的特定范围训练线性回归模型。您应该小心预测超出此范围的值，因为您的回归线可能不适用（例如，因变量可能在极值处开始非线性行为）。
*   你可以通过发现两个变量之间的一些虚假关系来错误地定义一个线性回归模型，而这两个变量实际上与另一个变量无关。您应该检查以确保变量可能在功能上相关的逻辑原因。
*   数据中的异常值或极值可能会偏离某些拟合类型（如 OLS）的回归线。有一些方法可以拟合更不受异常值影响的回归线，或者与异常值行为不同的回归线，例如正交最小二乘法或岭回归。

# 线性回归示例

为了说明线性回归，让我们以一个问题为例，创建我们的第一个机器学习模型！我们将要使用的示例数据是示例广告数据。它是`.csv`格式，如下所示：

```go
$ head Advertising.csv 
TV,Radio,Newspaper,Sales
230.1,37.8,69.2,22.1
44.5,39.3,45.1,10.4
17.2,45.9,69.3,9.3
151.5,41.3,58.5,18.5
180.8,10.8,58.4,12.9
8.7,48.9,75,7.2
57.5,32.8,23.5,11.8
120.2,19.6,11.6,13.2
8.6,2.1,1,4.8
```

该数据集包括一组属性，这些属性表示在广告渠道（`TV`、`Radio`、`Newspaper`上的支出，以及相应的销售额（`Sales`。本例中，我们的目标是通过广告支出的一个属性（自变量）对销售额（因变量）进行建模。

# 分析数据

为了确保我们创建了一个我们理解的模型，或者至少是一个过程，并确保我们可以在心里检查我们的结果，我们需要从数据分析开始每个机器学习模型构建过程。我们需要了解每个变量是如何分布的，以及它们的范围和可变性。

为此，我们将计算前面在[第 2 章](02.html#1DOR00-a5604e9f48ea4e63828d369cca8f0939)、*矩阵、概率和统计*中讨论的汇总统计数据。在这里，我们将利用`github.com/kniren/gota/dataframe`包中内置的方法，在一次操作中计算数据集所有列的汇总统计数据：

```go
// Open the CSV file.
advertFile, err := os.Open("Advertising.csv")
if err != nil {
    log.Fatal(err)
}
defer advertFile.Close()

// Create a dataframe from the CSV file.
advertDF := dataframe.ReadCSV(advertFile)

// Use the Describe method to calculate summary statistics
// for all of the columns in one shot.
advertSummary := advertDF.Describe()

// Output the summary statistics to stdout.
fmt.Println(advertSummary)
```

编译并运行此命令将得到以下结果：

```go
$ go build
$ ./myprogram
[7x5] DataFrame

    column   TV         Radio     Newspaper  Sales    
 0: mean     147.042500 23.264000 30.554000  14.022500
 1: stddev   85.854236  14.846809 21.778621  5.217457 
 2: min      0.700000   0.000000  0.300000   1.600000 
 3: 25%      73.400000  9.900000  12.600000  10.300000
 4: 50%      149.700000 22.500000 25.600000  12.900000
 5: 75%      218.500000 36.500000 45.100000  17.400000
 6: max      296.400000 49.600000 114.000000 27.000000
    <string> <float>    <float>   <float>    <float> 
```

如您所见，这以一个漂亮的表格形式打印出我们所有的汇总统计数据，包括平均值、标准偏差、最小值、最大值、*25%/75%*百分位数和中位数（或 50%百分位数）。

这些值为我们训练线性回归模型时将看到的数字提供了一个很好的数值参考。然而，这并不能让我们对数据有很好的视觉理解。为此，我们将为每列中的值创建直方图：

```go
// Open the advertising dataset file.
f, err := os.Open("Advertising.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
advertDF := dataframe.ReadCSV(f)

// Create a histogram for each of the columns in the dataset.
for _, colName := range advertDF.Names() {

    // Create a plotter.Values value and fill it with the
    // values from the respective column of the dataframe.
    plotVals := make(plotter.Values, advertDF.Nrow())
    for i, floatVal := range advertDF.Col(colName).Float() {
        plotVals[i] = floatVal
    }

    // Make a plot and set its title.
    p, err := plot.New()
    if err != nil {
        log.Fatal(err)
    }
    p.Title.Text = fmt.Sprintf("Histogram of a %s", colName)

    // Create a histogram of our values drawn
    // from the standard normal.
    h, err := plotter.NewHist(plotVals, 16)
    if err != nil {
        log.Fatal(err)
    }        

    // Normalize the histogram.
    h.Normalize(1)

    // Add the histogram to the plot.
    p.Add(h)

    // Save the plot to a PNG file.
    if err := p.Save(4*vg.Inch, 4*vg.Inch, colName+"_hist.png"); err != nil {
        log.Fatal(err)
    }
}
```

该程序将为每个直方图创建一个`.png`图像：

![](img/00029.gif)

现在，看看这些直方图和我们计算的汇总统计数据，我们需要考虑我们是否在线性回归假设下工作。特别是，我们可以看到并不是所有的变量都是正态分布的（也就是说，它们是钟形的）。销售可能有些钟形，但其他的看起来并不正常。

我们可以使用统计工具，如**分位数**（**q-q**图）来确定分布与正态分布的接近程度，我们甚至可以执行统计测试来确定正态分布后变量的概率。然而，大多数情况下，我们可以从直方图中得到一个大致的概念。

现在我们必须做出决定。至少我们的一些数据在技术上不符合线性回归模型的假设。我们现在可以执行以下操作之一：

*   尝试将变量转换为正态分布（例如，使用幂变换），然后在线性回归模型中使用这些转换后的变量。此选项的优点是，我们将在模型的假设范围内操作。缺点是我们会使我们的模型更难理解，也更难解释。
*   获取不同的数据来解决我们的问题。
*   忽略线性回归假设的问题，尝试创建模型。

对此可能有其他观点，但我建议您先尝试第三种选择。此选项没有太大危害，因为您可以快速训练线性回归模型。如果您最终得到了一个性能良好的模型，那么您就避免了进一步的复杂化，并且拥有了一个很好的简单模型。如果最终得到的模型性能很差，则可能需要求助于其他选项之一。

# 选择自变量

所以，现在我们对我们的数据有了一些直觉，并且已经认识到我们的数据如何符合线性回归模型的假设。现在，在预测因变量和每场比赛的平均分数时，我们如何选择哪个变量作为自变量？

做出这个决定最简单的方法是通过直观地探索因变量和所有自变量选择之间的相关性。特别是，您可以绘制因变量与其他变量的散点图（使用`gonum.org/v1/plot`）：

```go
// Open the advertising dataset file.
f, err := os.Open("Advertising.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
advertDF := dataframe.ReadCSV(f)

// Extract the target column.
yVals := advertDF.Col("Sales").Float()

// Create a scatter plot for each of the features in the dataset.
for _, colName := range advertDF.Names() {

    // pts will hold the values for plotting
    pts := make(plotter.XYs, advertDF.Nrow())

    // Fill pts with data.
    for i, floatVal := range advertDF.Col(colName).Float() {
        pts[i].X = floatVal
        pts[i].Y = yVals[i]
    }

    // Create the plot.
    p, err := plot.New()
    if err != nil {
        log.Fatal(err)
    }
    p.X.Label.Text = colName
    p.Y.Label.Text = "y"
    p.Add(plotter.NewGrid())

    s, err := plotter.NewScatter(pts)
    if err != nil {
        log.Fatal(err)
    }
    s.GlyphStyle.Radius = vg.Points(3)

    // Save the plot to a PNG file.
    p.Add(s)
    if err := p.Save(4*vg.Inch, 4*vg.Inch, colName+"_scatter.png"); err != nil {
        log.Fatal(err)
    }
}
```

这将创建以下散点图：

![](img/00030.jpeg)

当我们查看这些散点图时，我们想推断哪些属性（**TV**、**收音机**和/或**报纸**与我们的因变量**销售额**存在线性关系。也就是说，我们是否可以在这些散点图上画一条线，以符合**销售**与各自属性的趋势？这并不总是可能的，对于给定问题，您必须处理的所有属性也不可能都是这样。

在这种情况下，**广播**和**电视**似乎与**销售**有一定的线性相关。**报纸**可能与**销售**略有相关，但相关性并不明显。与**TV**的线性关系似乎最为明显，因此，让我们从**TV**作为线性回归模型的自变量开始。这将使我们的线性回归公式如下：

![](img/00031.jpeg)

这里需要注意的另一件事是，变量**TV**可能不是严格的同方差，这在前面作为线性回归的假设进行了讨论。这是值得注意的（并且可能值得在项目中记录），但我们将继续看看是否可以创建具有一定预测能力的线性回归模型。作为一种可能的解释，如果我们的模型表现不佳，我们总是可以重新考虑这个假设。

# 创建我们的培训和测试集

为了避免过度拟合并确保我们的模型能够推广，我们将数据集分为训练集和测试集，正如[第 3 章](03.html#23MNU0-a5604e9f48ea4e63828d369cca8f0939)、*评估和验证*中所述。我们不必为这里的抵制而烦恼，因为我们只需要通过模型训练一次，而不需要在训练和测试之间来回迭代。但是，如果您正在试验各种因变量和/或迭代调整模型的任何参数，您可能希望创建一个保留集，保存到模型开发过程结束以进行验证。

我们将使用`github.com/kniren/gota/dataframe`创建我们的培训和测试数据集，然后将它们保存到各自的`.csv`文件中。在这种情况下，我们将对培训和测试数据使用 80/20 分割：

```go
// Open the advertising dataset file.        
f, err := os.Open("Advertising.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
// The types of the columns will be inferred.
advertDF := dataframe.ReadCSV(f)

// Calculate the number of elements in each set.
trainingNum := (4 * advertDF.Nrow()) / 5
testNum := advertDF.Nrow() / 5
if trainingNum+testNum < advertDF.Nrow() {
    trainingNum++
}

// Create the subset indices.
trainingIdx := make([]int, trainingNum)
testIdx := make([]int, testNum)

// Enumerate the training indices.
for i := 0; i < trainingNum; i++ {
    trainingIdx[i] = i
}

// Enumerate the test indices.
for i := 0; i < testNum; i++ {
    testIdx[i] = trainingNum + i
}

// Create the subset dataframes.
trainingDF := advertDF.Subset(trainingIdx)
testDF := advertDF.Subset(testIdx)

// Create a map that will be used in writing the data
// to files.
setMap := map[int]dataframe.DataFrame{
    0: trainingDF,
    1: testDF,
}

// Create the respective files.
for idx, setName := range []string{"training.csv", "test.csv"} {

    // Save the filtered dataset file.
    f, err := os.Create(setName)
    if err != nil {
        log.Fatal(err)
    }

    // Create a buffered writer.
    w := bufio.NewWriter(f)

    // Write the dataframe out as a CSV.
    if err := setMap[idx].WriteCSV(w); err != nil {
        log.Fatal(err)
    }
}
```

此代码将输出我们将使用的以下培训和测试集：

```go
$ wc -l *.csv
  201 Advertising.csv
   41 test.csv
  161 training.csv
  403 total
```

我们在这里使用的数据没有以任何方式按数据排序或排序。但是，如果您处理的数据是按响应、日期或任何其他方式排序的，则必须将数据随机拆分为训练集和测试集。若您不这样做，您的训练和测试集可能只包括特定范围的响应，可能会受到时间/日期等的人为影响。

# 训练我们的模型

接下来，我们将实际训练或拟合线性回归模型。如果你还记得的话，这意味着我们正在寻找直线的斜率（*m*和截距（*b*），以最小化平方误差之和。为了执行此培训，我们将使用 Sajari 提供的非常好的软件包：`github.com/sajari/regression`。Sajari 是一家严重依赖 Go 和机器学习的网络搜索公司，他们在生产中使用[github.com/Sajari/Retression](http://github.com/sajari/regression)。

要使用[github.com/sajari/regression](http://github.com/sajari/regression)来训练回归模型，我们需要初始化一个`regression.Regression`值，设置两个标签，并用带标签的训练数据点填充`regression.Regression`值。在此之后，训练我们的线性回归模型就像调用`regression.Regression`值上的`Run()`方法一样简单：

```go
// Open the training dataset file.
f, err := os.Open("training.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)

// Read in all of the CSV records
reader.FieldsPerRecord = 4
trainingData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// In this case we are going to try and model our Sales (y)
// by the TV feature plus an intercept. As such, let's create
// the struct needed to train a model using github.com/sajari/regression.
var r regression.Regression
r.SetObserved("Sales")
r.SetVar(0, "TV")

// Loop of records in the CSV, adding the training data to the regression value.
for i, record := range trainingData {

    // Skip the header.
    if i == 0 {
        continue
    }

    // Parse the Sales regression measure, or "y".
    yVal, err := strconv.ParseFloat(record[3], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the TV value.
    tvVal, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Add these points to the regression value.
    r.Train(regression.DataPoint(yVal, []float64{tvVal}))
}

// Train/fit the regression model.
r.Run()

// Output the trained model parameters.
fmt.Printf("\nRegression Formula:\n%v\n\n", r.Formula)
```

编译并运行此操作将导致经过训练的线性回归公式打印到`stdout`：

```go
$ go build
$ ./myprogram 

Regression Formula:
Predicted = 7.07 + TV*0.05
```

在这里，我们可以看到，包确定了我们的线性回归线，截距为[T0]，斜率为[T1]。我们可以在这里进行一些心理检查，因为我们在散点图中看到了**电视**和**销售**之间的相关性是如何上升和向右的（即正相关）。这意味着公式中的斜率应为正，即。

# 评估训练模型

我们现在需要测量我们模型的性能，看看我们是否真的有能力使用**TV**作为自变量预测**销售**。为此，我们可以加载我们的测试集，使用我们训练的模型对每个测试示例进行预测，然后计算[第 3 章](03.html#23MNU0-a5604e9f48ea4e63828d369cca8f0939)、*评估和验证*中讨论的一个评估指标。

对于这个问题，让我们使用平均绝对误差（MAE）作为评估指标。这似乎是合理的，因为它产生的结果与我们的`Sales`值直接可比，我们不必太担心异常值或极值。

要使用我们训练的`regression.Regression`值计算**销售**预测值，我们只需要解析测试集中的值，并调用`regression.Regression`值上的`Predict()`方法。然后我们将这些预测值与观测值的差值，得到差值的绝对值，然后将所有绝对值相加，得到 MAE：

```go
// Open the test dataset file.
f, err = os.Open("test.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a CSV reader reading from the opened file.
reader = csv.NewReader(f)

// Read in all of the CSV records
reader.FieldsPerRecord = 4
testData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// Loop over the test data predicting y and evaluating the prediction
// with the mean absolute error.
var mAE float64
for i, record := range testData {

    // Skip the header.
    if i == 0 {
        continue
    }

    // Parse the observed Sales, or "y".
    yObserved, err := strconv.ParseFloat(record[3], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the TV value.
    tvVal, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Predict y with our trained model.
    yPredicted, err := r.Predict([]float64{tvVal})

    // Add the to the mean absolute error.
    mAE += math.Abs(yObserved-yPredicted) / float64(len(testData))
}

// Output the MAE to standard out.
fmt.Printf("MAE = %0.2f\n\n", mAE)
```

编译并运行此评估将得到以下结果：

```go
$ go build
$ ./myprogram 

Regression Formula:
Predicted = 7.07 + TV*0.05

MAE = 3.01
```

我们如何知道`MAE = 3.01`是好是坏？这也是为什么拥有一个好的数据心理模型很重要的原因。如果您还记得，我们已经计算了销售的平均值、范围和标准差。平均销售值为`14.02`，标准差为`5.21`。因此，我们的 MAE 小于销售值的标准偏差，约为平均值的 20%，并且我们的模型具有一定的预测能力。

恭喜你！我们建立了第一个具有预测能力的机器学习模型！

为了更好地了解我们的模型是如何运行的，我们还可以创建一个图来帮助我们可视化线性回归线。这可以通过`gonum.org/v1/plot`完成。然而，首先，让我们创建一个 predict 函数，它允许我们在不导入`github.com/sajari/regression`的情况下进行预测。这为我们提供了经过训练的模型的轻量级内存版本：

```go
// predict uses our trained regression model to made a prediction.
func predict(tv float64) float64 {
    return 7.07 + tv*0.05
}
```

然后，我们可以创建回归线的可视化：

```go
// Open the advertising dataset file.
f, err := os.Open("Advertising.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
advertDF := dataframe.ReadCSV(f)

// Extract the target column.
yVals := advertDF.Col("Sales").Float()

// pts will hold the values for plotting.
pts := make(plotter.XYs, advertDF.Nrow())

// ptsPred will hold the predicted values for plotting.
ptsPred := make(plotter.XYs, advertDF.Nrow())

// Fill pts with data.
for i, floatVal := range advertDF.Col("TV").Float() {
    pts[i].X = floatVal
    pts[i].Y = yVals[i]
    ptsPred[i].X = floatVal
    ptsPred[i].Y = predict(floatVal)
}

// Create the plot.
p, err := plot.New()
if err != nil {
    log.Fatal(err)
}
p.X.Label.Text = "TV"
p.Y.Label.Text = "Sales"
p.Add(plotter.NewGrid())

// Add the scatter plot points for the observations.
s, err := plotter.NewScatter(pts)
if err != nil {
    log.Fatal(err)
}
s.GlyphStyle.Radius = vg.Points(3)

// Add the line plot points for the predictions.
l, err := plotter.NewLine(ptsPred)
if err != nil {
    log.Fatal(err)
}
l.LineStyle.Width = vg.Points(1)
l.LineStyle.Dashes = []vg.Length{vg.Points(5), vg.Points(5)}

// Save the plot to a PNG file.
p.Add(s, l)
if err := p.Save(4*vg.Inch, 4*vg.Inch, "regression_line.png"); err != nil {
    log.Fatal(err)
}
```

在编译和运行时，它将生成此绘图：

![](img/00032.gif)

如您所见，我们训练的线性回归线遵循真实数据点的线性趋势。这是我们在正确轨道上的又一次视觉确认！

# 多元线性回归

线性回归不限于仅依赖于一个自变量的简单直线公式。多元线性回归与前面讨论的类似，但这里我们有多个自变量（*x<sub class="calibre25">1</sub>*、*x<sub class="calibre25">2</sub>*等等）。在这种情况下，我们的简单直线方程如下：

![](img/00033.jpeg)

这里，*x*是各种自变量，*m*是与这些自变量相关的各种斜率。我们还有一个拦截，*b*。

多元线性回归更难可视化和思考，因为这不再是一条可以在二维中可视化的直线。它是二维、三维或多维的线性曲面。然而，我们在单线性回归中使用的许多相同技术将继续使用。

多元线性回归与常规线性回归具有相同的假设。然而，我们还应该记住几个陷阱：

*   **过度拟合**：通过向模型中添加越来越多的自变量，我们增加了模型的复杂性，这使我们面临过度拟合的风险。我建议研究一种处理这个问题的技术，称为**正则化**。正则化在模型中创建一个惩罚项，该惩罚项是模型复杂度的函数，有助于控制这种效果。
*   **相对量表**：在某些情况下，你的一个自变量的量表与另一个自变量的量表相差几个数量级。其中较大的可以洗掉较小的影响，你可能需要考虑规范化变量。

考虑到这一点，让我们尝试将我们的**销售**模型从线性回归模型扩展到多元回归模型。回顾上一节的散点图，我们可以看到**电台**似乎也与**销售**呈线性相关，因此我们尝试创建一个多元线性回归模型，如下所示：

![](img/00034.jpeg)

要使用[github.com/sajari/regression](http://github.com/sajari/regression)实现这一点，我们只需要在`regression.Regression`值中标记另一个变量，并确保这些值在训练数据点中配对。然后，我们将运行回归，看看公式是如何得出的：

```go
// Open the training dataset file.
f, err := os.Open("training.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)

// Read in all of the CSV records
reader.FieldsPerRecord = 4
trainingData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// In this case we are going to try and model our Sales
// by the TV and Radio features plus an intercept.
var r regression.Regression
r.SetObserved("Sales")
r.SetVar(0, "TV")
r.SetVar(1, "Radio")

// Loop over the CSV records adding the training data.
for i, record := range trainingData {

    // Skip the header.
    if i == 0 {
        continue
    }

    // Parse the Sales.
    yVal, err := strconv.ParseFloat(record[3], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the TV value.
    tvVal, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the Radio value.
    radioVal, err := strconv.ParseFloat(record[1], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Add these points to the regression value.
    r.Train(regression.DataPoint(yVal, []float64{tvVal, radioVal}))
}

// Train/fit the regression model.
r.Run()

// Output the trained model parameters.
fmt.Printf("\nRegression Formula:\n%v\n\n", r.Formula)
```

编译并运行此函数可以得到以下回归公式：

```go
$ go build
$ ./myprogram

Regression Formula:
Predicted = 2.93 + TV*0.05 + Radio*0.18
```

如您所见，回归公式现在包括一个额外的`Radio`自变量项。截距值也改变了我们以前的单一回归模型。

我们可以使用`Predict`方法对该模型进行类似于单一回归模型的测试：

```go
// Open the test dataset file.
f, err = os.Open("test.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a CSV reader reading from the opened file.
reader = csv.NewReader(f)

// Read in all of the CSV records
reader.FieldsPerRecord = 4
testData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// Loop over the test data predicting y and evaluating the prediction
// with the mean absolute error.
var mAE float64
for i, record := range testData {

    // Skip the header.￼
    if i == 0 {
        continue
    }

    // Parse the Sales.
    yObserved, err := strconv.ParseFloat(record[3], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the TV value.
    tvVal, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the Radio value.
    radioVal, err := strconv.ParseFloat(record[1], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Predict y with our trained model.
    yPredicted, err := r.Predict([]float64{tvVal, radioVal})

    // Add the to the mean absolute error.
    mAE += math.Abs(yObserved-yPredicted) / float64(len(testData))
}

// Output the MAE to standard out.
fmt.Printf("MAE = %0.2f\n\n", mAE)
```

运行此操作将为我们的新多元回归模型揭示以下`MAE`：

```go
$ go build
$ ./myprogram

Regression Formula:
Predicted = 2.93 + TV*0.05 + Radio*0.18

MAE = 1.26
```

我们新的多元回归模型改善了我们的 MAE！现在，根据我们的广告支出，我们肯定处于相当好的状态来预测`Sales`。您还可以尝试在模型中添加`Newspaper`作为后续练习，以了解模型性能是如何受到影响的。

请记住，当您向模型添加更多的复杂性时，您正在牺牲简单性，并且您可能会面临过度拟合的危险，因此，如果模型性能的提高实际上为您的用例创造了更多的价值，那么您应该只添加更多的复杂性。

# 非线性和其他类型的回归

虽然我们在本章中重点介绍了线性回归，但您肯定不限于使用线性公式执行回归。可以通过一个或多个非线性项（如幂、指数或自变量的其他变换）对因变量建模。例如，我们可以通过多项式系列的*TV*术语来模拟*销售*：

![](img/00035.jpeg)

然而，请记住，当你增加这种复杂性时，你又一次将自己置于过度适应的危险之中。

在实现非线性回归方面，您不能使用`github.com/sajari/regression`，它仅限于线性回归。然而，`go-hep.org/x/hep/fit`允许您拟合或训练某些非线性模型，而 Go 社区中的其他人已经或正在开发其他非线性建模工具。

除 OLS 外，还有其他线性回归技术，有助于克服与最小二乘线性回归相关的一些假设和弱点。这些包括**脊回归**和**套索回归**。这两种技术都惩罚回归系数，以减轻自变量的多重共线性和非正态性的影响。

就 Go 实现而言，岭回归在`github.com/berkmancenter/ridge`中实现。与`github.com/sajari/regression`相反，我们的自变量和因变量数据通过 gonum 矩阵输入`github.com/berkmancenter/ridge`。因此，为了说明这种方法，让我们首先形成一个包含我们的广告支出特征（`TV`、`Radio`和`Newspaper`的矩阵，以及一个包含我们的`Sales`数据的矩阵。注意，在`github.com/berkmancenter/ridge`中，如果我们希望在模型中有一个截距，我们需要在输入自变量矩阵中显式添加一列截距。此列中的每个值仅为`1.0`。

```go
// Open the training dataset file.
f, err := os.Open("training.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)
reader.FieldsPerRecord = 4

// Read in all of the CSV records
rawCSVData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// featureData will hold all the float values that will eventually be
// used to form our matrix of features.
featureData := make([]float64, 4*len(rawCSVData))
yData := make([]float64, len(rawCSVData))

// featureIndex and yIndex will track the current index of the matrix values.
var featureIndex int
var yIndex int

// Sequentially move the rows into a slice of floats.
for idx, record := range rawCSVData {

    // Skip the header row.
    if idx == 0 {
        continue
    }

    // Loop over the float columns.
    for i, val := range record {

        // Convert the value to a float.
        valParsed, err := strconv.ParseFloat(val, 64)
        if err != nil {
            log.Fatal(err)
        }

        if i < 3 {

            // Add an intercept to the model.
            if i == 0 {
                featureData[featureIndex] = 1
                featureIndex++
            }

            // Add the float value to the slice of feature floats.
            featureData[featureIndex] = valParsed
            featureIndex++
        }

        if i == 3 {

            // Add the float value to the slice of y floats.
            yData[yIndex] = valParsed
            yIndex++
        }
    }
}

// Form the matrices that will be input to our regression.
features := mat64.NewDense(len(rawCSVData), 4, featureData)
y := mat64.NewVector(len(rawCSVData), yData)
```

接下来，我们用自变量和因变量矩阵创建一个新的`ridge.RidgeRegression`值，并调用`Regress()`方法来训练我们的模型。然后，我们可以打印出经过训练的回归公式：

```go
// Create a new RidgeRegression value, where 1.0 is the
// penalty value.
r := ridge.New(features, y, 1.0)

// Train our regression model.
r.Regress()

// Print our regression formula.
c1 := r.Coefficients.At(0, 0)
c2 := r.Coefficients.At(1, 0)
c3 := r.Coefficients.At(2, 0)
c4 := r.Coefficients.At(3, 0)
fmt.Printf("\nRegression formula:\n")
fmt.Printf("y = %0.3f + %0.3f TV + %0.3f Radio + %0.3f Newspaper\n\n", c1, c2, c3, c4)
```

编译并运行该程序可得出以下回归公式：

```go
$ go build
$ ./myprogram

Regression formula:
y = 3.038 + 0.047 TV + 0.177 Radio + 0.001 Newspaper
```

在这里，你可以看到`TV`和`Radio`的系数与我们用最小二乘回归得到的系数相似，但它们略有不同。另外，请注意，我们继续并为`Newspaper`特性添加了一个术语。

我们可以通过创建自己的`predict`函数来测试此岭回归公式：

```go
// predict uses our trained regression model to made a prediction based on a
// TV, Radio, and Newspaper value.
func predict(tv, radio, newspaper float64) float64 {
        return 3.038 + tv*0.047 + 0.177*radio + 0.001*newspaper
}
```

然后，我们使用此`predict`函数在我们的测试示例上测试我们的岭回归公式：

```go
// Open the test dataset file.
f, err := os.Open("test.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)

// Read in all of the CSV records
reader.FieldsPerRecord = 4
testData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// Loop over the holdout data predicting y and evaluating the prediction
// with the mean absolute error.
var mAE float64
for i, record := range testData {

    // Skip the header.
    if i == 0 {
        continue
    }

    // Parse the Sales.
    yObserved, err := strconv.ParseFloat(record[3], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the TV value.
    tvVal, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the Radio value.
    radioVal, err := strconv.ParseFloat(record[1], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the Newspaper value.
    newspaperVal, err := strconv.ParseFloat(record[2], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Predict y with our trained model.
    yPredicted := predict(tvVal, radioVal, newspaperVal)

    // Add the to the mean absolute error.
    mAE += math.Abs(yObserved-yPredicted) / float64(len(testData))
}

// Output the MAE to standard out.
fmt.Printf("\nMAE = %0.2f\n\n", mAE)
```

编译并运行此文件为我们提供了以下新的`MAE`：

```go
$ go build
$ ./myprogram

MAE = 1.26
```

请注意，向模型中添加[T0]实际上并没有改善我们的[T1]。因此，在这种情况下，这不是一个好主意，因为它会增加更多的复杂性，并且不会对我们的模型性能产生任何重大变化。

您添加到模型中的任何复杂性或复杂度都应该伴随着对此添加的复杂性的可测量的理由。使用一个复杂的模型，因为它在智力上很有趣，这会让人头疼。

# 工具书类

线性回归：

*   普通最小二乘回归直观解释：[http://setosa.io/ev/ordinary-least-squares-regression/](http://setosa.io/ev/ordinary-least-squares-regression/)

*   \12304；T0】文件：\12304；T1]http://godoc.org/github.com/sajari/regression \12304；T2]

多元回归：

*   多元回归可视化：[http://shiny.stat.calpoly.edu/3d_regression/](http://shiny.stat.calpoly.edu/3d_regression/)

非线性回归和其他回归：

*   T0 文件：T1https://godoc.org/go-hep.org/x/hep/fit T2
*   \12304；T0】文件：\12304；T1]https://godoc.org/github.com/berkmancenter/ridge \12304；T2]

# 总结

祝贺您已经正式使用 Go 完成了机器学习。特别是，您已经了解了回归模型，包括线性回归、多元回归、非线性回归和岭回归。您应该能够在 Go 中实现基本线性回归和多元回归。

既然我们已经对机器学习有所了解，我们将在下一章继续讨论分类问题。