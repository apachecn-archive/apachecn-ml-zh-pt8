# 十三、在移动和云平台上部署模型

在本章中，我们将讨论在运行安卓操作系统和**谷歌云平台** ( **GCP** )的移动设备上部署机器学习模型。

在移动设备上使用 C++ 可以让我们更快、更紧凑地编写程序。我们可以利用尽可能多的计算资源，因为现代编译器可以优化与目标 CPU 架构相关的程序。C++ 不使用额外的垃圾收集器进行内存管理，这会对程序性能产生重大影响。程序大小可以减少，因为 C++ 不使用额外的虚拟机，而是直接编译成机器代码。这些事实使 C++ 成为资源有限的移动设备的正确选择，并可用于解决繁重的计算任务。

使用 C++ 实现用于云的机器学习模型可以提供其他好处。正如我们前面提到的，您可以通过为您的特定体系结构编译程序来提高应用的性能；通常，这在数据预处理步骤中起着重要作用。本地应用的启动时间也比可解释程序短得多。如果您使用的是仅由客户端请求启动的云机器，这一事实可以使您的应用更具响应性。这些类型的机器用于降低云服务的成本。

本章将涵盖以下主题:

*   安卓手机上的图像分类
*   云中的机器学习——使用谷歌计算引擎

# 技术要求

以下是本章的技术要求:

*   Android Studio、Android SDK、Android NDK
*   谷歌账户
*   GCP SDK
*   PyTorch 库
*   `cpp-httplib`库
*   支持 C++ 17 的现代 C++ 编译器
*   CMake 构建系统版本> = 3.8

The code files for this chapter can be found at the following GitHub repo: [https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-CPP/tree/master/Chapter13](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-CPP/tree/master/Chapter13)

# 安卓手机上的图像分类

关于如何将机器学习模型部署到安卓移动设备，有两种流行的方法。我们可以使用 PyTorch 框架，它现在包含了 Caffe2，也可以使用 TensorFlow Lite 框架。我们将在本章中使用 PyTorch 框架，因为我们在前面几章中已经讨论过了。

# PyTorch 框架的移动版本

没有可用于移动设备的 PyTorch 二进制发行版，因此我们需要从源代码构建它。我们可以用与编译其常规版本相同的方式来实现这一点，但需要额外的 CMake 参数来启用移动模式。您还必须安装 Android **原生开发工具包** ( **NDK** )，其中包括适当版本的 C/C++ 编译器和构建应用所需的 Android 原生库。下面的代码片段显示了如何使用命令行环境来签出 PyTorch 并构建其安卓移动版本:

```cpp
cd /home/[USER]
git clone https://github.com/pytorch/pytorch.git
cd pytorch/
git checkout v1.2.0
git submodule update --init
export ANDROID_NDK=/home/[USER]/Android/Sdk/ndk/20.0.5594570
export ANDROID_ABI='armeabi-v7a'

/home/[USER]/pytorch/scripts/build_android.sh \
-DBUILD_CAFFE2_MOBILE=OFF \
-DBUILD_SHARED_LIBS=ON \
-DCMAKE_PREFIX_PATH=$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())') \
-DPYTHON_EXECUTABLE=$(python -c 'import sys; print(sys.executable)') \
```

这里我们假设`/home/[USER]`是用户的主目录。构建移动版 PyTorch 的主要要求是声明`ANDROID_NDK`环境变量，应该指向安卓 NDK 安装目录。安装安卓开发工具最简单的方法是下载安卓工作室集成开发环境，并从中使用软件开发工具包管理器工具。您可以在**工具** | **软件开发工具包管理器**菜单下找到软件开发工具包管理器。您可以使用此管理器安装适当的 Android SDK 版本。您可以使用管理器窗口中的**软件开发工具包工具**选项卡安装相应的软件开发工具包。您也可以使用此选项卡安装 CMake 实用程序。`ANDROID_ABI`环境变量可用于指定 ARM CPU 架构的兼容性，以便编译器生成特定于架构的代码。在这个例子中，我们使用了`armeabi-v7a`架构。

我们使用来自 PyTorch 源代码发行版的`build_android.sh`脚本来构建移动 PyTorch 二进制文件。这个脚本在内部使用了 CMake 命令，这就是它将 CMake 参数定义作为参数的原因。请注意，我们传递了`BUILD_CAFFE2_MOBILE=OFF`参数来禁用构建移动版本的 Caffe2，这在当前版本中很难使用，因为该库已被弃用。我们使用的第二个重要参数是`BUILD_SHARED_LIBS=ON`，它使我们能够构建共享库。我们这样做是因为移动 PyTorch 1.2 库的静态版本现在不能使用，因为初始化函数被破坏了。配置的其他参数是用于中间构建代码生成的 Python 安装路径。

现在我们有了移动 PyTorch 库，也就是`libc10.so`和`libtorch.so`，就可以开始开发应用了。我们将基于 ResNet-18 神经网络架构构建一个简单的图像分类应用。在 ResNet 网络系列中，这种架构的参数数量最少，因此我们可以在内存量较低的设备上使用它。

# 对模型快照使用 TorchScript

在本节中，我们将讨论如何获取模型快照文件，以便在我们的移动应用中使用它。在前面的章节中，我们讨论了如何保存和加载模型参数，以及如何使用 ONNX 格式在框架之间共享模型。当我们使用 PyTorch 框架时，还有另一种方法可以用来在 Python API 和 C++ API 之间共享模型，称为 TorchScript。

该方法使用实时模型跟踪来获得一种特殊类型的模型定义，该定义可以由 PyTorch 引擎执行，而不考虑 API。对于 PyTorch 1.2，只有 Python API 可以创建这样的定义，但是我们可以使用 C++ API 来加载模型并执行它。此外，PyTorch 框架的移动版本仍然不允许我们用功能齐全的 C++ API 编程神经网络；只有 ATen 库可用。

因此，在这个例子中，我们将使用 TorchScript 模型来执行图像分类。为了得到这个模型，我们需要使用 Python API 来加载预先训练好的模型，跟踪它，并保存模型快照。下面的代码展示了如何使用 Python 实现这一点:

```cpp
import torch
import urllib
from PIL import Image
from torchvision import transforms

# Download pretrained model
model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)
model.eval()

# Download an example image from the pytorch website
url, filename = ("https://github.com/pytorch/hub/raw/master/dog.jpg", "dog.jpg")

try:
    urllib.URLopener().retrieve(url, filename)
except:
    urllib.request.urlretrieve(url, filename)

# sample execution
input_image = Image.open(filename)
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 
    0.225]),
])
input_tensor = preprocess(input_image)

# create a mini-batch as expected by the model
input_batch = input_tensor.unsqueeze(0) 

traced_script_module = torch.jit.trace(model, input_batch)

traced_script_module.save("model.pt")
```

在这个编程示例中，我们执行了以下步骤:

1.  我们下载了一个带有`torch.hub.load()`功能的预训练模型。
2.  然后，我们用`urllib`模块下载了一个输入图像。
3.  获取输入图像后，我们使用`PIL`库对其进行调整大小和归一化。
4.  使用`unsqueeze()`函数，我们向输入张量添加了批次大小维度。
5.  然后，我们使用`torch.jit.trace()`函数运行加载的模型，并将其跟踪到脚本中。
6.  最后，我们简单地用`save()`方法将`script`模块保存到一个文件中。

现在我们已经保存了脚本模块，我们可以开始创建一个安卓应用，它将用于图像分类。

# 安卓工作室项目

在本节中，我们将使用安卓工作室集成开发环境来创建我们的移动应用。我们可以在 Android Studio IDE 中使用默认的**原生 C++** 向导来创建一个应用存根。安卓工作室将创建一个特定的项目结构；以下示例显示了它最有价值的部分:

```cpp
app
  |
  src
    |
    main
       cpp
         |
         CMakeLists.txt
         native-lib.cpp
       java
         |
         com
           |
           example
                 |
                 Camera2
                       |
                       MainActivity.java
       res
         |
         layout
              |
              activity_main.xml
         values
              |
              colors.xml
              strings.xml
              styles.xml
         ...
  build.gradle 
  ...
build.gradle
local.properties
...
```

`cpp`文件夹包含整个项目的 C++ 部分。在这个项目中，安卓工作室集成开发环境将 C++ 部分创建为一个本地共享库项目，该项目已经用 CMake 构建生成系统进行了配置。`java`文件夹包含项目的 Java 部分。在我们的例子中，它是一个定义主活动的文件——这个对象被用作用户界面元素和事件处理程序之间的连接。`res`文件夹包含项目资源，如用户界面元素和字符串定义。

我们还需要在`main`文件夹下创建`jniLibs`文件夹，其结构如下:

```cpp
app
  |
  src
    |
    main
       |
       ...
       jniLibs
             |
             armeabi-v7a
                       |
                       libc10.so
                       libtorch.so
             x86
               |
               ...
```

安卓工作室要求我们在这样的文件夹中放置额外的本机库，以正确地将它们打包到最终的应用中。它还允许 JNI 系统能够找到这些图书馆。请注意，我们将 PyTorch 库放在`armeabi-v7a`文件夹中，因为它们只是为这个 CPU 架构编译的。如果您有其他架构的库，您必须创建具有相应名称的文件夹。

IDE 使用 Gradle 构建系统进行项目配置，因此有两个名为`build.gradle`的文件，其中包含主设置，`local.properties`包含用户自定义值。

现在，我们可以根据自己的目的调整默认用户界面。我们将在下一节学习如何做到这一点。

# 项目的用户界面和 Java 部分

我们的应用将具有以下用户界面:

![](img/8150beae-261f-4140-87bf-3989ec05836d.png)

它有一个按钮，我们可以使用它来启动一个本机相机应用来拍摄照片，一个文本视图来显示图像分类描述，一个图像视图来显示照片。我们应该在项目的`activity_main.xml`文件中定义这些 UI 元素。下面的代码片段显示了这个文件:

```cpp
<?xml version="1.0" encoding="utf-8"?>
<RelativeLayout xmlns:android="http://schemas.android.com/apk/res/android"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:paddingLeft="10dp"
    android:paddingRight="10dp">
<TextView
    android:text="@string/btn_name"
    android:textStyle="bold"
    android:id="@+id/textViewClass"
    android:layout_width="wrap_content"
    android:layout_height="wrap_content"
    android:layout_centerHorizontal="true"
    android:layout_above="@+id/btnTakePicture"/>
<Button
    android:id="@+id/btnTakePicture"
    android:layout_width="wrap_content"
    android:layout_height="wrap_content"
    android:text="@string/btn_name"
    android:textStyle="bold"
    android:layout_centerHorizontal="true"
    android:layout_alignParentBottom="true" />
<ImageView
    android:layout_width="fill_parent"
    android:layout_height="fill_parent"
    android:id="@+id/capturedImage"
    android:layout_above="@+id/textViewClass"
    android:contentDescription="@string/img_desc"/>
</RelativeLayout>
```

我们还应该为`strings.xml`文件中的 UI 元素定义文本标题，你可以在`res`文件夹中找到。下面的代码片段显示了这个文件的一个有趣的部分:

```cpp
<resources>
    <string name="app_name">Camera2</string>
    <string name="btn_name">Take a photo</string>
    <string name="img_desc">Photo</string>
</resources>
```

现在已经定义了 UI 元素，我们可以将它们连接到`MainActivity`类中的事件处理程序，以使我们的应用响应用户的动作。下面的代码示例显示了我们如何修改`MainActivity`类，使其适合我们的需求:

```cpp
public class MainActivity extends AppCompatActivity {
    private ImageView imgCapture;
    private static final int Image_Capture_Code = 1;
    ...
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        initClassifier(getAssets());

        setContentView(R.layout.activity_main);
        imgCapture = findViewById(R.id.capturedImage);
        Button btnCapture = findViewById(R.id.btnTakePicture);
        btnCapture.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Intent cInt = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
                startActivityForResult(cInt, Image_Capture_Code);
            }
        });
    }
    ...
}
```

这里，我们向`MainActivity`类添加了对`ImageView`元素和`imgCapture`成员值的引用。我们还定义了`Image_Capture_Code`值，以便识别对应于用户图像分类请求的活动事件。

我们在`MainActivity`类的`onCreate()`方法中连接了用户界面元素和它们的事件处理程序。在这个方法中，我们通过调用`setContentView()`方法并将`Main Activity` XML 定义的标识符传递给它来定义用户界面元素的布局。然后，我们保存了对`ImageView`元素和`imgCapture`变量的引用。`findViewById()`方法用于从活动布局中获取用户界面元素的对象引用。同样，我们引用了 button 元素。使用按钮元素的`setOnClickListener()`方法，我们定义了按钮点击事件的事件处理程序。这个事件处理程序是`OnClickListener`类实例，我们在其中重写了`onClick()`方法。我们要求安卓系统通过在`onClick()`方法中实例化带有`MediaStore.ACTION_IMAGE_CAPTURE`参数的`Intent`类对象，用默认的相机应用捕捉照片。

我们识别了用我们之前定义的`Image_Capture_Code`代码捕获的图像，并将其传递给带有意图类对象`cInt`的`startActivityForResult()`方法。`startActivityForResult()`方法启动图像捕获软件，然后将结果传递给我们的活动对象的`onActivityResult`事件处理程序。下面的代码显示了它的实现:

```cpp
    @Override
    protected void onActivityResult(int requestCode, int resultCode, 
      Intent data) {
        if (requestCode == Image_Capture_Code) {
            if (resultCode == RESULT_OK) {
                Bitmap bp = (Bitmap) Objects.requireNonNull(
                                        data.getExtras()).get("data");
                if (bp != null) {
                    Bitmap argb_bp = bp.copy(Bitmap.Config.ARGB_8888, 
                                             true);
                    if (argb_bp != null) {
                        float ratio_w = (float) bp.getWidth() / (float) 
                                            bp.getHeight();
                        float ratio_h = (float) bp.getHeight() / (float) 
                                            bp.getWidth();

                        int width = 224;
                        int height = 224;

                        int new_width = Math.max((int) (height * ratio_w), 
                                                        width);
                        int new_height = Math.max(height, (int) (width * 
                                                        ratio_h));

                        Bitmap resized_bitmap = 
                                Bitmap.createScaledBitmap(argb_bp, 
                                new_width, new_height, false);
                        Bitmap cropped_bitmap = 
                                Bitmap.createBitmap(resized_bitmap, 0, 0, 
                                width, height);

                        int[] pixels = new int[width * height];
                        cropped_bitmap.getPixels(pixels, 0, width, 0, 0, 
                                                 width, height);
                        String class_name = classifyBitmap(pixels, width, 
                                                height);

                        imgCapture.setImageBitmap(cropped_bitmap);

                        TextView class_view = 
                                        findViewById(R.id.textViewClass);
                        class_view.setText(class_name);
                    }
                }
            } else if (resultCode == RESULT_CANCELED) {
                Toast.makeText(this, "Cancelled", 
                                Toast.LENGTH_LONG).show();
            }
        }
    }
```

`onActivityResult()`方法处理应用的结果。`Activity`当用户在主应用视图上按下按钮后创建照片时，调用此方法。在这个方法的第一行，我们检查了我们有`Image_Capture_Code`代码，它标识`Intent`包含位图。我们还通过比较`resultCode`和预定义的`RESULT_OK`值来检查是否有任何错误。然后，通过访问由`getExtras`方法返回的`Bundle`对象的`data`字段，我们从`Intent`数据对象中获得了一个`Bitmap`对象。如果`Bitmap`对象不为空，我们用`Bitmap`对象的`copy()`方法将其转换为 ARGB 格式；`Bitmap.Config.ARGB_8888`参数指定所需的格式。根据 ResNet 体系结构的要求，获取的`Bitmap`对象被缩放和裁剪为`224x224`。安卓框架中的`Bitmap`类已经有了一个名为`createScaledBitmap`的位图缩放方法。我们还使用了`createBitmap()`方法来裁剪原始图像，因为`createScaledBitmap()`方法根据捕获的图像创建了一个新的位图，但新的尺寸作为参数传递。我们进行了图像尺寸调整，保留了原来的宽高比，因为其中一个尺寸可以大于`224`；这就是为什么我们使用裁剪来制作最终的图像。

`Bitmap getPixels()`方法用于从`Bitmap`对象获取原始颜色值。该方法用`Int`类型的颜色值填充数组。该数组中的每个 *4* 字节代表一个颜色分量，最高的字节是`Alpha`值，最低的字节代表`Blue`值。该方法以主要行格式填充颜色值。然后，像素值被传递到本机库进行分类；详见`classifyBitmap()`方法调用。当原生库完成分类时，我们通过使用`setImageBitmap()`方法调用将其传递到`ImageView`对象来显示用于分类的裁剪图像。我们还通过调用`setText`方法在`TextField`对象中显示了分类文本。

有两种方法，`classifyBitmap`和`initClassifier`，它们是 JNI 对用 C++ 实现的本机库函数的调用。为了用 Java 代码连接本机库，我们使用了 **Java 本机接口** ( **JNI** )。这是一种标准机制，用于从 Java 调用 C/C++ 函数。首先，我们必须用`system.LoadLibrary`调用加载本机库。然后，我们必须通过将方法声明为`public native`来定义在本机库中实现的方法。下面的代码片段展示了如何在 Java 中定义这些方法:

```cpp
public class MainActivity extends AppCompatActivity {
...
static {
    System.loadLibrary("native-lib");
}
public native String classifyBitmap(int[] pixels, int width, int height);
public native void initClassifier(AssetManager assetManager);
...
}
```

请注意，我们在`onCreate()`方法中调用了`initClassifier()`方法，并将其传递到`AssetManager`对象中，该对象由`getAssets Activity()`方法返回。`AssetManager`管理器对象允许我们读取资产，例如打包到安卓 APK 应用捆绑包中的数据文件。要将资产添加到安卓应用，您必须在项目的`main`文件夹中创建`assets`文件夹，并将所需文件放在那里。对于这个例子，我们使用`model.pt`和`synset.txt`文件作为应用资产。`model.pt`文件是 TorchScript 模型快照，我们用来分类。另一方面，`synset.txt`文件包含分类描述的有序列表；它们的顺序与用于模型训练的类索引的顺序相同。可以从[https://github . com/onnx/models/blob/master/vision/classing/synset . txt](https://github.com/onnx/models/blob/master/vision/classification/synset.txt)下载该文件。

在下一节中，我们将讨论项目的 C++ 部分。

# 项目的 C++ 本地部分

我们在本机 C++ 库中执行主要的分类任务。安卓工作室 IDE 已经为我们创建了`native-lib.cpp`文件，所以我们只需要修改它。下面的代码片段显示了为了使用 JNI、PyTorch 和安卓资产库，我们应该包含哪些头文件:

```cpp
#include <jni.h>
#include <string>
#include <iostream>

#include <torch/script.h>
#include <caffe2/serialize/read_adapter_interface.h>

#include <android/asset_manager_jni.h>
#include <android/asset_manager.h>
```

如果想用安卓日志系统输出一些消息到 IDE 的`logcat`，可以定义如下宏，使用`__android_log_print()`功能:

```cpp
#include <android/log.h>

#define  LOGD(...)  __android_log_print(ANDROID_LOG_DEBUG, "CAMERA_TAG", __VA_ARGS__)
```

我们在 Java 代码中使用的第一个本机函数是`initClassifier()`函数。为了在 C++ 代码中实现它并使它在 Java 代码中可见，我们必须遵循 JNI 规则来使函数声明正确。函数的名称应该包括完整的 Java 包名，包括名称空间，我们的前两个必需参数应该是`JNIEnv*`和`jobject`类型。下面的代码显示了如何定义这个函数:

```cpp
extern "C" JNIEXPORT void JNICALL
Java_com_example_camera2_MainActivity_initClassifier(
                    JNIEnv *env, jobject /*self*/, jobject j_asset_manager) {
    AAssetManager *asset_manager = AAssetManager_fromJava(env, j_asset_manager);
    if (asset_manager != nullptr) {
        LOGD("initClassifier start OK");

        auto model = ReadAsset(asset_manager, "model.pt");
        if (!model.empty()) {
            g_image_classifier.InitModel(model);
        }

        auto synset = ReadAsset(asset_manager, "synset.txt");
        if (!synset.empty()) {
            VectorStreamBuf<char> stream_buf(synset);
            std::istream is(&stream_buf);
            g_image_classifier.InitSynset(is);
        }
        LOGD("initClassifier finish OK");
    }
}
```

`initClassifier()`函数初始化`g_image_classifier`全局对象，该对象属于`ImageClassifier`类型。在我们的应用中，我们使用这个对象来执行图像分类。我们初始化了两个主要的实体来使这个对象按预期工作。第一个是快照中的模型初始化，第二个是类描述，我们从 synset 文件中加载。正如我们之前看到的，synset 和 model snapshot 文件作为资产附加到我们的应用，因此为了访问它们，我们使用了对应用的`AssetManager`对象的引用(或指针)。当我们从 Java 代码中调用这个函数时，我们将 Java 引用作为函数的参数传递给了`AssetManager`对象。在 C/C++ 代码中，我们使用`AAssetManager_fromJava()`函数将 Java 引用转换为 C++ 指针。然后，我们使用`ReadAsset()`函数从应用捆绑包中读取资产作为`std::vector<char>`对象。我们的`ImageClassifier`类有`InitModel()`和`InitSynset()`方法读取相应的实体。

下面的代码显示了`ReadAsset()`函数的实现:

```cpp
std::vector<char> ReadAsset(AAssetManager *asset_manager, const std::string &name) {
    std::vector<char> buf;
    AAsset *asset = AAssetManager_open(asset_manager, name.c_str(), 
    AASSET_MODE_UNKNOWN);
    if (asset != nullptr) {
        LOGD("Open asset %s OK", name.c_str());
        off_t buf_size = AAsset_getLength(asset);
        buf.resize(buf_size + 1, 0);
        auto num_read = AAsset_read(asset, buf.data(), buf_size);
        LOGD("Read asset %s OK", name.c_str());

        if (num_read == 0)
            buf.clear();
        AAsset_close(asset);
        LOGD("Close asset %s OK", name.c_str());
    }
    return buf;
}
```

有四个安卓框架函数，我们用来从应用捆绑包中读取资产。`AAssetManager_open()`函数打开资产并返回指向`AAsset`对象的非空指针。该函数假设资源的路径是文件路径格式，并且该路径的根是`assets`文件夹。打开资产后，我们使用`AAsset_getLength()`函数获取文件大小，并使用`std::vector::resize()`方法为`std::vector<char>`分配内存。然后，我们使用`AAsset_read()`功能将整个文件读取到`buf`对象。

该功能执行以下操作:

*   获取指向要读取的资产对象的指针
*   指向要读入的内存缓冲区的`void*`指针
*   测量要读取的字节大小。

如您所见，资产应用编程接口与用于文件操作的标准 C 库应用编程接口非常相似。当我们处理完资产对象后，我们使用`AAsset_close()`通知系统我们不再需要访问该资产。如果您的资产是`.zip`存档格式，您应该检查`AAsset_read()`函数返回的字节数，因为安卓框架逐块读取存档。

您可能已经注意到，我们使用`VectorStreamBuf`适配器将数据传递给`ImageClassifier::InitSynset()`方法。这种方法采用的是`std::istream`类型的物体。为了将`std::vector<char>`转换为`std::istream`类型的对象，我们开发了以下适配器类:

```cpp
template<typename CharT, typename TraitsT = std::char_traits<CharT> >
struct VectorStreamBuf : public std::basic_streambuf<CharT, TraitsT> {
    explicit VectorStreamBuf(std::vector<CharT> &vec) {
        this->setg(vec.data(), vec.data(), vec.data() + vec.size());
    }
};
```

下面的代码显示了`ImageClassifier`类的声明:

```cpp
class ImageClassifier {
  public:
    using Classes = std::map<size_t, std::string>;

    ImageClassifier() = default;

    void InitSynset(std::istream &stream);

    void InitModel(const std::vector<char> &buf);

    std::string Classify(const at::Tensor &image);

  private:
    Classes classes_;
    torch::jit::script::Module model_;
};
```

我们在`native-lib.cpp`文件的开头以如下方式声明了这个类的全局对象:

```cpp
ImageClassifier g_image_classifier;
```

下面的代码展示了`InitSynset()`方法的实现:

```cpp
void ImageClassifier::InitSynset(std::istream &stream) {
    LOGD("Init synset start OK");
    classes_.clear();
    if (stream) {
        std::string line;
        std::string id;
        std::string label;
        std::string token;
        size_t idx = 1;
        while (std::getline(stream, line)) {
            auto pos = line.find_first_of(" ");
            id = line.substr(0, pos);
            label = line.substr(pos + 1);
            classes_.insert({idx, label});
            ++ idx;
        }
    }
    LOGD("Init synset finish OK");
}
```

synset 文件中的行采用以下格式:

```cpp
[ID] space character [Description text] 
```

因此，我们逐行读取这个文件，并在第一个空格字符的位置拆分每一行。每行的第一部分是类标识符，第二部分是类描述。这个文件中的所有类都是有序的，所以行号是用于训练模型的类号。因此，为了将模型的评估结果与正确的类描述相匹配，我们创建了字典(map)对象，其中`key`是行号，`value`是类描述。`InitSynset()`函数将`std::istream`作为一个参数，因为我们将对其他示例使用相同的代码，其中我们使用标准的 C++ Streams API 读取 synset 文件。

下面的代码展示了`ImageClassifier::InitModel()`方法的实现:

```cpp
void ImageClassifier::InitModel(const std::vector<char> &buf) {
    model_ = torch::jit::load(std::make_unique<ModelReader>(buf), 
    at::kCPU);
}
```

这里，我们简单地使用一个函数调用来加载 TorchScript 模型快照。`torch::jit::load()`功能为我们做了所有的辛苦工作；它加载模型并用权重初始化它，这些权重也保存在快照文件中。这个函数的主要困难是从内存缓冲区读取模型快照，就像我们的例子一样。`torch::jit::load()`函数不适用于标准的 C++ 流和类型；相反，它接受指向`caffe2::serialize::ReadAdapterInterface`类对象的指针。下面的代码展示了如何具体实现包装`std::vector<char>`对象的`caffe2::serialize::ReadAdapterInterface`类:

```cpp
class ModelReader : public caffe2::serialize::ReadAdapterInterface {
    public:
    explicit ModelReader(const std::vector<char> &buf) : buf_(&buf) {}

    ~ModelReader() override {};

    virtual size_t size() const override {
        return buf_->size();
    }

    virtual size_t read(uint64_t pos, void *buf, size_t n, 
                        const char *what)
    const override {
        std::copy_n(buf_->begin() + pos, n, reinterpret_cast<char *>(buf));
        return n;
    }

    private:
    const std::vector<char> *buf_;
};
```

`ModelReader`类覆盖了来自`caffe2::serialize::ReadAdapterInterface`基类的两个方法`size()`和`read()`。它们的实现非常明显:`size()`方法返回底层向量对象的大小，而`read()`方法使用标准算法函数，即`std::copy_n`，将向量中的 *n* 字节(字符)复制到目标缓冲区。

`ImageClassifier`类的主要目的是执行图像分类。下面的代码展示了这个类的目标方法的实现，即`Classify()`:

```cpp
std::string ImageClassifier::Classify(const at::Tensor &image) {
    std::vector<torch::jit::IValue> inputs;
    inputs.emplace_back(image);
    at::Tensor output = model_.forward(inputs).toTensor();

    LOGD("Output size %d %d %d", static_cast<int>(output.ndimension()),
         static_cast<int>(output.size(0)),
         static_cast<int>(output.size(1)));

    auto max_result = output.squeeze().max(0);
    auto max_index = std::get<1>(max_result).item<int64_t>();
    auto max_value = std::get<0>(max_result).item<float>();

    max_index += 1;

    return std::to_string(max_index) + " - " + std::to_string(max_value) + 
        " - " + classes_[static_cast<size_t>(max_index)];
}
```

该函数将包含图像数据的`at::Tensor`对象作为输入参数。我们使用了`torch::jit::script::Module`类的`forward()`方法来评估输入图像上的模型。注意`forward()`方法获取`torch::jit::IValue`对象的向量。有一个从`at::Tensor`类型到`torch::jit::IValue`类型的隐式转换，这意味着我们可以透明地使用 ATen 库的张量对象。模型的输出是一个`1x1000`维张量，其中每个值都是类得分。为了确定给定图像的最可能类别，我们使用`at::Tensor::max()`方法寻找具有最大值的列。前面的`squeeze()`方法去掉了第一维，使张量为一维。`at::Tensor::max()`方法返回一对值；第一个是实际最大值，第二个是它的索引。我们增加了我们得到的类的索引，因为我们在`InitSynset()`函数中有相同的增量。然后，我们用这个索引在`classes_`映射中找到类描述，这个类描述是我们在`InitSynset()`方法中找到的，是从`initClassifier()`函数中调用的。

我们需要实现的最后一个 JNI 函数是`classifyBitmap()`。下面的代码显示了我们如何声明它:

```cpp
extern "C" JNIEXPORT jstring JNICALL
Java_com_example_camera2_MainActivity_classifyBitmap(
JNIEnv *env, jobject /*self*/, jintArray pixels, jint width, jint height) {
...
}
```

该函数采用三个参数:`pixels`对象及其宽度和高度尺寸。`pixels`对象是对 Java `int[]`数组类型的引用，所以我们必须将其转换为 C/C++ 数组才能处理它。下面的代码展示了我们如何提取不同的颜色，并将它们放入不同的缓冲区:

```cpp
jboolean is_copy = 0;
jint *pixels_buf = env->GetIntArrayElements(pixels, &is_copy);

auto channel_size = static_cast<size_t>(width * height);
using ChannelData = std::vector<float>;
size_t channels_num = 3; // RGB image
std::vector<ChannelData> image_data(channels_num);
for (size_t i = 0; i < channels_num; ++ i) {
    image_data[i].resize(channel_size);
}

// split original image
for (int y = 0; y < height; ++ y) {
    for (int x = 0; x < width; ++ x) {
        auto pos = x + y * width;
        auto pixel_color = static_cast<uint32_t>(pixels_buf[pos]); 
                            // ARGB format
        uint32_t mask{0x000000FF};

        for (size_t i = 0; i < channels_num; ++ i) {
            uint32_t shift = i * 8;
            uint32_t channel_value = (pixel_color >> shift) & mask;
            image_data[channels_num - (i + 1)][pos] = 
                                     static_cast<float>(channel_value);
        }
    }
}

env->ReleaseIntArrayElements(pixels, pixels_buf, 0);
```

JNIEnv 的`GetIntArrayElements()`方法返回了指向`jint`数组元素的指针，其中`jint`类型实际上是常规的 C/C++ `int`类型。指针指向手边的图像像素数据，我们对其进行处理。我们将每个颜色值分成几个部分，因为我们需要分别标准化每个颜色通道。

我们定义了`std::vector<ChannelData>`类型的`image_data`对象来保存颜色通道的数据。每个通道对象都是`ChannelData`类型，下面是`std::vector<float>`。通道数据是通过逐行迭代图像像素并将每个像素颜色分成多个分量来填充的。我们通过将颜色值向右移动 8 位三次来获得每个颜色分量，该颜色值属于`int`类型。我们不需要阿尔法颜色成分；这就是为什么我们只做了三次轮班。移位后，我们通过应用带有`0x000000FF`掩码值的`AND`运算符来提取精确的分量值。我们还将颜色值转换为浮点类型，因为我们需要稍后在`[0,1]`范围内的值，并且我们需要对它们进行规范化。处理完像素值后，我们用`JNIEnv`对象的`ReleaseIntArrayElements()`方法释放数据指针。

现在我们已经从像素数据中提取了颜色通道，我们必须从它们创建张量对象。使用张量对象允许我们执行计算效率更高的矢量化计算。下面的代码片段显示了如何从浮点向量创建`at::Tensor`对象:

```cpp
std::vector<int64_t> channel_dims = {height, width};

std::vector<at::Tensor> channel_tensor;
at::TensorOptions options(at::kFloat);
options = options.device(at::kCPU).requires_grad(false);

for (size_t i = 0; i < channels_num; ++ i) {
    channel_tensor.emplace_back(
        torch::from_blob(image_data[i].data(), 
                         at::IntArrayRef(channel_dims),
                         options).clone());
}
```

请注意，我们在`at::TensorOptions`中指定了`at::kFloat`类型，以使其与我们的浮点通道向量兼容。我们还使用了`torch::from_blob()`函数从原始阵列数据制作张量对象；我们在前面的章节中使用了这个函数。简单地说，我们初始化了`channel_tensor`向量，它包含三个张量，每个张量都有颜色通道的值。

我们使用的 ResNet 模型要求我们标准化输入图像；也就是说，我们应该从每个通道中减去一个不同的预定义平均值，并用一个不同的预定义标准偏差值将其除。下面的代码展示了我们如何在`channel_tensor`容器中标准化颜色通道:

```cpp
std::vector<float> mean{0.485f, 0.456f, 0.406f};
std::vector<float> stddev{0.229f, 0.224f, 0.225f};

for (size_t i = 0; i < channels_num; ++ i) {
    channel_tensor[i] = ((channel_tensor[i] / 255.0f) - mean[i]) / stddev[i];
}
```

在我们标准化了每个通道之后，我们必须用它们制作一个张量来满足 ResNet 模型的要求。下面的代码展示了如何使用`stack()`功能组合频道:

```cpp
auto image_tensor = at::stack(channel_tensor);
image_tensor = image_tensor.unsqueeze(0);
```

`stack()`函数也给新的张量增加了一个新的维度。这个新张量的维度成为`3 x height x width`。

该模型的另一个要求是，它需要输入图像张量的批量尺寸。我们使用张量的`unsqueeze()`方法给张量增加一个新的维度，这样它的维度就变成了`1 x 3 x height x width`。

下面的代码显示了`classifyBitmap()`功能的最后一部分:

```cpp
std::string result = g_image_classifier.Classify(image_tensor);

return env->NewStringUTF(result.c_str());
```

这里，我们调用全局`g_image_classifier`对象的`Classify()`方法，在准备好的张量上评估加载的模型，该张量包含捕获的图像。然后，我们通过调用`JNIEnv`类型对象的`NewStringUTF()`方法，将获得的分类字符串转换为一个 Java `String`对象。正如我们前面提到的，应用的 Java 部分将在`onActivityResult()`方法中向用户显示这个字符串。

在这一节中，我们看了安卓系统的图像分类应用的实现。我们学习了如何将预训练模型从 Python 程序导出为 PyTorch 脚本文件。然后，我们深入研究了使用 Android Studio IDE 和 PyTorch C++ 库的移动版本开发移动应用。

在下一节中，我们将讨论并部署一个图像分类应用到谷歌计算引擎平台。

# 云中的机器学习——使用谷歌计算引擎

通常，在开发环境中实现应用后，我们需要将其部署到客户方的生产环境或 can 服务平台。服务变得非常受欢迎，因为您可以根据客户的需求配置计算环境，并在成本、可扩展性和性能之间实现出色的平衡。此外，此类服务的使用消除了您的客户维护他们正在使用的硬件设备的需要。

因此，让我们学习如何将一个简单的图像分类应用部署到谷歌计算引擎平台。最初，我们需要在开发环境中开发和测试这样的应用。我们将制作一个 HTTP 服务，用以多部分格式编码的图像数据来响应`POST`请求。让我们从实现服务器开始。

# 服务器

我们应用的核心是服务器。让我们假设我们已经以与上一节相同的方式实现了图像分类，也就是说，通过使用保存为 TorchScript 快照并加载到`torch::jit::script::Module`对象中的模型。我们将该功能封装在以下类中:

```cpp
class Network {
  public:
    Network(const std::string& snapshot_path,
            const std::string& synset_path,
            torch::DeviceType device_type);

    std::string Classify(const at::Tensor& image);

  private:
    torch::DeviceType device_type_;
    Classes classes_;
    torch::jit::script::Module model_;
};
```

下面的代码显示了我们应用的主例程的实现:

```cpp
#include <torch/script.h>
#include "network.h"
#include "third-party/httplib/httplib.h"
#include "utils.h"

int main(int argc, char** argv) {
    try {
        std::string snapshoot_path;
        std::string synset_path;
        std::string www_path;
        std::string host = "localhost";
        int port = 8080;

        if (argc >= 4) {
            snapshoot_path = argv[1];
            synset_path = argv[2];
            www_path = argv[3];
            if (argc >= 5)
                host = argv[4];
            if (argc >= 6)
                port = std::stoi(argv[5]);

            torch::DeviceType device_type = torch::cuda::is_available()
                                                    ? torch::DeviceType::CUDA
                                                    : torch::DeviceType::CPU;

            Network network(snapshoot_path, synset_path, device_type);
            ...
            // HTTP service implementation
            ...
        } else {
            std::cout << "usage: " << argv[0]
            << " <model snapshoot path> <synset file path> 
                <www dir=../../client> "
            "[host=localhost] [port=8080]\n";
        }
    } catch (const std::exception& err) {
        std::cerr << err.what();
    } catch (...) {
        std::cerr << "Unhandled exception";
    }
    return 1;
}
```

这里，我们在启动时读取应用所需的参数。有三个必需的参数:模型快照文件的路径、synset 文件的路径以及我们放置 HTML 客户端应用文件的目录的路径。还有两个可选参数:服务器主机 IP 地址和服务器网络端口。

读取程序参数后，我们可以用指定的模型快照和 synset 文件初始化`Network`类型对象。我们还动态确定在启动服务器的机器上是否有可用的 CUDA 设备。我们通过`torch::cuda::is_available()`功能做到了这一点。

如果有 CUDA 设备可用，我们可以将模型移动到该设备上，以提高计算性能。以下代码显示了如何将模型加载到指定的设备中:

```cpp
model_ = torch::jit::load(snapshot_path, device_type);
```

`torch::jit::load()`功能接受设备类型作为其第二个参数，并自动将模型移动到指定的设备。

有一个轻量级的 C++ 单文件头文件跨平台 HTTP/HTTPS 库名为`cpp-httplib`。我们可以用它来实现我们的服务器。下面的代码显示了我们如何使用`httplib::Server`类型来实例化`server`对象，以便它可以处理 HTTP 请求:

```cpp
httplib::Server server;
```

`httplib::Server`类还实现了一个简单的静态文件服务器。下面的代码片段显示了如何设置加载静态页面的目录:

```cpp
server.set_base_dir(www_path.c_str());
```

传递到`set_base_dir()`方法的路径应该指向我们用来存储服务的 HTML 页面的目录。为了能够看到服务器启动时发生了什么，我们可以配置日志功能。下面的代码显示了当服务器接受传入消息时如何打印最少的请求信息:

```cpp
server.set_logger([](const auto& req, const auto& /*res*/) {
    std::cout << req.method << "\n" << req.path << std::endl;
});
```

当我们的服务器工作时，它也能够处理 HTTP 错误。下面的代码片段显示了如何用错误状态信息填充响应对象:

```cpp
server.set_error_handler([](const auto& /*req*/, auto& res) {
    std::stringstream buf;
    buf << "<p>Error Status: <span style='color:red;'>";
    buf << res.status;
    buf << "</span></p>";
    res.set_content(buf.str(), "text/html");
});
```

出现错误时，服务器会将此响应对象发送给客户端。

现在，我们必须为我们的服务器对象配置处理程序，以便它能够处理`POST`请求。在`httplib::Server`类中有一个`POST`方法，我们可以用于此目的。此方法采用请求模式和处理程序对象的名称。

客户端应用应该使用特殊的 URL 模式来执行请求；例如，地址可以看起来像`http://localhost:8080/imgclassify`，其中`imgclassify`是模式。对于不同的请求，我们可以有不同的处理程序。处理程序可以是任何接受两个参数的可调用对象:第一个应该是`const Request`类型，而第二个应该是`Response&`类型。下面的代码显示了我们对图像分类请求的实现:

```cpp
server.Post("/imgclassify", [&](const auto& req, auto& res) {
    std::string response_string;
    for (auto& file : req.files) {
        auto body = req.body.substr(file.second.offset, 
                                    file.second.length);
        try {
            auto img = ReadMemoryImageTensor(body, 224, 224);
            response_string += "; " + network.Classify(img);
        } catch (...) {
            response_string += "; Classification failed";
        }
    }
    res.set_content(response_string.c_str(), "text/html");
});
```

在这个处理程序中，我们遍历了输入请求中的所有文件。对于每个文件，我们执行了以下步骤:

1.  提取代表图像的字节
2.  将字节解码成图像对象
3.  将图像对象转换为张量
4.  对图像进行分类

`Request`类型对象有一个`files`成员，它可以用来迭代与给定请求一起发送的文件相关的信息块。每个块都是`MultipartFile`类型的，包含关于文件名、类型、在整个消息正文中的起始位置和长度的信息。`Request`对象的主体是一个`std::string`对象，所以我们使用`substr()`方法通过指定文件的开始位置和长度来提取特定的文件数据。我们使用`ReadMemoryImageTensor()`功能将文件数据解码成图像。该函数还对图像进行了缩放和归一化，以满足 ResNet 模型的要求。调用这个函数的结果是一个 PyTorch 张量对象。然后，我们使用`Network`对象，使用从快照加载的 ResNet 模型对图像张量进行分类。此外，`Classify()`方法返回了一个包含我们从分类器中获得的分类信息的字符串。该字符串用于填充响应对象。

我们可以使用`httplib::Server`类型对象的`listen()`方法，使其能够接受传入的连接并处理消息。下面的代码显示了如何做到这一点:

```cpp
if(!server.listen(host.c_str(), port)) {
    std::cerr << "Failed to start server\n";
}
```

`listen()`方法自动将服务器套接字绑定到给定的 IP 地址和端口号。

在本节中，我们看了服务的服务器部分的主要实现阶段。在下一节中，我们将看看客户端的实现。

# 客户

我们服务器的客户端部分可以用不同的语言和支持 HTTP 协议的不同技术来实现。客户端应用最简单的实现可以用 HTML 和 Javascript 编写。我们可以在任何现代浏览器上使用这样的客户端。

在我们的客户端应用中将有两个文件。第一个将是`index.html`文件，它应该包含一个简单网页的定义，带有上传文件的表单。下面的代码显示了我们如何编写这个文件:

```cpp
<html lang="en">
    <head>
        <title>Upload Files</title>
    </head>
    <body>
        <form method="post" enctype="multipart/form-data">
            <input type="file" name="files[]" multiple>
            <input type="submit" value="Upload File" name="submit">
        </form>
        <script src="upload.js"></script>
    </body>
</html>
```

表单是一个标准的 HTML 元素，其中有两个字段。第一个字段是文件选择的标准输入元素，而第二个字段是表单数据提交的标准输入元素。

第二个文件是 HTML 表单事件处理程序的 JavaScript 实现。下面的代码片段代表了`upload.js`文件的实现:

```cpp
const url = 'http://localhost:8080/imgclassify';
const form = document.querySelector('form');

form.addEventListener('submit', e => {
    e.preventDefault();

    const files = document.querySelector('[type=file]').files;
    const formData = new FormData();

    for (let i = 0; i < files.length; i++) {
        let file = files[i];
        formData.append('files[]', file);
    }

    fetch(url, {
        method: 'POST',
        body: formData
    }).then(response => {
        console.log(response);
        if (response.ok) {
            let text = response.text();
            text.then(data=>{
                alert(data)});
        } else {
            alert("Error :" + response.status);
        }
    });
});
```

在这个实现中，我们通过`document.querySelector('form').files`调用从 HTML 文档中获取表单元素对象。然后，我们使用`addEventListener()`方法为`submit`事件添加了一个事件监听器。当用户按下提交按钮时，`submit`事件发生。关于事件侦听器的实现，我们做了以下工作:

1.  首先，我们获得了用户选择提交的文件列表，其中包含以下代码:

```cpp
document.querySelector('[type=file]').files
```

2.  然后，我们创建了`FormData`对象。
3.  接下来，我们通过向表单数据对象传递`file`对象来顺序调用`append()`方法，从而用`files`数据填充表单数据对象。
4.  最后，我们使用`formData`对象使用`fetch()`方法向服务器发送请求。

We defined the target server request URL in the `url` string object at the beginning of the file. You should change the IP address in this URL to the external address of your server.

`fetch()`方法返回了属于异步 API 的`promise`对象，所以我们使用了`then()`方法来定义客户端收到响应后将运行的代码。在响应处理程序中，我们用响应对象的`ok`字段检查了错误状态。如果出现错误，我们会显示带有错误状态代码的警报消息。在成功响应的情况下，我们从响应对象中收集文本数据。请注意，此操作是异步执行的。当文本数据准备好时，我们的处理程序将在警报窗口中显示它。

既然我们已经有了服务的服务器和客户端部分，我们建议您在本地开发环境中测试它们。这样做之后，我们可以编译服务器部分并在本地启动它。这里，包含客户端源代码的文件夹的路径应该作为命令行参数传递。然后，我们可以通过在浏览器中打开`http://localhost:8080`网址来测试我们的服务。

现在，我们将学习如何将该服务部署到服务器。

# 服务部署

现在，我们已经在本地开发环境中实现并测试了我们的服务器应用，我们准备将其部署到云中。我们需要有一个谷歌帐户，并在 GCP 注册，以便能够继续。只需免费订阅 GCP，即可执行以下步骤，并在谷歌计算引擎中尝试我们的服务器应用:

1.  登录您的谷歌帐户，前往 GCP:[https://console.cloud.google.com](https://console.cloud.google.com)。
2.  在主页面，打开**转到计算引擎**链接或使用**导航菜单**并选择**计算引擎**链接。
3.  在**计算引擎**页面，选择**虚拟机实例**选项。
4.  单击页面顶部的 **Cr** **创建实例**链接，创建具有以下特征的虚拟机实例:

```cpp
Name: classify-server
Zone: choose appropriate to you, us-central1-a
Generation: first
Machine-type: n1-standard-1
CPU platform: automatic
Boot disk: New 10 GB standard persistent disk
Image: Debian GNU/Linux 9 (stretch)
Identity and API access: Compute Engine default service account
Access scopes: Allow default access
Firewall: Allow HTTP traffic
```

我们还可以在虚拟机实例配置中添加一个图形处理器。请注意，GPU 会显著提高服务成本，因此请仔细考虑使用 GPU 是否适合您。通常对于机器学习推理任务，拥有多核 CPU 绰绰有余。在其他情况下，如果我们计划使用 GCP 来训练机器学习算法，其强大的图形处理器可以显著减少训练时间。

5.  在**虚拟机实例**页面，选择我们创建的虚拟机实例。点击页面顶部的**开始**按钮开始。
6.  为了能够有效地与 GCP 合作，我们需要安装 GCP 软件开发工具包。SDK 可以帮助我们在没有浏览器的情况下将文件从本地系统共享到远程虚拟机实例机器。在安装 GCP 软件开发工具包之前，请确保您的系统安装了 Python 2，版本号为 Python 2.7.9 或更高版本。GCP 软件开发工具包可以从[https://cloud.google.com/sdk/docs/](https://cloud.google.com/sdk/docs/)下载。SDK 是一个存档文件。我们可以将文件的内容提取到文件系统的任何位置。然后，我们可以使用`init`参数从`[bin]`文件夹运行 **gcloud** 应用来初始化 SDK。这看起来应该和`./google-cloud-sdk/bin/gcloud init`相似。这个实用程序应该要求我们登录我们的谷歌帐户继续。下面的代码片段显示了可能的命令行会话:

```cpp
Would you like to log in (Y/n)?  y
Your browser has been opened to visit:
https://accounts.google.com/o/oauth2/auth?...
```

授权后，我们可以为当前工作会话选择项目。当我们最初创建虚拟机实例时，GCP 初始化了项目，所以如果我们看一下**计算引擎**页面的顶部，我们应该会看到一个名为`My First Project`的选项卡。下面的代码片段显示了可能的命令行会话:

```cpp
Pick cloud project to use: 
[1] hardy-aleph-253219
[2] Create a new project
Please enter numeric choice or text value (must exactly match list item):  1
```

现在，我们可以为其他问题选择默认选项来完成 GCP 软件开发工具包的初始化。

7.  我们可以使用 GCP 软件开发工具包将服务器应用的源代码复制到运行的实例中。要将文件夹从本地机器复制到远程机器，我们可以使用以下命令:

```cpp
gcloud compute scp --recurse [LOCAL_PATH] [INSTANCE_NAME]:~/[DEST_PATH]
```

这里，`LOCAL_PATH`是我们本地机器上某个文件夹的路径，`INSTANCE_NAME`是目标 VM 实例的名称，`DEST_PATH`是我们用户在远程机器上的主目录中的文件夹名称。我们应该验证我们在本地机器和远程机器上使用的用户名是相同的，因为 gcloud 实用程序总是将文件放在本地机器上使用的用户名所在的主目录中。

8.  在**虚拟机实例**页面上，我们必须确定我们之前启动的虚拟机实例以及我们复制源文件的位置。然后，我们应该找到名为`Connect`的列，选择 **SSH** ，选择**在浏览器窗口中打开**选项。此操作将打开一个新的浏览器窗口，其中有一个连接到远程计算机的交互式命令行会话。我们还可以使用 GCP 软件开发工具包用标准实用程序配置 SSH 会话。
9.  在命令行窗口中，我们可以使用以下命令来配置在远程机器上构建服务器应用所需的开发环境:

```cpp
sudo apt-get install git
sudo apt-get install cmake
sudo apt-get install g++
sudo apt-get install libopencv-dev
sudo apt-get install libprotobuf-dev
sudo apt-get install unzip
sudo apt-get install python-pip
sudo apt-get install libopenblas-dev
sudo apt-get install pybind11-dev
pip install pyyaml
pip install typing
```

10.  现在我们已经配置了开发环境，我们可以继续使用源代码并构建所需的第三方库。我们有两个这样的依赖:T0 库和 PyTorch 框架。`cpp-httplib`库是单文件头文件库，克隆到我们的源代码树就足够了。下面的代码片段显示了您需要使用的命令:

```cpp
cd ~/[DEST_PATH]/server
git clone https://github.com/yhirose/cpp-httplib third-party/httplib
```

11.  有两种方法可以获得 PyTorch 框架的依赖性。如果你的环境有一个支持 CUDA 的 GPU，你可以从官方网站下载预编译的二进制文件。下面的代码片段展示了如何做到这一点:

```cpp
cd third-party
wget --no-check-certificate https://download.pytorch.org/libtorch/cu100/libtorch-shared-with-deps-1.2.0.zip
unzip libtorch-shared-with-deps-1.2.0.zip
cd ..
```

12.  第二种方法是从某些来源构建 PyTorch。如果您的环境没有 GPU，这是唯一的选择，因为官方二进制文件需要 CUDA 支持。但是，如果您的目标是只执行推理，则不需要这样做，因为在许多情况下，现代 CPU 可以完全满足您的需求。此外，当您从源代码构建 PyTorch 时，您可以在构建中包含`FBGEMM`库。这是一个由脸书工程师开发的用于服务器端推理的低精度、高性能矩阵-矩阵乘法和卷积库。现在，通过在构建过程中使用`USE_FBGEMM` CMake 选项，您可以将 FBGEMM 用作 <q>x86</q> 机器的 Caffe2 和 PyTorch 量化运算符的后端。下面的代码片段展示了如何从源代码中克隆、构建和安装 PyTorch C++ 库:

```cpp
cd third-party
git clone https://github.com/pytorch/pytorch.git
cd pytorch/
git checkout v1.2.0
git submodule update --init --recursive
mkdir build
cd build
cmake .. -DCMAKE_INSTALL_PREFIX=~/dev/server/third-party/libtorch -DUSE_CUDA=OFF -DUSE_CUDNN=OFF -DUSE_OPENMP=ON -DBUILD_TORCH=ON -DUSE_FBGEMM=ON -DBUILD_PYTHON=OFF
cmake --build . --target install -- -j8
```

请注意，我们必须递归初始化 PyTorch `git`子模块，因为它们中的许多子模块也包含许多作为子模块的依赖关系。

13.  现在我们已经配置了开发环境并收集了所有的依赖项，我们可以在移除的虚拟机上构建我们的服务器应用。为此，请使用以下命令:

```cpp
cd ~/[DEST_PATH]/server
mkdir build
cd build
cmake .. -DCMAKE_PREFIX_PATH=~/dev/server/third-party/libtorch
cmake --build . --target all
```

14.  要运行服务器应用，我们还需要两个文件:模型快照文件和包含类描述的 synset 文件。使用 GCP 软件开发工具包时，我们可以使用以下命令将它们从本地开发环境复制到远程计算机:

```cpp
gcloud compute scp [LOCAL_PATH]/model.pt [INSTANCE_NAME]:~/[DEST_PATH]/model
gcloud compute scp [LOCAL_PATH]/synset.txt [INSTANCE_NAME]:~/[DEST_PATH]/model
```

如果您只需要复制几个文件，您可以使用基于网络的 SSH 连接窗口。有一个从本地机器复制文件的菜单。

15.  在我们可以启动我们的应用之前，我们需要配置 GCP 防火墙，以允许传入连接到我们想要启动的服务器程序。在 GCP 控制台页面，进入**导航菜单**，打开 **VPC 网络**链接。在 **VPC 网络**页面，打开**防火墙规则**链接。然后，在**防火墙规则**页面，点击**创建防火墙规则**按钮，创建新的防火墙规则。我们可以在页面顶部找到这个按钮。对于新的防火墙规则，我们必须输入以下信息:

```cpp
Name: classify-server
Target tags: http-server
Actions on match: allow
Source IP ranges: 0.0.0.0/0
Protocol and ports: tcp:8080
```

然后，我们需要点击**创建**按钮来完成规则创建过程。

16.  我们还需要记住分配给我们正在使用的虚拟机实例的 IP 地址。有两个 IP 地址:一个是内部的，一个是外部的。当我们查看特定的虚拟机实例记录时，我们可以在**虚拟机实例**页面上找到它们。内部 IP 地址是静态分配的，我们甚至可以在停止的实例中看到它。启动实例时，会动态分配外部 IP 地址。

17.  要启动我们的服务器应用，我们可以使用以下命令:

```cpp
cd ~/[DEST_PATH]/server/build
./classify-server ~/[DEST_PATH]/model/model.pt ~/[DEST_PATH]/model/synset.txt ~/[DEST_PATH]/client/ [internal ip] 8080
```

`[internal ip]`是我们在*步骤 16* 中检查的 IP 地址。数字`8080`表示应用已经配置为监听端口`8080`上的传入消息。我们还必须仔细检查模型快照文件、synset 文件的路径，以及放置静态客户端文件的目录的路径。

18.  为了让我们的 HTML 客户端工作，我们必须更新`upload.js`文件。在文件的开头，有一个`url`字符串的定义。它将采用以下形式:

```cpp
const url = 'http://localhost:8080/imgclassify';
```

将`localhost`地址更改为我们在*步骤 16* 中检查的外部 IP 地址。通过这样做，我们可以在任何浏览器中使用以下网址访问我们的服务:

```cpp
http://[external ip]:8080
```

客户端的页面应该如下所示:

![](img/d817784d-d1ad-418f-8ddd-96590ca3c3be.png)

如果您提交纽芬兰犬图像，您将看到以下响应消息:

![](img/a71804b3-9a91-44de-bea3-d3a6ffdac348.png)

本页显示我们的模型给 **256** 类分配了 **13.909313** 的值，这是**纽芬兰犬**。这个分数是我们班最高的。

# 摘要

在本章中，我们讨论了如何将机器学习模型，尤其是神经网络部署到移动和云平台。我们研究过，在这些平台上，我们通常需要定制我们在项目中使用的机器学习框架。移动平台使用不同的 CPU，有时，它们有专门的神经网络加速器设备，所以你需要编译你的应用和机器学习框架关于这些架构。用于云机器的体系结构不同于开发环境，您通常将它们用于两个不同的目的。第一种情况是使用带有 GPU 的强大机器配置来加速机器学习训练过程，因此您需要在构建应用的同时考虑使用一个或多个 GPU。另一种情况是使用云机器进行推理。在这种情况下，您通常根本不需要图形处理器，因为现代的中央处理器可以满足您的性能要求。

在这一章中，我们为安卓平台开发了一个图像分类应用。我们通过 JNI 学习了如何将 Java 模块与本机 C++ 库连接起来。然后，我们研究了如何使用 NDK 为安卓构建 PyTorch C++ 库，并了解了使用移动版本的局限性。

最后，我们为谷歌计算引擎平台实现并部署了 HTTP 图像分类服务。我们学习了如何创建和配置虚拟机实例，以及如何在特定的虚拟机实例机器上配置环境，以便构建我们的应用及其第三方依赖关系。我们看到虚拟机实例的不同配置可能需要机器学习框架的不同运行时二进制文件，因此我们为所选配置构建了 PyTorch C++ 库。这种定制的构建使我们能够利用纯 CPU 机器的最大性能。出于测试目的，我们还为我们的服务实现了一个简单的 HTML 客户端。

# 进一步阅读

*   PyTorch C++ API： [https://pytorch.org/cppdocs/](https://pytorch.org/cppdocs/)
*   将文件传输到实例(GCP):[https://cloud . Google . com/compute/docs/instance/transfer-files](https://cloud.google.com/compute/docs/instances/transfer-files)
*   谷歌云软件开发工具包文档:[https://cloud.google.com/sdk/docs/](https://cloud.google.com/sdk/docs/)
*   使用计算引擎和 REST 构建移动应用:[https://cloud . Google . com/solutions/Mobile/Mobile-计算引擎-rest](https://cloud.google.com/solutions/mobile/mobile-compute-engine-rest)
*   应用开发者文档:[https://developer.android.com/docs](https://developer.android.com/docs)
*   android sdk
*   安卓 NDK:用 C/C++ 原生库写安卓应用:[https://experience . jet ruby . com/Android-ndk-Using-C-C-原生库写安卓-应用-21550cdd86a](https://expertise.jetruby.com/android-ndk-using-c-c-native-libraries-to-write-android-apps-21550cdd86a)