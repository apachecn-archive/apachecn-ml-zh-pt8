# 十二、导出和导入模型

在本章中，我们将讨论如何在训练期间和训练之后保存和加载模型参数。这是一个相当重要的问题，因为真实的模型训练可能需要很长时间(从几天到几周)，我们希望能够保存中间结果，然后加载它们用于生产中的评估模式。

在随机应用崩溃的情况下，这种常规的保存操作可能是有益的。任何**机器学习** ( **ML** )框架的另一个实质性特征是其导出模型架构的能力，这允许我们在框架之间共享模型，并使模型部署更加容易。本章的主题是展示如何使用不同的 C++ 库导出和导入模型参数，如权重和偏差值。本章的第二部分是关于**开放神经网络交换** ( **ONNX** )格式的，该格式目前在不同的 ML 框架中越来越流行，可以用来共享训练好的模型。这种格式适合共享模型架构和模型参数。

本章将涵盖以下主题:

*   C++ 库中的 ML 模型序列化 API
*   深入研究 ONNX 格式

# 技术要求

以下是本章的技术要求:

*   `Dlib`库
*   `Shark-ML`库

*   `Shogun`库
*   `PyTorch`库
*   支持 C++ 17 的现代 C++ 编译器
*   CMake 构建系统版本> = 3.8

本章的代码文件可在以下 GitHub repo 中找到:[https://GitHub . com/PacktPublishing/动手机器学习与 CPP/tree/master/Chapter12](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-CPP/tree/master/Chapter12)

# C++ 库中的 ML 模型序列化 API

在本节中，我们将讨论`Dlib`、`Shogun`、`Shark-ML`和`PyTorch`库中的 ML 模型共享 API。在不同的 C++ 库之间共享 ML 模型有三种主要类型:

*   共享模型参数(权重)
*   共享整个模型的架构
*   共享模型架构及其训练参数

在接下来的部分中，我们将看看每个库中有什么样的 API，并强调它支持什么类型的共享。

# 用 Dlib 进行模型序列化

`Dlib`库对`decision_function`和神经网络类型对象使用序列化 API。让我们通过实现一个真实的例子来学习如何使用它。

首先，我们定义神经网络、回归核和训练样本的类型:

```cpp
 using namespace dlib;

 using NetworkType = loss_mean_squared<fc<1, input<matrix<double>>>>;
 using SampleType = matrix<double, 1, 1>;
 using KernelType = linear_kernel<SampleType>;
```

然后，我们用以下代码生成训练数据:

```cpp
 size_t n = 1000;
 std::vector<matrix<double>> x(n);
 std::vector<float> y(n);

 std::random_device rd;
 std::mt19937 re(rd());
 std::uniform_real_distribution<float> dist(-1.5, 1.5);

 // generate data
 for (size_t i = 0; i < n; ++ i) {
     x[i](0, 0) = i;
     y[i] = func(i) + dist(re);
 }
```

`x`代表预测变量，`y`代表目标变量。目标变量`y`用均匀随机噪声加盐以模拟真实数据。这些变量具有线性相关性，由以下函数定义:

```cpp
 double func(double x) {
     return 4\. + 0.3 * x;
 }
```

生成数据后，我们使用`vector_normalizer`类型对象对其进行规范化。这种类型的对象可以在训练后重新使用，以便用学习到的平均值和标准偏差来标准化数据。下面的片段展示了它是如何实现的:

```cpp
 vector_normalizer<matrix<double>> normalizer_x;
 normalizer_x.train(x);

 for (size_t i = 0; i < x.size(); ++ i) {
     x[i] = normalizer_x(x[i]);
 }
```

最后，我们用`krr_trainer`类型的对象训练`decision_function`对象进行核岭回归:

```cpp
 void TrainAndSaveKRR(const std::vector<matrix<double>>& x,
                      const std::vector<float>& y) {
    krr_trainer<KernelType> trainer;
     trainer.set_kernel(KernelType());
     decision_function<KernelType> df = trainer.train(x, y);
     serialize("dlib-krr.dat") << df;
 }
```

Note that we initialized the trainer object with the instance of the `KernelType` object.

现在我们已经有了训练好的`decision_function`对象，我们可以用`serialize`函数返回的流对象将它序列化到一个文件中:

```cpp
 serialize("dlib-krr.dat") << df;
```

该函数采用存储文件的名称，并返回一个输出流对象。我们使用`<<`运算符将学习到的回归模型的权重放入文件中。这种序列化方法只保存模型参数。

同样的方法可以用来序列化`Dlib`库中几乎所有的 ML 模型。下面的代码显示了如何使用它来序列化神经网络的参数:

```cpp
 void TrainAndSaveNetwork(const std::vector<matrix<double>>& x,
 const std::vector<float>& y) {
     NetworkType network;
     sgd solver;
     dnn_trainer<NetworkType> trainer(network, solver);
     trainer.set_learning_rate(0.0001);
     trainer.set_mini_batch_size(50);
     trainer.set_max_num_epochs(300);
     trainer.be_verbose();
     trainer.train(x, y);
     network.clean();

     serialize("dlib-net.dat") << network;
     net_to_xml(network, "net.xml");
 }
```

对于神经网络，还有`net_to_xml`函数，保存模型结构，但是没有函数将这个保存的结构加载到我们的程序中。实现加载功能是用户的责任。如果我们希望在框架之间共享模型，那么`net_to_xml`函数是存在的，正如在`Dlib`文档中所写的那样。

为了检查参数序列化是否如预期的那样工作，我们生成新的测试数据来评估其上加载的模型:

```cpp
 std::cout << "Target values \n";
 std::vector<matrix<double>> new_x(5);
 for (size_t i = 0; i < 5; ++ i) {
     new_x[i].set_size(1, 1);
     new_x[i](0, 0) = i;
     new_x[i] = normalizer_x(new_x[i]);
     std::cout << func(i) << std::endl;
 }
```

注意，我们重用了规格化器对象。一般来说，它的参数也应该序列化和加载，因为在评估过程中，我们需要将新数据转换为与训练数据相同的统计特征。

要在`Dlib`库中加载一个序列化对象，我们可以使用`deserialize`函数。该函数采用文件名并返回输入流对象:

```cpp
 void LoadAndPredictKRR(const std::vector<matrix<double>>& x) {
     decision_function<KernelType> df;
     deserialize("dlib-krr.dat") >> df;

     // Predict
     std::cout << "KRR predictions \n";
     for (auto& v : x) {
         auto p = df(v);
         std::cout << static_cast<double>(p) << std::endl;
     }
 }
```

如前所述，`Dlib`库中的序列化只存储模型参数。因此，为了加载它们，我们需要使用具有与执行序列化之前相同属性的模型对象。对于回归模型，这意味着我们应该实例化一个具有相同内核类型的决策函数对象。对于神经网络模型，这意味着我们应该实例化一个与我们用于序列化的类型相同的网络对象:

```cpp
  void LoadAndPredictNetwork(const std::vector<matrix<double>>& x) {
     NetworkType network;
     deserialize("dlib-net.dat") >> network;

     // Predict
     auto predictions = network(x);
     std::cout << "Net predictions \n";
     for (auto p : predictions) {
         std::cout << static_cast<double>(p) << std::endl;
     }
 }
```

在本节中，我们看到`Dlib`序列化 API 允许我们保存和加载 ML 模型参数，但是序列化和加载模型架构的选项有限。在下一节中，我们将查看`Shogun`库模型的序列化 API。

# 幕府的模型序列化

`Shogun`库可以保存 ASCII、JSON、XML、HDF5 等不同文件格式的模型参数。该库不能从文件中加载模型架构，只能保存和加载精确模型的权重。但是神经网络有一个例外:`Shogun`库可以从 JSON 文件中加载网络结构。以下示例显示了此功能的一个示例。

和前面的例子一样，我们从生成训练数据开始:

```cpp
const int32_t n = 1000;
SGMatrix<float64_t> x_values(1, n);
SGVector<float64_t> y_values(n);

std::random_device rd;
std::mt19937 re(rd());
std::uniform_real_distribution<double> dist(-1.5, 1.5);

// generate data
for (int32_t i = 0; i < n; ++ i) {
    x_values.set_element(i, 0, i);

    auto y_val = func(i) + dist(re);
    y_values.set_element(y_val, i);
}

auto x = some<CDenseFeatures<float64_t>>(x_values);
auto y = some<CRegressionLabels>(y_values);

// rescale
auto x_scaler = some<CRescaleFeatures>();
x_scaler->fit(x);
x_scaler->transform(x, true);
```

我们用预测变量值填充了`CDenseFeatures`类型的`x`对象，用目标变量值填充了`CRegressionLabels`类型的`y`对象。线性相关性与我们在前面的例子中使用的相同。我们还使用`CRescaleFeatures`类型的对象重新调整了`x`值。

为了展示`Shogun`库中的序列化 API 是如何工作的，我们将使用`CLinearRidgeRegression`和`CNeuralNetwork`模型。

下面的代码示例展示了如何训练和序列化`CLinearRidgeRegression`模型:

```cpp
void TrainAndSaveLRR(Some<CDenseFeatures<float64_t>> x,
                     Some<CRegressionLabels> y) {
    float64_t tau_regularization = 0.0001;
    auto model =
    some<CLinearRidgeRegression>(tau_regularization, nullptr, nullptr);
    model->set_labels(y);
    if (!model->train(x)) {
        std::cerr << "training failed\n";
    }

    auto file = some<CSerializableHdf5File>("shogun-lr.dat", 'w');
    if (!model->save_serializable(file)) {
        std::cerr << "Failed to save the model\n";
    }
}
```

这里，我们将模型参数保存为 HDF5 文件格式，对象为`CSerializableHdf5File`类型。对于其他文件格式，我们可以在库中找到相应的类型。`Shogun`库中的所有可序列化模型分别具有保存和加载模型参数的`save_serializable`和`load_serializable`功能。这些函数采用序列化文件对象。在这种情况下，这个物体属于`CSerializableHdf5File`类型。

下面的代码显示了如何训练和保存神经网络对象的参数:

```cpp
void TrainAndSaveNET(Some<CDenseFeatures<float64_t>> x,
                     Some<CRegressionLabels> y) {
    auto dimensions = x->get_num_features();
    auto layers = some<CNeuralLayers>();
    layers = wrap(layers->input(dimensions));
    layers = wrap(layers->linear(1));
    auto all_layers = layers->done();

    auto network = some<CNeuralNetwork>(all_layers);
    // configure network parameters
    ...

    network->set_labels(y);
    if (network->train(x)) {
        auto file = some<CSerializableHdf5File>("shogun-net.dat", 'w');
        if (!network->save_serializable(file)) {
            std::cerr << "Failed to save the model\n";
        }
    } else {
        std::cerr << "Failed to train the network\n";
    }
}
```

在这里，我们可以看到神经网络序列化类似于序列化线性回归模型。

为了测试我们的序列化模型，我们将生成一组新的测试数据。下面的代码显示了我们是如何做到这一点的:

```cpp
SGMatrix<float64_t> new_x_values(1, 5);
std::cout << "Target values : \n";
for (index_t i = 0; i < 5; ++ i) {
    new_x_values.set_element(static_cast<double>(i), 0, i);
    std::cout << func(i) << std::endl;
}

auto new_x = some<CDenseFeatures<float64_t>>(new_x_values);
x_scaler->transform(new_x, true);
```

请注意，我们重用了`x_scaler`对象。但是正如我们前面提到的，如果我们计划在培训后停止并重新启动我们的应用，它也应该被序列化和加载。

下面的代码显示了反序列化过程:

```cpp
void LoadAndPredictLRR(Some<CDenseFeatures<float64_t>> x) {
    auto file = some<CSerializableHdf5File>("shogun-lr.dat", 'r');
    auto model = some<CLinearRidgeRegression>();
    if (model->load_serializable(file)) {
        auto y_predict = model->apply_regression(x);
        std::cout << "LR predicted values: \n" << y_predict->to_string() << 
            << std::endl;
    }
}
...
LoadAndPredictLRR(new_x);
```

这里，创建了新的`CLinearRidgeRegression`对象，并使用`load_serializable`方法加载其参数。

正如我们前面提到的，有一个特定的函数用于从 JSON 文件或字符串中加载神经网络结构。问题是我们不能用库 API 将这个结构导出为文件，所以应该自己制作或者自己编写自定义导出器。然而，这个功能允许我们定义神经网络体系结构，而不用声明式编程。这对实验很有用，因为我们不需要重新编译整个应用。它还允许我们在没有程序更新的情况下将新架构部署到生产中，但是请注意，我们需要注意保留输入和输出网络张量维度。

要从`Shogun`库中的 JSON 格式的字符串加载神经网络，我们可以使用`CNeuralNetworkFileReader`类型的对象。下面的代码显示了如何使用它:

```cpp
Some<CNeuralNetwork> NETFromJson() {
    CNeuralNetworkFileReader reader;
    const char* net_str =
    "{"
        "\"optimization_method\": \"NNOM_GRADIENT_DESCENT\","
        "\"max_num_epochs\": 1000,"
        "\"gd_mini_batch_size\": 0,"
        "\"gd_learning_rate\": 0.01,"
        "\"gd_momentum\": 0.9,"

        "\"layers\":"
        "{"
            "\"input1\":"
            "{"
                "\"type\": \"NeuralInputLayer\","
                "\"num_neurons\": 1,"
                "\"start_index\": 0"
                "},"
            "\"linear1\":"
            "{"
                "\"type\": \"NeuralLinearLayer\","
                "\"num_neurons\": 1,"
                "\"inputs\": [\"input1\"]"
                "}"
            "}"
        "}";
    auto network = wrap(reader.read_string(net_str));

    return network;
}
```

这里，我们用一个 JSON 字符串定义了神经网络架构。这与我们用于训练的神经网络架构相同。然后，`CNeuralNetworkFileReader`对象的`read_string`方法用于加载和创建一个`CNeuralNetwork`类型的对象。

下面的代码显示了如何使用`NETFromJson`函数从 JSON 字符串创建一个网络对象，并用序列化的参数初始化它:

```cpp
void LoadAndPredictNET(Some<CDenseFeatures<float64_t>> x) {
    auto file = some<CSerializableHdf5File>("shogun-net.dat", 'r');

    auto network = NETFromJson();

    if (network->load_serializable(file)) {
        auto new_x = some<CDenseFeatures<float64_t>>(x);
        auto y_predict = network->apply_regression(new_x);
        std::cout << "Network predicted values: \n"
        << y_predict->to_string() << std::endl;
    }
}
```

新创建的神经网络对象属于`CNeuralNetwork`类型。我们使用新神经网络对象的`load_serializable`方法来加载先前序列化的参数。保持用于序列化和反序列化的 ML 模型对象的相同体系结构是非常重要的，因为不同的体系结构会在执行反序列化时导致运行时错误。

在本节中，我们将了解如何使用`Shogun`库 API 进行序列化。这个库没有提供任何可以用来导出 ML 模型架构的函数，但是它可以从一个 JSON 字符串中加载它们。ML 模型参数可以序列化为各种文件格式。在下一节中，我们将深入研究`Shark-ML`库的序列化 API。

# 使用 Shark-ML 进行模型序列化

`Shark-ML`库有一个统一的 API，用于序列化各种模型。每个模型分别有保存和加载模型参数的`write`和`read`方法。这些方法将`boost::archive`对象的实例作为输入参数。

让我们看一个使用`Shark-ML`库的模型参数序列化的例子。首先，我们为线性回归模型生成训练数据，就像我们在前面的例子中所做的那样:

```cpp
std::vector<RealVector> x_data(n);
std::vector<RealVector> y_data(n);

std::random_device rd;
std::mt19937 re(rd());
std::uniform_real_distribution<double> dist(-1.5, 1.5);

RealVector x_v(1);
RealVector y_v(1);
for (size_t i = 0; i < n; ++ i) {
    x_v(0) = i;
    x_data[i] = x_v;

    y_v(0) = func(i) + dist(re);  // add noise
    y_data[i] = y_v;
}

Data<RealVector> x = createDataFromRange(x_data);
Data<RealVector> y = createDataFromRange(y_data);
RegressionDataset data(x, y);
```

这里，我们创建了两个向量，`x_data`和`y_data`，它们包含`RealVector`类型的预测器和目标值对象。然后，我们制作了`Data`类型的`x`和`y`对象，并将其放置到`RegressionDataset`类型的`data`对象中。

下面的代码显示了如何用我们之前初始化的数据集对象训练线性模型对象:

```cpp
LinearModel<> model;
LinearRegression trainer;
trainer.train(model, data);
```

这里，我们使用`LinearRegression`类型的训练器训练`LinearModel`对象。

现在我们已经训练了模型，我们可以使用`boost::archive::polymorphic_binary_oarchive`对象将其参数保存在一个文件中。下面的代码显示了如何做到这一点:

```cpp
std::ofstream ofs("shark-linear.dat");
boost::archive::polymorphic_binary_oarchive oa(ofs);
model.write(oa);
```

档案对象`oa`是用`std::ofstream`类型的`ofs`对象初始化的。选择输出流类型是因为我们需要保存模型参数。

以下代码显示了如何加载保存的模型参数:

```cpp
 std::ifstream ifs("shark-linear.dat");
 boost::archive::polymorphic_binary_iarchive ia(ifs);
 LinearModel<> model;
 model.read(ia); 
```

我们用`read`方法加载模型参数，该方法采用了`boost::archive::polymorphic_binary_iarchive`对象，该对象是用`std::ifstream`对象初始化的。请注意，我们创建了一个新的`LinearModel`对象。

不使用二进制序列化，`Shark-ML`库允许我们使用`boost::archive::polymorphic_text_oarchive`和`boost::archive::polymorphic_text_iarchive`类型序列化为 ASCII 文本文件。

下面的代码显示了如何生成新的测试值，以便我们可以检查模型:

```cpp
 std::vector<RealVector> new_x_data;
 for (size_t i = 0; i < 5; ++ i) {
    new_x_data.push_back({static_cast<double>(i)});
    std::cout << func(i) << std::endl;
 } 
```

以下代码显示了如何将模型用于预测目的:

```cpp
 auto prediction = model(createDataFromRange(new_x_data));
 std::cout << "Predictions: \n" << prediction << std::endl;
```

预测是通过调用模型的对象函数运算符做出的。

在这一节中，我们看到`Shark-ML`库有一个我们可以用来保存和加载参数的 API，但是它缺少保存和加载 ML 模型架构的功能。

在下一节中，我们将查看`PyTorch`库的序列化 API。

# 使用 PyTorch 进行模型序列化

在本节中，我们将讨论`PyTorch` C++ 库中可用的两种网络参数序列化方法。第一种是使用`torch::save`函数，第二种是使用`torch::serialize::OutputArchive`类型的对象将参数写入其中。

让我们为神经网络的进一步使用做准备。

# 神经网络初始化

让我们从生成训练数据开始。下面的代码展示了我们如何做到这一点:

```cpp
 torch::DeviceType device = torch::cuda::is_available()
                                 ? torch::DeviceType::CUDA
                                 : torch::DeviceType::CPU;

 std::random_device rd;
 std::mt19937 re(rd());
 std::uniform_real_distribution<float> dist(-0.1f, 0.1f);

 // generate data
 size_t n = 1000;
 torch::Tensor x;
 torch::Tensor y;
 {
     std::vector<float> values(n);
     std::iota(values.begin(), values.end(), 0);
     std::shuffle(values.begin(), values.end(), re);

     std::vector<torch::Tensor> x_vec(n);
     std::vector<torch::Tensor> y_vec(n);
     for (size_t i = 0; i < n; ++ i) {
         x_vec[i] = torch::tensor(
         values[i],
         torch::dtype(torch::kFloat).device(device).requires_grad(false));

         y_vec[i] = torch::tensor(
         (func(values[i]) + dist(re)),
         torch::dtype(torch::kFloat).device(device).requires_grad(false));
     }
     x = torch::stack(x_vec);
     y = torch::stack(y_vec);
 }

 // normalize data
 auto x_mean = torch::mean(x, /*dim*/ 0);
 auto x_std = torch::std(x, /*dim*/ 0);
 x = (x - x_mean) / x_std;

```

通常，我们希望利用尽可能多的硬件资源。所以，首先，我们通过`torch::cuda::is_available()`调用检查了系统中是否有采用 CUDA 技术的 GPU。然后，我们生成了 1000 个预测变量值，并对它们进行了洗牌。对于每个值，我们使用前面示例中使用的线性函数计算目标值。所有的值都通过`torch::tensor`函数调用被移动到`torch::Tensor`对象中。请注意，我们使用了之前检测到的设备来创建张量。将所有值移动到张量后，我们使用`torch::stack`函数将预测值和目标值连接成两个不同的单个张量。这是使用`PyTorch`线性代数例程执行数据标准化所必需的。然后，我们使用`torch::mean`和`torch::std`函数来计算预测值的平均值和标准偏差，并对它们进行归一化。

在下面的代码中，我们定义了`NetImpl`类，它实现了我们的神经网络:

```cpp
 class NetImpl : public torch::nn::Module {
   public:
     NetImpl() {
         l1_ = torch::nn::Linear(torch::nn::LinearOptions(1, 
                                 8).with_bias(true));
         register_module("l1", l1_);
         l2_ = torch::nn::Linear(torch::nn::LinearOptions(8, 
                                 4).with_bias(true));
         register_module("l2", l2_);
         l3_ = torch::nn::Linear(torch::nn::LinearOptions(4, 
                                 1).with_bias(true));
         register_module("l3", l3_);

         // initialize weights
         for (auto m : modules(false)) {
             if (m->name().find("Linear") != std::string::npos) {
                 for (auto& p : m->named_parameters()) {
                     if (p.key().find("weight") != std::string::npos) {
                         torch::nn::init::normal_(p.value(), 0, 0.01);
                     }
                     if (p.key().find("bias") != std::string::npos) {
                         torch::nn::init::zeros_(p.value());
                     }
                 }
             }
         }
     }

     torch::Tensor forward(torch::Tensor x) {
         auto y = l1_(x);
         y = l2_(y);
         y = l3_(y);
         return y;
     }
   private:
     torch::nn::Linear l1_{nullptr};
     torch::nn::Linear l2_{nullptr};
     torch::nn::Linear l3_{nullptr};
 }
 TORCH_MODULE(Net);

```

在这里，我们将我们的神经网络模型定义为具有三个完全连接的神经元层和线性激活函数的网络。每层都是`torch::nn::Linear`型。在我们模型的构造器中，我们用小的随机值初始化了所有的网络参数。我们通过迭代所有网络模块(参见`modules`方法调用)并将`torch::nn::init::normal_`函数应用于`named_parameters()`模块方法返回的参数来实现这一点。用`torch::nn::init::zeros_`函数将偏差初始化为零。`named_parameters()`方法返回由字符串名称和张量值组成的对象，因此在初始化时，我们使用了它的`value`方法。

现在，我们可以用生成的训练数据训练模型。下面的代码展示了如何训练我们的模型:

```cpp
 Net model;
 model->to(device);

 // initialize optimizer ----------------------------------------------
 double learning_rate = 0.01;
 torch::optim::Adam optimizer(
          model->parameters(),
          torch::optim::AdamOptions(learning_rate).weight_decay(0.00001));

 // training
 int64_t batch_size = 10;
 int64_t batches_num = static_cast<int64_t>(n) / batch_size;
 int epochs = 10;
 for (int epoch = 0; epoch < epochs; ++ epoch) {
     // train the model -----------------------------------------------
     model->train();  // switch to the training mode

     // Iterate the data
     double epoch_loss = 0;
     for (int64_t batch_index = 0; batch_index < batches_num; ++ batch_index) {
         auto batch_x = x.narrow(0, batch_index * batch_size, batch_size);
         auto batch_y = y.narrow(0, batch_index * batch_size, batch_size);

         // Clear gradients
         optimizer.zero_grad();

         // Execute the model on the input data
         torch::Tensor prediction = model->forward(batch_x);

         torch::Tensor loss = torch::mse_loss(prediction, batch_y);

         // Compute gradients of the loss and parameters of our model
         loss.backward();

         // Update the parameters based on the calculated gradients.
         optimizer.step();
     }
 }
```

为了利用我们所有的硬件资源，我们将模型移动到选定的计算设备上。然后，我们初始化了一个优化器。在我们的例子中，优化器使用了亚当算法。之后，我们在各个时期运行一个标准的训练循环，对于每个时期，我们获取训练批次，清除优化器的梯度，执行向前传递，计算损失，执行向后传递，并使用优化器步骤更新模型权重。

为了从数据集中选择一批训练数据，我们使用了张量`narrow`方法，该方法返回一个降维的新张量。此函数采用新的尺寸数量作为第一个参数，起始位置作为第二个参数，元素数量作为第三个参数。

正如我们之前提到的，有两种方法可以用来序列化 C++ API 中`PyTorch`的模型参数(Python API 提供了更大的范围)。让我们看看他们。

# 使用手电筒::保存和手电筒::加载功能

我们可以用来保存模型参数的第一种方法是使用`torch::save`函数，该函数递归地保存来自传递的模块的参数:

```cpp
 torch::save(model, "pytorch_net.pt"); 
```

为了在我们的定制模块中正确使用它，我们需要用`register_module`模块的方法注册父模块中的所有子模块。

要加载保存的参数，我们可以使用`torch::load`功能:

```cpp
 Net model_loaded;
 torch::load(model_loaded, "pytorch_net.pt"); 
```

函数用从文件中读取的值填充传递的模块参数。

# 使用 PyTorch 归档对象

第二种方法是使用`torch::serialize::OutputArchive`类型的对象，并将我们想要保存的参数写入其中。下面的代码展示了如何为我们的模型实现`SaveWeights`方法。此方法将我们模块中存在的所有参数和缓冲区写入`archive`对象，然后使用`save_to`方法将它们写入文件:

```cpp
 void NetImpl::SaveWeights(const std::string& file_name) {
     torch::serialize::OutputArchive archive;
     auto parameters = named_parameters(true /*recurse*/);
     auto buffers = named_buffers(true /*recurse*/);
     for (const auto& param : parameters) {
         if (param.value().defined()) {
             archive.write(param.key(), param.value());
         }
     }
     for (const auto& buffer : buffers) {
         if (buffer.value().defined()) {
             archive.write(buffer.key(), buffer.value(), /*is_buffer*/ true);
         }
     }
     archive.save_to(file_name);
 }

```

保存缓冲张量也很重要。可以使用`named_buffers`模块的方法从模块中检索缓冲区。这些对象表示用于评估不同模块的中间值。例如，我们可以运行批次标准化模块的平均值和标准偏差值。如果我们使用序列化来保存中间步骤，并且如果我们的培训过程由于某种原因而停止，我们需要他们继续接受培训。

要加载以这种方式保存的参数，我们可以使用`torch::serialize::InputArchive`对象。下面的代码展示了如何为我们的模型实现`LoadWeights`方法:

```cpp
 void NetImpl::LoadWeights(const std::string& file_name) {
     torch::serialize::InputArchive archive;
     archive.load_from(file_name);
     torch::NoGradGuard no_grad;
     auto parameters = named_parameters(true /*recurse*/);
     auto buffers = named_buffers(true /*recurse*/);
     for (auto& param : parameters) {
         archive.read(param.key(), param.value());
     }
     for (auto& buffer : buffers) {
         archive.read(buffer.key(), buffer.value(), /*is_buffer*/ true);
     }
 }
```

该方法使用`archive`对象的`load_from`方法从文件中加载参数。然后，我们使用`named_parameters`和`named_buffers`方法从模块中获取参数和缓冲区，并使用`archive`对象的`read`方法递增填充它们的值。请注意，我们使用了一个`torch::NoGradGuard`类的实例来告诉`PyTorch`库，我们不执行任何模型计算和图形相关的操作。这样做很重要，因为`PyTorch`构造计算图和任何不相关的操作都会导致错误。

现在，我们可以使用我们的带有负载参数的`model_loaded`模型的新实例来评估一些测试数据上的模型。注意，我们需要用`eval`方法将模型切换到评估模型。生成的测试数据值也应该用`torch::tensor`函数转换成张量对象，并移动到我们的模型使用的相同计算设备。下面的代码展示了我们如何实现这一点:

```cpp
 model_loaded->to(device);
 model_loaded->eval();
 std::cout << "Test:\n";
 for (int i = 0; i < 5; ++ i) {
     auto x_val = static_cast<float>(i) + 0.1f;
     auto tx = torch::tensor(x_val, torch::dtype(torch::kFloat).device(device));
     tx = (tx - x_mean) / x_std;

     auto ty = torch::tensor(func(x_val), torch::dtype(torch::kFloat).device(device));

     torch::Tensor prediction = model_loaded->forward(tx);

     std::cout << "Target:" << ty << std::endl;
     std::cout << "Prediction:" << prediction << std::endl;
 }
```

在本节中，我们看了`PyTorch`库中的两种序列化类型。第一种方法是使用`torch::save`和`torch::load`功能，分别轻松保存和加载所有模型参数。第二种方法是使用`torch::serialize::InputArchive`和`torch::serialize::OutputArchive`类型的对象，这样我们就可以选择想要保存和加载的参数。

在下一节中，我们将讨论 ONNX 文件格式，它允许我们在不同框架之间共享我们的 ML 模型架构和模型参数。

# 深入研究 ONNX 格式

ONNX 格式是一种特殊的文件格式，用于在不同框架之间共享神经网络架构和参数。它基于谷歌 Protobuf 格式和库。这种格式存在的原因是为了在不同的环境和不同的设备上测试和运行相同的神经网络模型。通常，研究人员使用他们知道如何使用的编程框架来开发模型，然后出于生产目的或如果他们想与其他研究人员或开发人员共享他们的模型，在不同的环境中运行该模型。所有领先的框架都支持这种格式，如`PyTorch,``TensorFlow``MXNet`等。但是现在，这些框架的 C++ API 缺乏对这种格式的支持，在撰写本文时，它们只有一个处理 ONNX 格式的 Python 接口。前段时间，脸书开发了 Caffe2 神经网络框架，以便在不同平台上以最佳性能运行模型。这个框架也有一个 C++ 应用编程接口，它能够加载和运行以 ONNX 格式保存的模型。现在这个框架已经和`PyTorch`合并了。在`PyTorch`有一个计划是去除咖啡因 2 原料药，用一种新的混合原料药来代替。但在撰写本文时，Caffe2 C++ API 仍可作为`PyTorch` 1.2 ( `libtorch`)库的一部分使用。

通常，作为开发人员，我们不需要知道 ONNX 格式在内部是如何工作的，因为我们只对保存模型的文件感兴趣。在内部，ONNX 格式是一个 Protobuf 格式的文件。下面的代码显示了 ONNX 文件的第一部分，它描述了如何使用 ResNet 神经网络架构进行图像分类:

```cpp
 ir_version: 3
 graph {
     node {
         input: "data"
         input: "resnetv24_batchnorm0_gamma"
         input: "resnetv24_batchnorm0_beta"
         input: "resnetv24_batchnorm0_running_mean"
         input: "resnetv24_batchnorm0_running_var"
         output: "resnetv24_batchnorm0_fwd"
         name: "resnetv24_batchnorm0_fwd"
         op_type: "BatchNormalization"
         attribute {
             name: "epsilon"
             f: 1e-05
             type: FLOAT
         }
         attribute {
             name: "momentum"
             f: 0.9
             type: FLOAT
         }
         attribute {
             name: "spatial"
             i: 1
             type: INT
         }
     }
     node {
         input: "resnetv24_batchnorm0_fwd"
         input: "resnetv24_conv0_weight"
         output: "resnetv24_conv0_fwd"
         name: "resnetv24_conv0_fwd"
         op_type: "Conv"
         attribute {
             name: "dilations"
             ints: 1
             ints: 1
             type: INTS
         }
         attribute {
             name: "group"
             i: 1
             type: INT
         }
         attribute {
             name: "kernel_shape"
             ints: 7
             ints: 7
             type: INTS
         }
         attribute {
             name: "pads"
             ints: 3
             ints: 3
             ints: 3
             ints: 3
             type: INTS
         }
         attribute {
             name: "strides"
             ints: 2
             ints: 2
             type: INTS
         }
     }
     ...
 }

```

通常，ONNX 文件以二进制格式出现，以减小文件大小并提高加载速度。

现在，让我们学习如何使用 Caffe2 C++ API 来加载和运行 ONNX 模型。不幸的是，只有一个可用的 C++ 库 API 可以用来运行以 ONNX 格式保存的模型。这是因为 Caffe2 可以自动将它们转换为其内部表示。其他库在其 Python 模块中进行这种转换。ONNX 社区为公开可用的模型动物园([https://github.com/onnx/models](https://github.com/onnx/models))中最流行的神经网络架构提供预训练模型。有很多现成的模型可以用来解决不同的 ML 任务。比如我们可以拿 ResNet-50 模型进行图像分类任务([https://github . com/onnx/models/tree/master/vision/classing/ResNet](https://github.com/onnx/models/tree/master/vision/classification/resnet))。对于这个模型，我们必须下载相应的带有图像类描述的 synset 文件，以便能够以人类可读的方式返回分类结果。文件的链接是[。](https://github.com/onnx/models/blob/master/vision/classification/synset.txt)

为了能够使用 Caffe2 C++ API，我们必须使用以下头:

```cpp
 #include <caffe2/core/init.h>
 #include <caffe2/onnx/backend.h>
 #include <caffe2/utils/proto_utils.h>
```

但是，我们仍然需要将我们的程序链接到`libtorch.so`库。

首先，我们需要初始化 Caffe2 库:

```cpp
 caffe2::GlobalInit(&argc, &argv); 
```

然后，我们需要加载 Protobuf 模型表示。这可以通过`onnx_torch::ModelProto`类的一个实例来完成。要使用这个类的一个对象来加载模型，我们需要使用`ParseFromIstream`方法，该方法将`std::istream`对象作为输入参数。下面的代码显示了如何使用`onnx_torch::ModelProto`类的对象:

```cpp
 onnx_torch::ModelProto model_proto;
 {
     std::ifstream file(argv[1], std::ios_base::binary);
     if (!file) {
         std::cerr << "File " << argv[1] << "can't be opened\n";
         return 1;
     }
     if (!model_proto.ParseFromIstream(&file)) {
         std::cerr << "Failed to parse onnx model\n";
         return 1;
     }
 }
```

`caffe2::onnx::Caffe2Backend`类应该用于将 Protobuf ONNX 模型转换为 Caffe2 的内部表示。该类包含`Prepare`方法，该方法采用 Protobuf 格式的字符串，以及模型描述、包含计算设备名称的字符串和一些附加设置(通常，这些设置可以为空)。下面的代码展示了如何在我们准备模型之前使用`onnx_torch::ModelProto`类的`SerializeToString`方法来创建模型的字符串表示:

```cpp
 std::string model_str;
 if (model_proto.SerializeToString(&model_str)) {
     caffe2::onnx::Caffe2Backend onnx_backend;
     std::vector<caffe2::onnx::Caffe2Ops> ops;
     auto model = onnx_backend.Prepare(model_str, "CPU", ops);
     if (model != nullptr) {
     ...
     }
 }
```

既然我们已经为评估准备了模型，我们就必须准备输入和输出数据容器。在我们的例子中，输入是大小为`1 x 3 x 224 x 224`的张量，表示用于分类的 RGB 图像。但是 Caffe2 ONNX 模型以`caffe2::TensorCPU`对象的矢量作为输入，所以我们需要将我们的图像移动到`inputs`矢量。Caffe2 张量对象不可复制，但可以移动。`outputs`向量应该为空。

下面的代码片段显示了如何为模型准备输入和输出数据:

```cpp
 caffe2::TensorCPU image = ReadImageTensor(argv[2], 224, 224);

 std::vector<caffe2::TensorCPU> inputs;
 inputs.push_back(std::move(image));

 std::vector<caffe2::TensorCPU> outputs(1);

```

模型是`Caffe2BackendRep`类的对象，使用`Run`方法进行评估。我们可以通过以下方式使用它:

```cpp
model->Run(inputs, &outputs);
```

这个模型的输出是用于训练模型的 ImageNet 数据集的 1，000 个类中的每一个的图像得分(概率)。下面的代码显示了如何解码模型的输出:

```cpp
 std::map<size_t, std::string> classes = ReadClasses(argv[3]);
 for (auto& output : outputs) {
     const auto& probabilities = output.data<float>();
     std::vector<std::pair<float, int>> pairs;  // prob : class index
     for (auto i = 0; i < output.size(); i++) {
         if (probabilities[i] > 0.01f) {
             pairs.push_back(
             std::make_pair(probabilities[i], i + 1));  // 0 - background
         }
     }
     std::sort(pairs.begin(), pairs.end());
     std::reverse(pairs.begin(), pairs.end());
     pairs.resize(std::min(5UL, pairs.size()));
     for (auto& p : pairs) {
         std::cout << "Class " << p.second << " Label "
         << classes[static_cast<size_t>(p.second)] << " Prob "
         << p.first << std::endl;
  }
```

这里，我们从`outputs`向量迭代每个输出张量。在我们的例子中，只有一个项目，但是如果我们在`inputs`向量中使用几个输入图像，我们会有几个结果。然后，我们将分值和类别索引放入相应对的向量中。这个向量是按分数降序排序的。然后，我们打印了五个分数最高的班级。

为了访问`caffe2::TensorCPU`对象的元素，我们使用了`data<float>()`方法，该方法将指针返回到张量的`const`行顺序浮点值。在这个例子中，输出张量的维数为`1x1000`，所以我们像在线性阵列中一样访问它的值。

为了正确完成程序，我们必须关闭谷歌`protobuf`库，我们用它来加载所需的 ONNX 文件:

```cpp
 google::protobuf::ShutdownProtobufLibrary();
```

在这一节中，我们看了一个如何在`PyTorch`和 Caffe2 库中处理 ONNX 格式的例子，但是我们仍然需要学习如何将输入图像加载到 Caffe2 张量对象中，我们将它用于模型的输入。

# 将图像加载到 Caffe2 张量中

让我们学习如何将图像加载到 Caffe2 张量对象中，并根据模型的输入要求进行修改。该模型期望输入图像归一化，并且形状为(`N x 3 x H x W`)的三通道 RGB 图像，其中 *N* 是批量大小， *H* 和 *W* 预计至少为 224 像素宽。归一化假设图像被加载到`[0, 1]`的范围内，然后使用等于`[0.485, 0.456, 0.406]`的平均值和等于`[0.229, 0.224, 0.225]`的标准偏差进行归一化。

假设我们有以下图像加载的函数定义:

```cpp
caffe2::TensorCPU ReadImageTensor(const std::string& file_name,
                                                 int width,
                                                 int height) {
   ...
}
```

让我们编写它的实现。对于图像加载，我们将使用 OpenCV 库:

```cpp
// load image
auto image = cv::imread(file_name, cv::IMREAD_COLOR);

if (!image.cols || !image.rows) {
   return {};
}

if (image.cols != width || image.rows != height) {
   // scale image to fit
   cv::Size scaled(std::max(height * image.cols / image.rows, width),
   std::max(height, width * image.rows / image.cols));
   cv::resize(image, image, scaled);

  // crop image to fit
   cv::Rect crop((image.cols - width) / 2, (image.rows - height) / 2, width,
   height);
   image = image(crop);
}
```

这里，我们使用`cv::imread`功能从文件中读取图像。如果图像尺寸不等于指定尺寸，我们需要使用`cv::resize`功能调整图像尺寸，如果图像尺寸超过指定尺寸，则裁剪图像。

然后，我们将图像转换为浮点类型和 RGB 格式:

```cpp
image.convertTo(image, CV_32FC3);
 cv::cvtColor(image, image, cv::COLOR_BGR2RGB);
```

格式化完成后，我们可以将图像分割成红色、绿色和蓝色三个独立的通道。我们还应该使颜色值正常化。下面的代码显示了如何做到这一点:

```cpp
 std::vector<cv::Mat> channels(3);
 cv::split(image, channels);

 std::vector<double> mean = {0.485, 0.456, 0.406};
 std::vector<double> stddev = {0.229, 0.224, 0.225};

 size_t i = 0;
 for (auto& c : channels) {
    c = ((c / 255) - mean[i]) / stddev[i];
    ++ i;
 }
```

用相应的平均值减去每个通道，然后除以标准化过程的相应标准偏差。

然后，我们应该连接这些通道:

```cpp
 cv::vconcat(channels[0], channels[1], image);
 cv::vconcat(image, channels[2], image);
 assert(image.isContinuous());
```

使用`cv::vconcat`函数将归一化通道连接成一个连续的图像。

下面的代码显示了如何用图像数据初始化 Caffe2 张量:

```cpp
 std::vector<int64_t> dims = {1, 3, height, width};

 caffe2::TensorCPU tensor(dims, caffe2::DeviceType::CPU);
 std::copy_n(reinterpret_cast<float*>(image.data),
             image.size().area(),
             tensor.mutable_data<float>());

 return tensor;
```

这里，图像数据被复制到`caffe2::TensorCPU`对象中，该对象被初始化为指定的尺寸。计算装置等于`caffe2::DeviceType::CPU`。默认情况下，这个张量对象是用浮点底层类型创建的，因此我们使用`mutable_data<float>()`成员函数来访问张量的内部存储。使用`cv::Mat::data`类型成员访问 OpenCV 图像数据。我们将图像数据转换为浮点类型，因为这个成员变量属于`unsigned char *`类型。像素的数据是用标准的`std::copy_n`功能复制的。最后，在最后一段代码中，我们返回了张量对象。

ONNX 格式示例中使用的另一个重要函数是可以从 synset 文件中读取类定义的函数。我们将在下一节讨论这个问题。

# 正在读取类定义文件

我们在这个例子中使用`ReadClasses`函数来加载对象的地图。这里，关键字是图像类索引，值是文本类描述。这个函数很简单，它逐行读取 synset 文件。在这样的文件中，每行包含一个数字和一个类描述字符串，用空格字符分隔。下面的代码显示了它的定义:

```cpp
using Classes = std::map<size_t, std::string>;
Classes ReadClasses(const std::string& file_name) {
  Classes classes;
  std::ifstream file(file_name);
  if (file) {
    std::string line;
    std::string id;
    std::string label;
    std::string token;
    size_t idx = 1;
    while (std::getline(file, line)) {
       std::stringstream line_stream(line);
       size_t i = 0;
       while (std::getline(line_stream, token, ' ')) {
          switch (i) {
            case 0:
              id = token;
              break;
            case 1:
              label = token;
              break;
          }
          token.clear();
          ++ i;
       }
       classes.insert({idx, label});
       ++ idx;
    }
  }
  return classes;
}
```

请注意，我们在内部`while`循环中使用了`std::getline`函数来标记单行字符串。为此，我们指定了定义分隔符字符值的第三个参数。

# 摘要

在本章中，我们学习了如何在不同的 ML 框架中保存和加载模型参数。我们看到我们在`Shogun`、`Shark-ML`、`Dlib`和`PyTorch`库中使用的所有框架都有一个用于模型参数序列化的 API。通常，这些是处理模型对象和一些输入输出流的非常简单的函数。此外，我们还讨论了另一种类型的序列化 API，它可以用来保存和加载整个模型架构。在撰写本文时，我们使用的框架并不完全支持这样的功能。`Shogun toolkit`可以从 JSON 描述中加载神经网络架构，但不能导出它们。`Dlib`库可以导出 XML 格式的神经网络，但无法加载。`PyTorch` C++ API 缺少一个支持导出的模型架构，但是它可以加载和评估已经通过其 TorchScript 功能从 Python API 导出的模型架构。然而`PyTorch`库确实提供了对 Caffe2 库 API 的访问，它允许我们从 C++ 加载和评估以 ONNX 格式保存的模型。

我们简要地看了 ONNX 格式，并意识到它是在不同 ML 框架之间共享模型的一种相当流行的格式。它支持几乎所有用于有效序列化复杂神经网络模型的操作和对象。在撰写本文时，它得到了所有流行的 ML 框架的支持，如`TensorFlow`、`PyTorch`、`MXNet`等。此外，微软提供了 ONNX 运行时实现，它允许我们运行 ONNX 模型的推理，而无需依赖任何其他框架。

在本章的最后，我们开发了一个 C++ 应用，可以用来运行 ResNet-50 模型的推理，该模型是用 MXNet 框架训练的，并以 ONNX 格式导出。这个应用是用 Caffe2 C++ API 实现的，以便加载模型并在加载的图像上对其进行评估以进行分类。

在下一章中，我们将讨论如何将使用 C++ 库开发的 ML 模型部署到移动设备和服务器实例中。

# 进一步阅读

*   Shark-ML 文档:[http://www . shark-ML . org/sphinx _ pages/build/html/rest _ sources/tutorials/tutorials . html](http://www.shark-ml.org/sphinx_pages/build/html/rest_sources/tutorials/tutorials.html)
*   Dlib 文件: [http://dlib.net/](http://dlib.net/)
*   幕府工具包文档:[https://www.shogun-toolbox.org/](https://www.shogun-toolbox.org/)
*   PyTorch C++ API： [https://pytorch.org/cppdocs/](https://pytorch.org/cppdocs/)
*   ONNX 官方页面:[https://onnx.ai/](https://onnx.ai/)
*   ONNX 模型动物园:[https://github.com/onnx/models](https://github.com/onnx/models)
*   用于图像分类的 ONNX ResNet 模型:[https://github . com/ONNX/models/tree/master/vision/classing/ResNet](https://github.com/onnx/models/tree/master/vision/classification/resnet)
*   caf 2 教程:https://github . com/leonardvanriel/caf 2 _ CPP _ tutorial/