# 三、超越基本神经网络——自编码器和 RBM

既然我们已经学会了如何建立和训练一个简单的神经网络，我们应该建立一些适合现实世界问题的模型。

在本章中，我们将讨论如何建立一个模型来识别和生成笔迹，以及执行协同过滤。

在本章中，我们将介绍以下主题：

*   加载数据–经**修改的国家标准与技术研究所**（**MNIST**数据库
*   手写识别神经网络的建立
*   构建自编码器–生成 MNIST 数字
*   构建用于 Netflix 式协同过滤的**受限 Boltzmann 机器**（**RBM**）

# 加载数据–MNIST

Before we can even begin to train or build our model, we first need to get some data. As it turns out, a lot of people have made data available online for us to use for this purpose. One of the best-curated datasets around is MNIST, which we will use for the first two examples in this chapter.

我们将学习如何下载 MNIST 并将其加载到我们的 Go 程序中，以便在我们的模型中使用它。

# 什么是 MNIST？

在本章中，我们将使用一个称为 MNIST 数据库的流行数据集。Yann LeCun、Corinna Cortes 和 Christopher Burges 已在[上提供了该信息 http://yann.lecun.com/exdb/mnist](http://yann.lecun.com/exdb/mnist) 。

这个数据库之所以得名，是因为它是由两个包含手写数字黑白图像的数据库混合而成的。这是一个理想数据集的示例，它已经为我们进行了良好的预处理和格式化，以便我们可以立即开始使用它。下载时，它已经分为训练集和测试（验证）集，训练集中有 60000 个带标签的示例，测试集中有 10000 个带标签的示例。

每个图像正好是 28 x 28 像素，包含 1 到 255 之间的值（反映像素强度或灰度值）。这大大简化了我们的工作，因为这意味着我们可以立即将图像放入矩阵/张量中，并开始在其上训练我们的模型。

# 加载 MNIST

Gorgonia 在其`examples`文件夹中附带了一个 MNIST 加载器，我们可以通过在导入中添加以下内容轻松地在代码中使用它：

```go
"gorgonia.org/gorgonia/examples/mnist"
```

Then, we can add the following lines to our code:

```go
var inputs, targets tensor.Tensor
var err error
inputs, targets, err = mnist.Load(“train”, “./mnist/, “float64”)
```

这会将我们的图像加载到名为`inputs`的张量中，将我们的标签加载到名为`targets`的张量中（假设您已将相关文件解压缩到`mnist`文件夹中，该文件夹应该是运行可执行文件的地方）。

在本例中，我们正在加载 MNIST 的训练集，因此它将为图像生成一个大小为 60000 x 784 的二维张量，为标签生成另一个大小为 60000 x 10 的二维张量。Gorgonia 中的加载程序也将有助于将所有数字重新缩放到 0 到 1 之间；在训练模型时，我们喜欢小的、标准化的数字。

# 手写识别神经网络的建立

现在我们已经加载了所有有用的数据，让我们好好利用它。因为它充满了手写数字，我们应该建立一个模型来识别这个手写体和它所说的内容。

在[第 2 章](02.html)*中，什么是神经网络？如何训练神经网络**？*我们演示了如何构建一个简单的神经网络。现在，是时候建立更实质性的东西了：从 MNIST 数据库识别笔迹的模型。

# 模型结构简介

First, let's think back to the original example: we had a single-layer network, which we wanted to get from a 4 x 3 matrix to a 4 x 1 vector. Now, we have to get from an MNIST image that is 28 x 28 pixels to one single number. This number is our network's guess about which number the image actually represents.

下面的屏幕截图展示了我们可以在 MNIST 数据中找到的一个粗略示例：标签旁边手写数字的一些灰度图像（单独存储）：

![](img/5b39bdad-3532-4e10-92e3-71ed72237ed9.png)

# 层

请记住，我们使用的是张量，因此我们需要将这些数据与这些数据格式联系起来。单个图像可以是 28 x 28 矩阵，也可以是 784 值长向量。我们的标签目前是 0 到 9 之间的整数。然而，由于这些是真正的分类值，而不是从 0 到 9 的连续数值，因此最好将结果转化为向量。与其要求我们的模型直接产生这个结果，我们应该把输出看作是 10 个值的向量，位置上的 1 告诉我们它认为是哪个数字。

这给了我们正在使用的参数；我们必须输入 784 个值，然后从经过培训的网络中获取 10 个值。在本例中，我们按照下图构建层：

![](img/e1e90645-3505-49bb-80cf-6b5a953f34fa.png)

This structure would typically be described as a network with two hidden layers of **300** and **100** units each. This can be implemented in Gorgonia with the following code:

```go
type nn struct {
    g *gorgonia.ExprGraph
    w0, w1, w2 *gorgonia.Node

    out *gorgonia.Node
    predVal gorgonia.Value
}

func newNN(g *gorgonia.ExprGraph) *nn {
    // Create node for w/weight
    w0 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(784, 300), gorgonia.WithName("w0"), gorgonia.WithInit(gorgonia.GlorotN(1.0)))
   w1 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(300, 100), gorgonia.WithName("w1"), gorgonia.WithInit(gorgonia.GlorotN(1.0)))
    w2 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(100, 10), gorgonia.WithName("w2"), gorgonia.WithInit(gorgonia.GlorotN(1.0)))

    return &nn{
        g: g,
        w0: w0,
        w1: w1,
        w2: w2,
    }
}
```

我们还使用了您在[第 2 章](02.html)*中学习的 ReLU 激活函数，什么是神经网络，如何训练神经网络？*。*事实证明，ReLU 非常适合这项任务。因此，我们网络的前向传递如下所示：*

 ```go
func (m *nn) fwd(x *gorgonia.Node) (err error) {
    var l0, l1, l2 *gorgonia.Node
    var l0dot, l1dot*gorgonia.Node

    // Set first layer to be copy of input
    l0 = x

    // Dot product of l0 and w0, use as input for ReLU
    if l0dot, err = gorgonia.Mul(l0, m.w0); err != nil {
        return errors.Wrap(err, "Unable to multiply l0 and w0")
    }

    // Build hidden layer out of result
    l1 = gorgonia.Must(gorgonia.Rectify(l0dot))

    // MOAR layers

    if l1dot, err = gorgonia.Mul(l1, m.w1); err != nil {
        return errors.Wrap(err, "Unable to multiply l1 and w1")
    }
    l2 = gorgonia.Must(gorgonia.Rectify(l2dot))

    var out *gorgonia.Node
    if out, err = gorgonia.Mul(l2, m.w2); err != nil {
        return errors.Wrapf(err, "Unable to multiply l2 and w2")
    }

    m.out, err = gorgonia.SoftMax(out)
    gorgonia.Read(m.out, &m.predVal)
    return
}
```

您可以看到，我们网络的最终输出被传递给 Gorgonia`SoftMax`函数。通过将所有值重新调整为介于`0`和`1`之间的值，将输出压缩为`1`之和。这是有用的，因为我们正在使用 ReLU 激活单元，它可以进入非常大的数量。我们希望有一种简单的方法使我们的值尽可能接近我们的标签，标签如下所示：

```go
[ 0.1 0.1 0.1 1.0 0.1 0.1 0.1 0.1 0.1 ]
```

由`SoftMax`训练的模型将产生如下值：

```go
[ 0 0 0 0.999681 0 0.000319 0 0 0 0 ]
```

通过取该向量中具有最大值的元素，我们可以看到预测标签为`4`。

# 训练

训练模型需要几个重要的组件。我们有输入，但我们还需要一个损失函数和一种解释输出的方法，以及为我们的模型训练过程设置一些其他超参数。

# 损失函数

损失函数在训练我们的网络中起着重要作用。我们没有详细讨论它们，但它们的作用是在模型出错时告诉模型，以便从错误中吸取教训。

在这个例子中，我们使用的是一个版本的交叉熵损失，它已被修改为尽可能有效。

It should be noted that cross-entropy loss would typically be expressed in pseudocode, such as the following:

```go
crossEntropyLoss = -1 * sum(actual_y * log(predicted_y))
```

然而，在我们的案例中，我们将采用更简单的版本：

```go
loss = -1 * mean(actual_y * predicted_y)
```

因此，我们正在实现损失函数，如下所示：

```go
losses, err := gorgonia.HadamardProd(m.out, y)
if err != nil {
    log.Fatal(err)
}
cost := gorgonia.Must(gorgonia.Mean(losses))
cost = gorgonia.Must(gorgonia.Neg(cost))

// we wanna track costs
var costVal gorgonia.Value
gorgonia.Read(cost, &costVal)
```

作为练习，您可以将损失函数修改为更常用的交叉熵损失，并比较结果。

# 时代、迭代和批量大小

由于我们的数据集现在要大得多，我们还需要考虑训练它的实用性。逐项进行培训是可以的，但我们也可以分批进行培训。我们不需要在 MNIST 中对所有 60000 个项目进行培训，而是可以将数据分成 600 个迭代，每个迭代批处理 100 个项目。对于我们的数据集，这意味着输入模型 100 x 784 矩阵作为输入，而不是 784 值的长向量。我们也可以为它提供一个 100 x 28 x 28 的三维张量，但我们将在后面的章节中讨论一个模型架构，该架构充分利用了这种结构。

因为我们是在编程语言中完成这项工作的，所以我们可以按如下方式构建一个循环：

```go
for b := 0; b < batches; b++ {
    start := b * bs
    end := start + bs
    if start >= numExamples {
        break
    }
    if end > numExamples {
        end = numExamples
    }
}
```

然后，在每个循环中，我们可以插入我们的逻辑来提取必要的信息，以便输入到我们的机器中：

```go
var xVal, yVal tensor.Tensor
if xVal, err = inputs.Slice(sli{start, end}); err != nil {
    log.Fatal("Unable to slice x")
}

if yVal, err = targets.Slice(sli{start, end}); err != nil {
    log.Fatal("Unable to slice y")
}
// if err = xVal.(*tensor.Dense).Reshape(bs, 1, 28, 28); err != nil {
// log.Fatal("Unable to reshape %v", err)
// }
if err = xVal.(*tensor.Dense).Reshape(bs, 784); err != nil {
    log.Fatal("Unable to reshape %v", err)
}

gorgonia.Let(x, xVal)
gorgonia.Let(y, yVal)
if err = vm.RunAll(); err != nil {
    log.Fatalf("Failed at epoch %d: %v", i, err)
}
solver.Step(m.learnables())
vm.Reset()
```

在深度学习中你会听到很多的另一个术语是“时代”。Epochs 实际上只是将输入数据多次运行到数据中。如果您还记得，梯度下降是一个迭代过程：它严重依赖于重复来收敛到最优解。这意味着，尽管只有 60000 张训练图像，但我们有一个简单的方法来改进我们的模型：我们可以重复这个过程多次，直到我们的网络融合。

我们当然可以用几种不同的方法来处理这个问题。例如，当前一个历元和当前历元之间的损失函数差异足够小时，我们可以停止重复。我们还可以采用冠军挑战者的方法，并从我们的测试集上获得作为冠军出现的各个时代的权重。然而，为了保持示例的简单性，我们将选择任意数量的纪元；在这种情况下，100。

当我们开始时，让我们添加一个进度条，以便我们可以观看我们的模型列车：

```go
batches := numExamples / bs
log.Printf("Batches %d", batches)
bar := pb.New(batches)
bar.SetRefreshRate(time.Second / 20)
bar.SetMaxWidth(80)

for i := 0; i < *epochs; i++ {
    // for i := 0; i < 1; i++ {
    bar.Prefix(fmt.Sprintf("Epoch %d", i))
    bar.Set(0)
    bar.Start()
    // put iteration and batch logic above here
    bar.Update()
    log.Printf("Epoch %d | cost %v", i, costVal)
}
```

# 测试和验证

Training is all well and good, but we also need to know whether or not our model is actually doing what it claims to be doing. We can reuse our training code, but let's make a few changes.

首先，让我们删除`solver`命令。我们正在测试我们的模型，而不是训练它，所以我们不应该更新权重：

```go
solver.Step(m.learnables())
```

其次，让我们从数据集中实际获取一个图像，并将其保存到一个方便的文件中：

```go
for j := 0; j < xVal.Shape()[0]; j++ {
    rowT, _ := xVal.Slice(sli{j, j + 1})
    row := rowT.Data().([]float64)

    img := visualizeRow(row)

    f, _ := os.OpenFile(fmt.Sprintf("images/%d - %d - %d - %d.jpg", b, j, rowLabel, rowGuess), os.O_CREATE|os.O_WRONLY|os.O_TRUNC, 0644)
    jpeg.Encode(f, img, &jpeg.Options{jpeg.DefaultQuality})
    f.Close()
}
```

如您所见，以下几点是正确的：

*   `b`是我们的批号
*   `j`是该批次中的项目编号
*   `rowLabel` is the MNIST-provided label
*   `rowGuess`是我们模型的猜测还是预测

现在，让我们添加一些方法，将数据标签和预测提取为更易于阅读的格式（即，作为 0 到 9 的整数）。

对于我们的数据标签，让我们添加以下内容：

```go
yRowT, _ := yVal.Slice(sli{j, j + 1})
yRow := yRowT.Data().([]float64)
var rowLabel int
var yRowHigh float64

for k := 0; k < 10; k++ {
    if k == 0 {
        rowLabel = 0
        yRowHigh = yRow[k]
    } else if yRow[k] > yRowHigh {
        rowLabel = k
        yRowHigh = yRow[k]
    }
}
```

对于我们的预测，我们首先需要将它们提取成熟悉的格式。在这种情况下，让我们将它们放在一个张量中，这样我们就可以重用所有以前的代码：

```go
arrayOutput := m.predVal.Data().([]float64)
yOutput := tensor.New(
            tensor.WithShape(bs, 10),                                             tensor.WithBacking(arrayOutput)
            )
```

请注意，`m.predVal`的输出包含我们的预测值，是一个`float64`数组。还可以检索对象的原始形状，这有助于生成正确形状的张量。在本例中，我们已经知道形状，所以我们将直接输入这些参数。

当然，预测代码类似于从预处理的 MNIST 数据集中提取标签：

```go
// get prediction
predRowT, _ := yOutput.Slice(sli{j, j + 1})
predRow := predRowT.Data().([]float64)
var rowGuess int
var predRowHigh float64

// guess result
for k := 0; k < 10; k++ {
    if k == 0 {
        rowGuess = 0
        predRowHigh = predRow[k]
    } else if predRow[k] > predRowHigh {
        rowGuess = k
        predRowHigh = predRow[k]
    }
}
```

对于所有这些艰苦的工作，您将得到一个文件夹，其中充满了图像文件，并带有以下标签和猜测：

![](img/67b13aba-0122-4f41-9098-22e70d235005.png)

您将看到，在当前的形式中，我们的模型与一些（可能很差的）笔迹作斗争。

# 仔细观察

或者，您可能还想检查预测结果，以便更好地了解模型中发生的情况。在这种情况下，您可能希望将结果提取到一个`.csv`文件中，您可以使用以下代码执行此操作：

```go
arrayOutput := m.predVal.Data().([]float64)
yOutput := tensor.New(tensor.WithShape(bs, 10), tensor.WithBacking(arrayOutput))

file, err := os.OpenFile(fmt.Sprintf("%d.csv", b), os.O_CREATE|os.O_WRONLY, 0777)
if err = xVal.(*tensor.Dense).Reshape(bs, 784); err != nil {
 log.Fatal("Unable to create csv", err)
}
defer file.Close()
var matrixToWrite [][]string

for j := 0; j < yOutput.Shape()[0]; j++ {
  rowT, _ := yOutput.Slice(sli{j, j + 1})
  row := rowT.Data().([]float64)
  var rowToWrite []string

  for k := 0; k < 10; k++ {
      rowToWrite = append(rowToWrite, strconv.FormatFloat(row[k], 'f', 6, 64))
  }
  matrixToWrite = append(matrixToWrite, rowToWrite)
}

csvWriter := csv.NewWriter(file)
csvWriter.WriteAll(matrixToWrite)
csvWriter.Flush()
```

在下面的屏幕截图和代码输出中可以看到违规数字的输出。

The following is the screenshot output:

![](img/83c522b4-e303-45c7-95d7-6be78ea024c0.png)

您还可以在代码中观察相同的输出：

```go
[ 0  0  0.000457  0.99897  0  0  0  0.000522  0.000051  0 ]
```

类似地，您可以看到它从一个好的猜测中发生了轻微的波动，如以下屏幕截图所示：

![](img/84ce9f72-9a9a-4ef9-bbc8-384ba1e4bb57.png)

在代码格式中，这也是相同的：

```go
[0 0 0 0 0 0 0 1 0 0]
```

# 练习

我们已经从[第 2 章](02.html)*扩展了我们的简单示例，什么是神经网络，如何训练神经网络？*相当多。在这一点上，这将是一个好主意，有一点乐趣。尝试以下方法并亲自观察发生了什么，以便更好地了解您的选择可能产生的影响。例如，您应该尝试以下所有操作：

*   改变损失函数
*   更改每个层中的单元数
*   更改层的数量
*   更改纪元的数量
*   更改批处理大小

# 构建自编码器–生成 MNIST 数字

自编码器正是它听起来的样子：它自动学习如何对数据进行编码。通常，自编码器的目标是训练它以更少的维度自编码数据，或者从数据中挑选某些细节或其他有用的东西。它还可用于消除数据中的噪声或压缩数据。

一般来说，自编码器有两个部分；一半是编码器，一半是解码器。我们倾向于串联训练这两个部分，目标是使解码器的输出尽可能接近我们的输入。

# 层

就像以前一样，我们需要考虑我们的投入和产出。我们再次使用 MNIST，因为编码数字是一个有用的特性。因此，我们知道我们的输入是 784 像素，我们知道我们的输出也必须有 784 像素。

既然我们已经有了帮助函数来将输入和输出解码成张量，我们就可以把这项工作放在一边，直接转到我们的神经网络。我们的网络如下：

![](img/59013001-e839-43de-83d6-0d5eae84747c.png)

We can reuse most of our code from the last example and just change up our layers:

```go
func newNN(g *gorgonia.ExprGraph) *nn {
    // Create node for w/weight
    w0 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(784, 128), gorgonia.WithName("w0"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))
    w1 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(128, 64), gorgonia.WithName("w1"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))
    w2 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(64, 128), gorgonia.WithName("w2"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))
    w3 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(128, 784), gorgonia.WithName("w3"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))

    return &nn{
        g: g,
        w0: w0,
        w1: w1,
        w2: w2,
        w3: w3,
    }
}
```

然而，这一次，我们将不使用 ReLU 激活函数，因为我们知道我们的输出必须是 0 和 1。我们正在使用`Sigmoid`激活功能，因为这为我们提供了方便的输出。正如您在下面的代码块中所看到的，当我们对每一层使用 ReLU 时，您也可以在除最后一层之外的任何地方使用 ReLU 激活函数，因为理想情况下，输出层应限制为介于`0`和`1`之间的值：

```go
func (m *nn) fwd(x *gorgonia.Node) (err error) {
    var l0, l1, l2, l3, l4 *gorgonia.Node
    var l0dot, l1dot, l2dot, l3dot *gorgonia.Node

    // Set first layer to be copy of input
    l0 = x

    // Dot product of l0 and w0, use as input for Sigmoid
    if l0dot, err = gorgonia.Mul(l0, m.w0); err != nil {
        return errors.Wrap(err, "Unable to multiple l0 and w0")
    }
    l1 = gorgonia.Must(gorgonia.Sigmoid(l0dot))

    if l1dot, err = gorgonia.Mul(l1, m.w1); err != nil {
        return errors.Wrap(err, "Unable to multiple l1 and w1")
    }
    l2 = gorgonia.Must(gorgonia.Sigmoid(l1dot))

    if l2dot, err = gorgonia.Mul(l2, m.w2); err != nil {
        return errors.Wrap(err, "Unable to multiple l2 and w2")
    }
    l3 = gorgonia.Must(gorgonia.Sigmoid(l2dot))

    if l3dot, err = gorgonia.Mul(l3, m.w3); err != nil {
        return errors.Wrap(err, "Unable to multiple l3 and w3")
    }
    l4 = gorgonia.Must(gorgonia.Sigmoid(l3dot))

    // m.pred = l3dot
    // gorgonia.Read(m.pred, &m.predVal)
    // return nil

    m.out = l4
    gorgonia.Read(l4, &m.predVal)
    return

}
```

# 训练

和以前一样，我们需要一个损失函数来进行训练。自编码器的输入和输出也不同！

# 损失函数

This time, our loss function is different. We are using the mean of the squared errors that have pseudocode, which looks something like this:

```go
mse = sum( (actual_y - predicted_y) ^ 2 ) / num_of_y
```

这可以在 Gorgonia 中实现，如下所示：

```go
losses, err := gorgonia.Square(gorgonia.Must(gorgonia.Sub(y, m.out)))
if err != nil {
    log.Fatal(err)
}
cost := gorgonia.Must(gorgonia.Mean(losses))
```

# 输入和输出

注意，这一次，我们的输入和输出是相同的。这意味着我们不需要获取数据集的标签，并且当我们运行虚拟机时，我们可以将`x`和`y`都设置为我们的输入数据：

```go
gorgonia.Let(x, xVal)
gorgonia.Let(y, xVal)
```

# 时代、迭代和批量大小

这个问题很难解决。您会发现，为了了解我们的输出是如何改进的，在这里运行几个阶段是非常有价值的，因为我们可以在进行培训时编写模型的输出，代码如下：

```go
for j := 0; j < 1; j++ {
    rowT, _ := yOutput.Slice(sli{j, j + 1})
    row := rowT.Data().([]float64)

    img := visualizeRow(row)

    f, _ := os.OpenFile(fmt.Sprintf("training/%d - %d - %d training.jpg", j, b, i), os.O_CREATE|os.O_WRONLY|os.O_TRUNC, 0644)
    jpeg.Encode(f, img, &jpeg.Options{jpeg.DefaultQuality})
    f.Close()
}
```

在我们培训模型时，我们现在可以看到它在每个时代都有所改进：

![](img/4a955319-e0f1-4444-a4ac-b86763072c17.png)

You can see that we start with almost pure noise, and then very quickly get to a blurry shape, which slowly gets sharper as we progress through the epochs.

# 测试和验证

我们不会详细介绍测试代码，因为我们已经介绍了如何从输出中获取图像，但请注意，`y`现在也有`784`列宽：

```go
arrayOutput := m.predVal.Data().([]float64)
yOutput := tensor.New(tensor.WithShape(bs, 784), tensor.WithBacking(arrayOutput))

for j := 0; j < yOutput.Shape()[0]; j++ {
    rowT, _ := yOutput.Slice(sli{j, j + 1})
    row := rowT.Data().([]float64)

    img := visualizeRow(row)

    f, _ := os.OpenFile(fmt.Sprintf("images/%d - %d output.jpg", b, j), os.O_CREATE|os.O_WRONLY|os.O_TRUNC, 0644)
    jpeg.Encode(f, img, &jpeg.Options{jpeg.DefaultQuality})
    f.Close()
}
```

现在，这里是有趣的部分；从我们的自编码器中获取结果：

![](img/99aa8eea-c18d-4fb1-b57f-766adac96015.png)

您会注意到，结果的定义明显不如输入图像。但是，它也会去除图像中的一些噪声！

# 为 Netflix 风格的协同过滤构建 RBM

现在我们将探索一种不同的无监督学习技术，在我们的示例中，这种技术能够处理反映给定用户组对特定内容偏好的数据。本节将介绍有关网络架构和概率分布的新概念，以及如何在推荐系统的实际实现中使用它们，特别是推荐特定用户可能感兴趣的电影。

# Introduction to RBMs

By their textbook definition, RBMs are **probabilistic graphical models**, which—given what we've already covered regarding the structure of neural networks—simply means a bunch of neurons that have weighted connections to another bunch of neurons.

这些网络有两层：一层**可见**层和一层**隐藏**层。可见层是将数据输入的层，而隐藏层是不直接暴露于数据的层，但必须为手头的任务开发有意义的数据表示。这些任务包括降维、协同过滤、二进制分类等。受限意味着连接不是横向的（即，在同一层的节点之间），而是每个隐藏单元连接到网络各层中的每个可见单元。图形是无向的，这意味着数据不会固定在一个方向上流动。这说明如下：

![](img/0da31393-6c89-4254-b0b9-c37b5f52c3a5.png)

训练过程相当简单，不同于我们的普通神经网络，因为我们不仅要进行预测，测试预测的强度，然后通过网络反向传播误差。就我们的 RBM 而言，这只是故事的一半。

为了进一步分解训练过程，RBM 的向前传球如下所示：

*   可见层节点值乘以连接权重
*   将隐藏单位偏差添加到结果值的所有节点的总和（强制激活）
*   应用激活功能
*   给出了隐藏节点的值（激活概率）

如果这是一个深层网络，则隐藏层的输出将作为输入传递到另一层。这种架构的一个例子是一个深刻的信念网络（To.T2A.DBN OutT3），这是 Geoff Hinton 和他的团队在多伦多大学的另一个重要作品，它使用多个 RBMS 相互堆叠在一起。

Our RBM is not, however, a deep network. Thus, we will do something different with the hidden unit output. We will use it to attempt to reconstruct the input (visible units) of the network. We will do this by using the hidden units as input for the backward or reconstruction phase of network training.

向后传球看起来与向前传球类似，通过以下步骤执行：

1.  将隐藏层的激活作为输入乘以连接权重
2.  可见单位偏差被添加到乘法结果的所有节点之和
3.  计算重建误差，或预测输入和实际输入之间的差异（我们通过向前传递知道）
4.  该误差用于更新权重，以尽量减小重建误差

这两种状态（隐藏层的预测激活和可见层的预测输入）一起形成联合概率分布。

如果您在数学上倾向于，则两个过程的公式如下所示：

*   **向前传球**：a（隐藏节点激活）的概率被赋予一个加权输入*x*：

*p（a|x；w）*

*   **Backward pass**: The probability of *x* (visible layer input) is given a weighted activation, *a*:

*p（x；a；w）*

*   因此，联合概率分布仅由以下公式给出：

*p（a，x）*

因此，重建可以被认为与我们目前讨论的技术不同。它既不是回归（预测给定输入集的连续输出），也不是分类（为给定输入集应用类标签）。我们在重建阶段计算误差的方法清楚地说明了这一点。我们不仅仅把投入和预测投入作为一个实数（产出的差异）来衡量；相反，我们比较了`x`输入的所有值与*重构*输入的所有值的概率分布。我们使用一种称为**库尔贝克-莱布勒散度**的方法来进行这种比较。本质上，这种方法测量每个概率分布曲线下不重叠的面积。然后，我们尝试进行重量调整，并重新运行训练循环，以减少这种差异（错误），从而使曲线更接近，如下图所示：

![](img/b661799c-c5b3-4e91-a0cf-65d936f27cc7.png)

在培训结束时，当这个错误被最小化时，我们就可以预测某个特定用户可能会对哪些其他电影竖起大拇指。

# 用于协同过滤的 RBMs

如本节导言所述，成果管理制可在多种情况下使用，可采用有监督或无监督的方式。**协同过滤**是一种预测用户偏好的策略，其基本假设是，如果用户*a*喜欢项目*Z*，而用户*B*也喜欢项目*Z*，那么用户*B*可能也喜欢该用户*的其他东西 A*喜欢（比如，项目*Y*。

每当 Netflix 向我们推荐一些东西，或者每次 Amazon 向我们推荐一台新的真空吸尘器时（当然，因为我们买了一台真空吸尘器，现在很明显我们在家用电器中使用它），我们都会看到这个用例在起作用。

既然我们已经介绍了 RBM 是什么、它们是如何工作的以及它们是如何使用的一些理论，那么让我们开始构建一个 RBM 吧！

# 准备我们的数据–GroupLens 电影收视率

我们正在使用 GroupLens 数据集。它包含从 MovieLens 收集的用户、电影和收视率的集合（[http://www.movielens.org A. T1），由明尼苏达大学的一些学术研究人员管理。](http://www.movielens.org)

我们需要解析用冒号分隔的`ratings.dat`文件，用于`userids`、`ratings`和`movieids`。然后我们可以将`movieids`与`movies.dat`中的匹配。

首先，让我们看看构建电影索引所需的代码：

```go
package main

import (

  // "github.com/aotimme/rbm"

  "fmt"
  "log"
  "math"
  "strconv"

  "github.com/yunabe/easycsv"
  g "gorgonia.org/gorgonia"
  "gorgonia.org/tensor"
)

var datasetfilename string = "dataset/cleanratings.csv"
var movieindexfilename string = "dataset/cleanmovies.csv"    

func BuildMovieIndex(input string) map[int]string {

  var entrycount int
  r := easycsv.NewReaderFile(input, easycsv.Option{
    Comma: ',',
  })

  var entry struct {
    Id int `index:"0"`
    Title string `index:"1"`
  }

  //fix hardcode
  movieindex := make(map[int]string, 3952)

  for r.Read(&entry) {
    // fmt.Println(entry)
    movieindex[entry.Id] = entry.Title
    // entries = append(entries, entry)
    entrycount++
  }

  return movieindex

}
```

现在，我们编写一个函数来导入原始数据并将其转换为一个*m*x*n*矩阵。在这种情况下，行表示单个用户，列表示数据集中每部电影的（标准化）评分：

```go
func DataImport(input string) (out [][]int, uniquemovies map[int]int) {
  //
  // Initial data processing
  //
  // import from CSV, read into entries var
  r := easycsv.NewReaderFile(input, easycsv.Option{
    Comma: ',',
  })

  var entry []int
  var entries [][]int
  for r.Read(&entry) {
    entries = append(entries, entry)
  }

  // maps for if unique true/false
  seenuser := make(map[int]bool)
  seenmovie := make(map[int]bool)

  // maps for if unique index
  uniqueusers := make(map[int]int)
  uniquemovies = make(map[int]int)

  // counters for uniques
  var uniqueuserscount int = 0
  var uniquemoviescount int = 0

  // distinct movie lists/indices
  for _, e := range entries {
    if seenmovie[e[1]] == false {
      uniquemovies[uniquemoviescount] = e[1]
      seenmovie[e[1]] = true
      uniquemoviescount++
    } else if seenmovie[e[1]] == true {
      // fmt.Printf("Seen movie %v before, aborting\n", e[0])
      continue
    }
  }
  // distinct user lists/indices
  for _, e := range entries {
    if seenuser[e[0]] == false {
      uniqueusers[uniqueuserscount] = e[0]
      seenuser[e[0]] = true
      uniqueuserscount++
      // uniqueusers[e[0]] =
    } else if seenuser[e[0]] == true {
      // fmt.Printf("Seen user %v before, aborting\n", e[0])
      continue
    }
  }

  uservecs := make([][]int, len(uniqueusers))
  for i := range uservecs {
    uservecs[i] = make([]int, len(uniquemovies))
  }
```

以下是主循环，我们处理 CSV 中的每一行，然后使用正确的索引添加到用户的主片段和电影分级的子片段：

```go
  var entriesloop int
  for _, e := range entries {
    // hack - wtf
    if entriesloop%100000 == 0 && entriesloop != 0 {
      fmt.Printf("Processing rating %v of %v\n", entriesloop, len(entries))
    }
    if entriesloop > 999866 {
      break
    }
    var currlike int

    // normalisze ratings
    if e[2] >= 4 {
      currlike = 1
    } else {
      currlike = 0
    }

    // add to a user's vector of index e[1]/movie num whether current movie is +1
    // fmt.Println("Now looping uniquemovies")
    for i, v := range uniquemovies {
      if v == e[1] {
        // fmt.Println("Now setting uservec to currlike")
        // uservec[i] = currlike
        // fmt.Println("Now adding to uservecs")
        uservecs[e[0]][i] = currlike
        break
      }
    }
    // fmt.Printf("Processing rating %v of %v\n", entriesloop, len(entries))
    entriesloop++
  }
  // fmt.Println(uservecs)
  // os.Exit(1)

  // fmt.Println(entry)
  if err := r.Done(); err != nil {
    log.Fatalf("Failed to read a CSV file: %v", err)
  }
  // fmt.Printf("length uservecs %v and uservecs.movies %v", len(uservecs))
  fmt.Println("Number of unique users: ", len(seenuser))
  fmt.Println("Number of unique movies: ", len(seenmovie))
  out = uservecs

  return

}
```

# 在戈尔戈尼亚建立 RBM

现在，我们已经清理了数据，创建了培训或测试集，并编写了生成网络所需输入所需的代码，我们可以开始编写 RBM 本身。

首先，我们从我们现在的标准`struct`开始，我们将网络的各个组件连接到脚手架上：

```go
const cdS = 1
type ggRBM struct {
    g *ExprGraph
    v *Node // visible units
    vB *Node // visible unit biases - same size tensor as v
    h *Node // hidden units
    hB *Node // hidden unit biases - same size tensor as h
    w *Node // connection weights
    cdSamples int // number of samples for contrastive divergence - WHAT ABOUT MOMENTUM
}
func (m *ggRBM) learnables() Nodes {
    return Nodes{m.w, m.vB, m.hB}
}
```

然后，我们添加附加到 RBM 的助手函数：

1.  首先，我们为我们的`ContrastiveDivergence`学习算法添加`func`（Gibbs 采样）：

```go
// Uses Gibbs Sampling
func (r *ggRBM) ContrastiveDivergence(input *Node, learnRate float64, k int) {
   rows := float64(r.TrainingSize)

 // CD-K
   phMeans, phSamples := r.SampleHiddenFromVisible(input)
   nvSamples := make([]float64, r.Inputs)
// iteration 0

   _, nvSamples, nhMeans, nhSamples := r.Gibbs(phSamples, nvSamples)

   for step := 1; step < k; step++ {

       /*nvMeans*/ _, nvSamples, nhMeans, nhSamples = r.Gibbs(nhSamples, nvSamples)

   }

   // Update weights
   for i := 0; i < r.Outputs; i++ {

       for j := 0; j < r.Inputs; j++ {

           r.Weights[i][j] += learnRate * (phMeans[i]*input[j] - nhMeans[i]*nvSamples[j]) / rows
       }
       r.Biases[i] += learnRate * (phSamples[i] - nhMeans[i]) / rows
   }

   // update hidden biases
   for j := 0; j < r.Inputs; j++ {

       r.VisibleBiases[j] += learnRate * (input[j] - nvSamples[j]) / rows
   }
}
```

2.  现在，我们添加函数来采样各自的可见或隐藏层：

```go
func (r *ggRBM) SampleHiddenFromVisible(vInput *Node) (means []float64, samples []float64) {
   means = make([]float64, r.Outputs)
   samples = make([]float64, r.Outputs)
   for i := 0; i < r.Outputs; i++ {
       mean := r.PropagateUp(vInput, r.Weights[i], r.Biases[i])
       samples[i] = float64(binomial(1, mean))
       means[i] = mean
   }
   return means, samples
}

func (r *ggRBM) SampleVisibleFromHidden(hInput *Node) (means []float64, samples []float64) {
   means = make([]float64, r.Inputs)
   samples = make([]float64, r.Inputs)
   for j := 0; j < r.Inputs; j++ {
       mean := r.PropagateDown(hInput, j, r.VisibleBiases[j])
       samples[j] = float64(binomial(1, mean))
       means[j] = mean
   }
   return means, samples
}
```

3.  接下来，我们添加两个函数来处理权重更新的传播：

```go
func (r *ggRBM) PropagateDown(h *Node, j int, hB *Node) *Node {
   retVal := 0.0
   for i := 0; i < r.Outputs; i++ {
       retVal += r.Weights[i][j] * h0[i]
   }
   retVal += bias
   return sigmoid(retVal)
}

func (r *ggRBM) PropagateUp(v *Node, w *Node, vB *Node) float64 {
   retVal := 0.0
   for j := 0; j < r.Inputs; j++ {
       retVal += weights[j] * v0[j]
   }
   retVal += bias
   return sigmoid(retVal)
}
```

4.  Now, we add a function for Gibbs sampling (as used in our previous `ContrastiveDivergence` function) as well as a function to perform the reconstruction step in our network:

```go
func (r *ggRBM) Gibbs(h, v *Node) (vMeans []float64, vSamples []float64, hMeans []float64, hSamples []float64) {
   vMeans, vSamples = r.SampleVisibleFromHidden(r.h)
   hMeans, hSamples = r.SampleHiddenFromVisible(r.v)
   return
}

func (r *ggRBM) Reconstruct(x *Node) *Node {
   hiddenLayer := make([]float64, r.Outputs)
   retVal := make([]float64, r.Inputs)

   for i := 0; i < r.Outputs; i++ {
       hiddenLayer[i] = r.PropagateUp(x, r.Weights[i], r.Biases[i])
   }

   for j := 0; j < r.Inputs; j++ {
       activated := 0.0
       for i := 0; i < r.Outputs; i++ {
           activated += r.Weights[i][j] * hiddenLayer[i]
       }
       activated += r.VisibleBiases[j]
       retVal[j] = sigmoid(activated)
   }
   return retVal
}
```

5.  之后，我们添加了用于实例化 RBM 的函数：

```go
func newggRBM(g *ExprGraph, cdS int) *ggRBM {

   vT := tensor.New(tensor.WithBacking(tensor.Random(tensor.Int, 3952)), tensor.WithShape(3952, 1))

   v := NewMatrix(g,
       tensor.Int,
       WithName("v"),
       WithShape(3952, 1),
       WithValue(vT),
   )

   hT := tensor.New(tensor.WithBacking(tensor.Random(tensor.Int, 200)), tensor.WithShape(200, 1))

   h := NewMatrix(g,
       tensor.Int,
       WithName("h"),
       WithShape(200, 1),
       WithValue(hT),
   )

   wB := tensor.Random(tensor.Float64, 3952*200)
   wT := tensor.New(tensor.WithBacking(wB), tensor.WithShape(3952*200, 1))
   w := NewMatrix(g,
       tensor.Float64,
       WithName("w"),
       WithShape(3952*200, 1),
       WithValue(wT),
   )

   return &ggRBM{
       g: g,
       v: v,
       h: h,
       w: w,
       // hB: hB,
       // vB: vB,
       cdSamples: cdS,
   }
}
```

6.  最后，我们训练模型：

```go
func main() {
   g := NewGraph()
   m := newggRBM(g, cdS)
   data, err := ReadDataFile(datasetfilename)
   if err != nil {
       log.Fatal(err)
   }
   fmt.Println("Data read from CSV: \n", data)
   vm := NewTapeMachine(g, BindDualValues(m.learnables()...))
   // solver := NewVanillaSolver(WithLearnRate(1.0))
   for i := 0; i < 1; i++ {
       if vm.RunAll() != nil {
           log.Fatal(err)
       }
   }
}
```

在执行代码之前，我们需要对数据进行一点预处理。这是因为数据集中使用的分隔符是`::`，但我们希望将其更改为`,`。本章的存储库包括文件夹根目录中的`preprocess.sh`，它为我们执行以下操作：

```go
#!/bin/bash
export LC_CTYPE=C
export LANG=C
cat ratings.dat | sed 's/::/,/g' > cleanratings.csv
cat movies.dat | sed 's/,//g; s/::/,/g' > cleanmovies.csv
```

现在我们已经很好地格式化了数据，让我们执行 RBM 的代码，并观察如下输出：

![](img/89483ba0-81b9-4fc5-af89-96331c709723.png)

在这里，我们看到我们的数据导入功能处理收视率和电影索引文件，以及构建长度为`3706`的每用户向量，该向量索引所有用户的收视率（标准化为`0`/`1`：

![](img/1493bb5a-51fd-497f-b5ad-4f96229aafa2.png)

培训阶段完成后（此处设置为 1000 次迭代），RBM 为随机选择的用户生成一组建议。

您现在可以使用不同的超参数进行实验，并尝试输入您自己的数据！

# 总结

在本章中，我们学习了如何构建简单的多层神经网络和自编码器。我们还探讨了概率图形模型 RBM 的设计和实现，RBM 以无监督的方式用于创建电影推荐引擎。

强烈建议您在其他数据块上尝试这些模型和架构，以了解它们的性能。

In the next chapter, we will have a look at the hardware side of deep learning, and also find out how exactly CPUs and GPUs serve our computational needs.

# 进一步阅读

*   *Restricted Boltzmann Machines for Collaborative Filtering*, the original paper of the research group at the University of Toronto, available at [https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf](https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf)
*   *限制性玻尔兹曼机器模拟人类选择*，这篇论文探讨了 RBM 在模拟*人类选择*（在我们的例子中，是一种对电影类型的偏好）方面有效的概念，并提出了在其他领域的应用，如心理学，可在[上找到://papers.nips.cc/paper/5280-restricted-boltzmann-machines-modeling-human-choice.pdf](https://papers.nips.cc/paper/5280-restricted-boltzmann-machines-modeling-human-choice.pdf)*