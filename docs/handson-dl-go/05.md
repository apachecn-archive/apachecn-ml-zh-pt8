# 五、基于循环神经网络的下一个词的预测

到目前为止，我们已经介绍了一些基本的神经网络结构及其学习算法。这些是设计能够执行更高级任务（如机器翻译、语音识别、时间序列预测和图像分割）的网络所必需的构建块。在本章中，我们将介绍一类算法/架构，这些算法/架构由于能够对数据中的顺序依赖关系进行建模而擅长于这些任务和其他任务。

这些算法已经被证明是非常强大的，它们的变体在工业和消费者用例中得到了广泛的应用。这包括机器翻译、文本生成、命名实体识别和传感器数据分析。当你说*好的，谷歌！*或*嘿，Siri！*，在幕后，一种经过训练的**循环神经网络**（**RNN**正在进行推理。所有这些应用的共同主题是，这些序列（例如时间*x*处的传感器数据，或语料库中单词在位置*y*处的出现）都可以用*时间*作为其调节维度进行建模。正如我们将看到的，我们可以表示数据并相应地构造张量。

自然语言处理和理解就是一个很好的例子。如果我们有大量的文本，比如莎士比亚的文集，那么我们能对这篇文本说些什么呢？我们可以详细说明文本的统计特性，即有多少个单词，这些单词中有多少是唯一的，字符总数等等，但我们也从自己的阅读经验中固有地知道，文本/语言的一个重要特性是**序列**；也就是说，单词出现的顺序。这种顺序有助于我们理解句法和语法，更不用说意义本身了。正是在分析这类数据时，我们迄今为止所覆盖的网络才有不足之处。

在本章中，我们将学习以下主题：

*   什么是基本 RNN
*   如何培训 RNN
*   RNN 架构的改进，包括**选通递归单元****GRU**/**长短时记忆****LSTM**网络
*   如何使用 Gorgonia 中的 LSTM 单元实现 RNN

# 香草香精

根据他们更为乌托邦式的描述，RNN 能够做一些迄今为止我们所覆盖的网络无法做到的事情：记住。更准确地说，在具有单个隐藏层的简单网络中，网络的输出以及该隐藏层的状态与训练序列中的下一个元素相结合，形成新网络的输入（具有其自身的可训练隐藏状态）。*香草*RNN 可可视化如下：

![](img/a9747787-4cb2-4224-be45-34022f24be3a.png)

让我们把它拆开一点。上图中的两个网络是同一事物的两种不同表示形式。一个处于**滚动**状态，这只是计算图的抽象表示，其中无限多个时间步由**（t）**表示。然后，我们使用**展开的****RNN**作为网络数据的馈送和训练。

对于给定的前向传递，该网络接受两个输入，其中**X**是一段训练数据的表示，以及先前的*隐藏*状态**S**（在**t0**处初始化为零向量）和时间步**t**（序列中的位置）对这些输入及其可训练参数的乘积重复操作（输入向量串联，即，`Sigmoid`激活）。然后，我们应用我们的学习算法，对反向传播稍加修改，我们将在下面介绍，从而获得 RNN 是什么、由什么组成以及如何工作的基本模型。

# 培训 RNN

我们训练这些网络的方式是通过使用**时间反向传播**（**BPTT**）。这是一个奇特的名字，是对你在[第 2 章](02.html)中已经知道的东西的一个小小的变化，*什么是神经网络？我如何训练神经网络？*。在本节中，我们将详细探讨这种变化。

# 时间反向传播

With RNNs, we have multiple copies of the same network, one for each timestep. Therefore, we need a way to backpropagate the error derivatives and calculate weight updates for each of the parameters in every timestep. The way we do this is simple. We're following the contours of a function so that we can try and optimize its shape. We have multiple copies of the trainable parameters, one at each timestep, and we want these copies to be consistent with each other so that when we calculate all the gradients for a given parameter, we take their average. We use this to update the parameter at *t0* for each iteration of the learning process.

目标是计算跨时间步累积的误差，展开/滚动网络并相应地更新权重。当然，这需要计算成本；也就是说，所需的计算量随着时间步数的增加而增加。处理这个问题的方法是*截断*（因此，*截断 BPTT*）输入/输出对的序列，这意味着我们一次只滚动/展开 20 个时间步的序列，使问题易于处理。

对于那些有兴趣探索这背后的数学的人，可以在本章的*进一步阅读*部分找到更多信息。

# 成本函数

我们使用 RNN 的代价函数是交叉熵损失。与简单的二进制分类任务相比，RNN 的实现没有什么特别之处。这里，我们比较两个概率分布，一个是预测的，一个是预期的。我们计算每个时间步的误差并求和

# RNN 和消失梯度

RNN 本身是一项重要的架构创新，但在渐变*消失*方面遇到了问题。当梯度值变得如此之小以至于更新也同样微小时，这会减慢甚至停止学习。你的数字神经元死亡，你的网络也不能做你想让它做的事。但是，记忆差的神经网络比完全没有记忆的神经网络好吗？

让我们放大一点，讨论一下遇到这个问题时实际发生了什么。回想一下反向传播期间计算给定权重值的公式：

*W=W-LR*G*

这里，权重值等于权重减（学习率乘以梯度）。

您的网络正在跨层和跨时间步传播错误导数。数据集越大，时间步长和参数的数量就越多，因此层的数量也就越多。在每个步骤中，展开的 RNN 都包含一个激活函数，该函数将网络的输出压缩到 0 到 1 之间。

在非常接近零的梯度值上重复这些操作意味着神经元*死亡*，或停止*发射*。神经元模型的计算图上的数学表示变得脆弱。这是因为，如果我们正在学习的参数变化太小，不会对网络本身的输出产生影响，那么网络将无法学习该参数的值。

So, instead of using the entirety of the hidden state from the previous timestep, is there another way to make the network a bit smarter in terms of what information it chooses to keep as we step our network through time during the training process? The answer is yes! Let's consider these changes to the network architecture.

# 使用 GRU/LSTM 单位扩充您的 RNN

那么，如果你想造一台像死去的作家一样写作的机器呢？或者理解两周前股票价格暴涨可能意味着今天股票将再次暴涨？对于在训练早期观察到关键信息的序列预测任务，例如在*t+1*处，但需要在*t+250*处进行准确预测，常规 RNN 会遇到困难。这就是 LSTM（对于某些任务，还有 GRU）网络出现的地方。与一个简单的单元不同，您有多个条件*迷你*神经网络，每个神经网络决定是否跨时间步携带信息。现在我们将详细讨论这些变化。

# 长短时记忆单元

特别感谢瑞士研究小组于 1997 年发表了一篇题为*长短期记忆*的论文，该论文描述了一种使用更高级*记忆*进一步增强 RNN 的方法。

那么，在这个上下文中*记忆*实际上意味着什么？LSTM 使用*哑*RNN 单元并添加另一个神经网络（由输入、操作和激活组成），该神经网络将选择从一个时间步传递到另一个时间步的信息。它通过维持一个*单元状态*（像一个香草 RNN 单元）和一个新的隐藏状态来实现这一点，然后将这两个状态输入下一步。如下图所示，这些*门*了解哪些信息应保持在隐藏状态：

![](img/ece2f76d-c95f-4967-bb34-ee6440867053.png)

在这里，我们可以看到多个门包含在**r（t）**、**z（t）**和**h（t）**中。每个都有一个激活功能：Sigmoid 代表**r**和**z**和**tanh**代表**h（t）**

# Gated Recurrent Units

LSTM 装置的替代品是 GRU。这些首先由深度学习史上另一位重要人物 Yoshua Bengio 领导的团队描述。他们的初始论文*使用 RNN 编码器-统计机器翻译解码器学习短语表示*（2014 年），提供了一种有趣的方式来思考这些增强 RNN 有效性的方法。

具体而言，他们在普通 RNN 和 LSTM/GRU 单元中的`Tanh`激活函数之间画出了等价关系，也将它们描述为*激活*。其激活性质的区别在于信息是保留、不变还是在单元本身中更新。实际上，`Tanh`功能的使用意味着你的网络对信息的选择性更高，这些信息将你的网络从一步带到下一步。

GRU 与 LSTM 的不同之处在于，它们摆脱了*单元状态*，从而减少了网络正在执行的张量操作的总数。它们还使用一个复位门代替 LSTM 的输入和遗忘门，进一步简化了网络结构。

以下是 GRU 的逻辑表示：

![](img/edaf938d-d2ab-4fd9-84ec-ced8c2bda57d.png)

在这里，我们可以看到单个复位门（**z（t）**和**r（t）**中的遗忘/输入门的组合，其中单个状态**S（t）**被转入下一个时间步。

# 门的偏置初始化

最近，在一次 ML 会议上，*学习表征国际会议*上，Facebook 人工智能研究团队发表了一篇论文，描述了 RNN 的进展。本文关注的是使用 GRU/LSTM 单元增强的 RNN 的有效性。虽然对论文的深入研究超出了本书的范围，但您可以在本章末尾的*进一步阅读*一节中了解更多。他们的研究中出现了一个有趣的假设：这些单元可以以某种方式初始化其偏差向量，这将提高网络学习长期相关性的能力。他们公布了他们的结果，结果表明，训练时间和减少困惑的速度似乎有所改善：

![](img/cd383a11-ad1c-4bb9-9f55-50f4172d12ea.png)

该图取自论文，表示网络在*y*轴上的损耗，以及*x*轴上的训练迭代次数。红色表示 c*hrono 初始化*。

这是一项非常新的研究，理解基于 LSTM/GRU 的网络为何表现如此出色具有明确的科学价值。本文的主要实际意义，即门控单元偏差的初始化，为我们提供了另一种工具来提高模型性能并节省宝贵的 GPU 周期。目前，这些性能改进是字级 PTB 和字符级 text8 数据集上最显著的（尽管仍然是增量的）。我们将在下一节中构建的网络可以很容易地进行调整，以测试此更改的相对性能改进。

# 在 Gorgonia 建立 LSTM

既然我们已经讨论了 RNN 是什么，如何培训它们，以及如何修改它们以提高性能，那么让我们构建一个 RNN！接下来的几节将介绍如何处理和表示使用 LSTM 单元的 RNN 的数据。我们还将看看网络本身是什么样子，GRU 单元的代码，以及一些用于理解我们的网络正在做什么的工具。

# 表示文本数据

虽然我们的目标是预测给定句子中的下一个单词，或者（理想情况下）预测一系列有意义且符合某种英语语法/语法标准的单词，但实际上我们将在字符级别对数据进行编码。这意味着我们需要获取文本数据（在本例中，是威廉·莎士比亚的作品集）并生成一系列标记。这些标记可能是完整的句子、单个单词，甚至是字符本身，这取决于我们训练的模型类型。

一旦我们标记了文本数据，我们需要将这些标记转换成某种数值表示形式，以便于计算。正如我们所讨论的，在我们的例子中，这些表示是张量。然后将这些标记转换为一些张量，并对文本执行一些操作，以提取文本的不同属性，以下称为我们的*语料库*。

这里的目的是生成一个词汇向量（长度为*n*的向量，其中*n*是语料库中唯一字符的数量）。我们将使用这个向量作为模板来编码每个字符。

# 输入和处理输入

让我们首先在项目目录的根目录中创建一个`vocab.go`文件。在这里，您将定义一些表示序列开始/结束的保留 unicode 字符，以及填充序列的`BLANK`字符。

请注意，此处不包括我们的`shakespeare.txt`输入文件。相反，我们建立了一个词汇表和索引，并将我们的输入`corpus`分为几块：

```go
package main

import (
  "fmt"
  "strings"
)

const START rune = 0x02
const END rune = 0x03
const BLANK rune = 0x04

// vocab related
var sentences []string
var vocab []rune
var vocabIndex map[rune]int
var maxsent int = 30

func initVocab(ss []string, thresh int) {
  s := strings.Join(ss, " ")
  fmt.Println(s)
  dict := make(map[rune]int)
  for _, r := range s {
    dict[r]++
  }

  vocab = append(vocab, START)
  vocab = append(vocab, END)
  vocab = append(vocab, BLANK)
  vocabIndex = make(map[rune]int)

  for ch, c := range dict {
    if c >= thresh {
      // then add letter to vocab
      vocab = append(vocab, ch)
    }
  }

  for i, v := range vocab {
    vocabIndex[v] = i
  }

  fmt.Println("Vocab: ", vocab)
  inputSize = len(vocab)
  outputSize = len(vocab)
  epochSize = len(ss)
  fmt.Println("\ninputs :", inputSize)
  fmt.Println("\noutputs :", outputSize)
  fmt.Println("\nepochs: :", epochSize)
  fmt.Println("\nmaxsent: :", maxsent)
}

func init() {
  sentencesRaw := strings.Split(corpus, "\n")

  for _, s := range sentencesRaw {
    s2 := strings.TrimSpace(s)
    if s2 != "" {
      sentences = append(sentences, s2)
    }

  }

  initVocab(sentences, 1)
}
```

We can now create the next chunk of code, which provides us with helper functions that we will need later on. More specifically, we will add two sampling functions: one is temperature-based, where the probability of already-high probability words is increased, and decreased in the case of low-probability words. The higher the temperature, the greater the probability bump in either direction. This gives you another tunable feature in your LSTM-RNN.

最后，我们将包括一些用于处理`byte`和`uint`切片的函数，允许您轻松地对它们进行比较/交换/评估：

```go
package main

import (
  "math/rand"

  "gorgonia.org/gorgonia"
  "gorgonia.org/tensor"
)

func sampleT(val gorgonia.Value) int {
  var t tensor.Tensor
  var ok bool
  if t, ok = val.(tensor.Tensor); !ok {
    panic("Expects a tensor")
  }

  return tensor.SampleIndex(t)
}

func sample(val gorgonia.Value) int {

  var t tensor.Tensor
  var ok bool
  if t, ok = val.(tensor.Tensor); !ok {
    panic("expects a tensor")
  }
  indT, err := tensor.Argmax(t, -1)
  if err != nil {
    panic(err)
  }
  if !indT.IsScalar() {
    panic("Expected scalar index")
  }
  return indT.ScalarValue().(int)
}

func shuffle(a []string) {
  for i := len(a) - 1; i > 0; i-- {
    j := rand.Intn(i + 1)
    a[i], a[j] = a[j], a[i]
  }
}

type byteslice []byte

func (s byteslice) Len() int { return len(s) }
func (s byteslice) Less(i, j int) bool { return s[i] < s[j] }
func (s byteslice) Swap(i, j int) { s[i], s[j] = s[j], s[i] }

type uintslice []uint

func (s uintslice) Len() int { return len(s) }
func (s uintslice) Less(i, j int) bool { return s[i] < s[j] }
func (s uintslice) Swap(i, j int) { s[i], s[j] = s[j], s[i] }
```

接下来，我们将创建一个`lstm.go`文件，在其中我们将定义我们的 LSTM 单位。它们看起来像小神经网络，因为正如我们前面讨论的，它们就是这样的。将定义输入、遗忘和输出门及其相关权重/偏差。

`MakeLSTM()`函数将把这些单位添加到图形中。LSTM 也有许多方法；也就是说，`learnables()`用于生成我们可学习的参数，`Activate()`用于定义我们的单元在处理输入数据时执行的操作：

```go
package main

import (
  . "gorgonia.org/gorgonia"
)

type LSTM struct {
  wix *Node
  wih *Node
  bias_i *Node

  wfx *Node
  wfh *Node
  bias_f *Node

  wox *Node
  woh *Node
  bias_o *Node

  wcx *Node
  wch *Node
  bias_c *Node
}

func MakeLSTM(g *ExprGraph, hiddenSize, prevSize int) LSTM {
  retVal := LSTM{}

  retVal.wix = NewMatrix(g, Float, WithShape(hiddenSize, prevSize), WithInit(GlorotN(1.0)), WithName("wix_"))
  retVal.wih = NewMatrix(g, Float, WithShape(hiddenSize, hiddenSize), WithInit(GlorotN(1.0)), WithName("wih_"))
  retVal.bias_i = NewVector(g, Float, WithShape(hiddenSize), WithName("bias_i_"), WithInit(Zeroes()))

  // output gate weights

  retVal.wox = NewMatrix(g, Float, WithShape(hiddenSize, prevSize), WithInit(GlorotN(1.0)), WithName("wfx_"))
  retVal.woh = NewMatrix(g, Float, WithShape(hiddenSize, hiddenSize), WithInit(GlorotN(1.0)), WithName("wfh_"))
  retVal.bias_o = NewVector(g, Float, WithShape(hiddenSize), WithName("bias_f_"), WithInit(Zeroes()))

  // forget gate weights

  retVal.wfx = NewMatrix(g, Float, WithShape(hiddenSize, prevSize), WithInit(GlorotN(1.0)), WithName("wox_"))
  retVal.wfh = NewMatrix(g, Float, WithShape(hiddenSize, hiddenSize), WithInit(GlorotN(1.0)), WithName("woh_"))
  retVal.bias_f = NewVector(g, Float, WithShape(hiddenSize), WithName("bias_o_"), WithInit(Zeroes()))

  // cell write

  retVal.wcx = NewMatrix(g, Float, WithShape(hiddenSize, prevSize), WithInit(GlorotN(1.0)), WithName("wcx_"))
  retVal.wch = NewMatrix(g, Float, WithShape(hiddenSize, hiddenSize), WithInit(GlorotN(1.0)), WithName("wch_"))
  retVal.bias_c = NewVector(g, Float, WithShape(hiddenSize), WithName("bias_c_"), WithInit(Zeroes()))
  return retVal
}

func (l *LSTM) learnables() Nodes {
  return Nodes{
    l.wix, l.wih, l.bias_i,
    l.wfx, l.wfh, l.bias_f,
    l.wcx, l.wch, l.bias_c,
    l.wox, l.woh, l.bias_o,
  }
}

func (l *LSTM) Activate(inputVector *Node, prev lstmout) (out lstmout, err error) {
  // log.Printf("prev %v", prev.hidden.Shape())
  prevHidden := prev.hidden
  prevCell := prev.cell
  var h0, h1, inputGate *Node
  h0 = Must(Mul(l.wix, inputVector))
  h1 = Must(Mul(l.wih, prevHidden))
  inputGate = Must(Sigmoid(Must(Add(Must(Add(h0, h1)), l.bias_i))))

  var h2, h3, forgetGate *Node
  h2 = Must(Mul(l.wfx, inputVector))
  h3 = Must(Mul(l.wfh, prevHidden))
  forgetGate = Must(Sigmoid(Must(Add(Must(Add(h2, h3)), l.bias_f))))

  var h4, h5, outputGate *Node
  h4 = Must(Mul(l.wox, inputVector))
  h5 = Must(Mul(l.woh, prevHidden))
  outputGate = Must(Sigmoid(Must(Add(Must(Add(h4, h5)), l.bias_o))))

  var h6, h7, cellWrite *Node
  h6 = Must(Mul(l.wcx, inputVector))
  h7 = Must(Mul(l.wch, prevHidden))
  cellWrite = Must(Tanh(Must(Add(Must(Add(h6, h7)), l.bias_c))))

  // cell activations
  var retain, write *Node
  retain = Must(HadamardProd(forgetGate, prevCell))
  write = Must(HadamardProd(inputGate, cellWrite))
  cell := Must(Add(retain, write))
  hidden := Must(HadamardProd(outputGate, Must(Tanh(cell))))
  out = lstmout{
    hidden: hidden,
    cell: cell,
  }
  return
}

type lstmout struct {
  hidden, cell *Node
}
```

正如我们前面提到的，我们还将包括 GRU-RNN 的代码。这段代码是模块化的，因此您可以将 LSTM 替换为 GRU，从而扩展您可以进行的实验种类和可以处理的用例范围。

让我们创建一个名为`gru.go`的文件。它将遵循与`lstm.go`相同的结构，但将减少闸门数量：

```go
package main

import (
  "fmt"

  . "gorgonia.org/gorgonia"
  "gorgonia.org/tensor"
)

var Float = tensor.Float32

type contextualError interface {
  error
  Node() *Node
  Value() Value
  InstructionID() int
  Err() error
}

type GRU struct {

  // weights for mem
  u *Node
  w *Node
  b *Node

  // update gate
  uz *Node
  wz *Node
  bz *Node

  // reset gate
  ur *Node
  wr *Node
  br *Node
  one *Node

  Name string // optional name
}

func MakeGRU(name string, g *ExprGraph, inputSize, hiddenSize int, dt tensor.Dtype) GRU {
  // standard weights
  u := NewMatrix(g, dt, WithShape(hiddenSize, hiddenSize), WithName(fmt.Sprintf("%v.u", name)), WithInit(GlorotN(1.0)))
  w := NewMatrix(g, dt, WithShape(hiddenSize, inputSize), WithName(fmt.Sprintf("%v.w", name)), WithInit(GlorotN(1.0)))
  b := NewVector(g, dt, WithShape(hiddenSize), WithName(fmt.Sprintf("%v.b", name)), WithInit(Zeroes()))

  // update gate
  uz := NewMatrix(g, dt, WithShape(hiddenSize, hiddenSize), WithName(fmt.Sprintf("%v.uz", name)), WithInit(GlorotN(1.0)))
  wz := NewMatrix(g, dt, WithShape(hiddenSize, inputSize), WithName(fmt.Sprintf("%v.wz", name)), WithInit(GlorotN(1.0)))
  bz := NewVector(g, dt, WithShape(hiddenSize), WithName(fmt.Sprintf("%v.b_uz", name)), WithInit(Zeroes()))

  // reset gate
  ur := NewMatrix(g, dt, WithShape(hiddenSize, hiddenSize), WithName(fmt.Sprintf("%v.ur", name)), WithInit(GlorotN(1.0)))
  wr := NewMatrix(g, dt, WithShape(hiddenSize, inputSize), WithName(fmt.Sprintf("%v.wr", name)), WithInit(GlorotN(1.0)))
  br := NewVector(g, dt, WithShape(hiddenSize), WithName(fmt.Sprintf("%v.bz", name)), WithInit(Zeroes()))

  ones := tensor.Ones(dt, hiddenSize)
  one := g.Constant(ones)
  gru := GRU{
    u: u,
    w: w,
    b: b,

    uz: uz,
    wz: wz,
    bz: bz,

    ur: ur,
    wr: wr,
    br: br,

    one: one,
  }
  return gru
}

func (l *GRU) Activate(x, prev *Node) (retVal *Node, err error) {
  // update gate
  uzh := Must(Mul(l.uz, prev))
  wzx := Must(Mul(l.wz, x))
  z := Must(Sigmoid(
    Must(Add(
      Must(Add(uzh, wzx)),
      l.bz))))

  // reset gate
  urh := Must(Mul(l.ur, prev))
  wrx := Must(Mul(l.wr, x))
  r := Must(Sigmoid(
    Must(Add(
      Must(Add(urh, wrx)),
      l.br))))

  // memory for hidden
  hiddenFilter := Must(Mul(l.u, Must(HadamardProd(r, prev))))
  wx := Must(Mul(l.w, x))
  mem := Must(Tanh(
    Must(Add(
      Must(Add(hiddenFilter, wx)),
      l.b))))

  omz := Must(Sub(l.one, z))
  omzh := Must(HadamardProd(omz, prev))
  upd := Must(HadamardProd(z, mem))
  retVal = Must(Add(upd, omzh))
  return
}

func (l *GRU) learnables() Nodes {
  retVal := make(Nodes, 0, 9)
  retVal = append(retVal, l.u, l.w, l.b, l.uz, l.wz, l.bz, l.ur, l.wr, l.br)
  return retVal
}
```

当我们继续将网络的各个部分拉到一起时，我们需要在 LSTM/GRU 代码之上的最后一层抽象，即网络本身。我们遵循的命名约定是*序列到*（或`s2s`网络的命名约定。在我们的示例中，我们预测文本的下一个字符。这个序列是任意的，可以是单词或句子，甚至是语言之间的映射。因此，我们将创建一个`s2s.go`文件。

由于这实际上是一个较大的神经网络，包含了我们之前在`lstm.go`/`gru.go`中定义的小型神经网络，因此结构类似。我们可以看到，LSTM 正在处理我们网络的输入（而不是普通 RNN 单元），并且我们在`t-0`有`dummy`节点用于处理输入，以及输出节点：

```go
package main

import (
  "encoding/json"
  "io"
  "log"
  "os"

  "github.com/pkg/errors"
  . "gorgonia.org/gorgonia"
  "gorgonia.org/tensor"
)

type seq2seq struct {
  in        LSTM
  dummyPrev *Node // vector
  dummyCell *Node // vector
  embedding *Node // NxM matrix, where M is the number of dimensions of the embedding

  decoder *Node
  vocab []rune

  inVecs []*Node
  losses []*Node
  preds []*Node
  predvals []Value
  g *ExprGraph
  vm VM
}

// NewS2S creates a new Seq2Seq network. Input size is the size of the embedding. Hidden size is the size of the hidden layer
func NewS2S(hiddenSize, embSize int, vocab []rune) *seq2seq {
  g := NewGraph()
  // in := MakeGRU("In", g, embSize, hiddenSize, Float)s
  in := MakeLSTM(g, hiddenSize, embSize)
  log.Printf("%q", vocab)

  dummyPrev := NewVector(g, Float, WithShape(embSize), WithName("Dummy Prev"), WithInit(Zeroes()))
  dummyCell := NewVector(g, Float, WithShape(hiddenSize), WithName("Dummy Cell"), WithInit(Zeroes()))
  embedding := NewMatrix(g, Float, WithShape(len(vocab), embSize), WithInit(GlorotN(1.0)), WithName("Embedding"))
  decoder := NewMatrix(g, Float, WithShape(len(vocab), hiddenSize), WithInit(GlorotN(1.0)), WithName("Output Decoder"))

  return &seq2seq{
    in: in,
    dummyPrev: dummyPrev,
    dummyCell: dummyCell,
    embedding: embedding,
    vocab: vocab,
    decoder: decoder,
    g: g,
  }
}

func (s *seq2seq) learnables() Nodes {
  retVal := make(Nodes, 0)
  retVal = append(retVal, s.in.learnables()...)
  retVal = append(retVal, s.embedding)
  retVal = append(retVal, s.decoder)
  return retVal
}
```

由于我们使用的是一个静态图，Gorgonia 的`TapeMachine`，我们需要一个函数来在初始化网络时构建网络。这些值中的许多将在运行时被替换：

```go
func (s *seq2seq) build() (cost *Node, err error) {
  // var prev *Node = s.dummyPrev
  prev := lstmout{
    hidden: s.dummyCell,
    cell: s.dummyCell,
  }
  s.predvals = make([]Value, maxsent)

  var prediction *Node
  for i := 0; i < maxsent; i++ {
    var vec *Node
    if i == 0 {
      vec = Must(Slice(s.embedding, S(0))) // dummy, to be replaced at runtime
    } else {
      vec = Must(Mul(prediction, s.embedding))
    }
    s.inVecs = append(s.inVecs, vec)
    if prev, err = s.in.Activate(vec, prev); err != nil {
      return
    }
    prediction = Must(SoftMax(Must(Mul(s.decoder, prev.hidden))))
    s.preds = append(s.preds, prediction)
    Read(prediction, &s.predvals[i])

    logprob := Must(Neg(Must(Log(prediction))))
    loss := Must(Slice(logprob, S(0))) // dummy, to be replaced at runtime
    s.losses = append(s.losses, loss)

    if cost == nil {
      cost = loss
    } else {
      cost = Must(Add(cost, loss))
    }
  }

  _, err = Grad(cost, s.learnables()...)
  return
}
```

我们现在可以定义网络本身的训练循环：

```go

func (s *seq2seq) train(in []rune) (err error) {

  for i := 0; i < maxsent; i++ {
    var currentRune, correctPrediction rune
    switch {
    case i == 0:
      currentRune = START
      correctPrediction = in[i]
    case i-1 == len(in)-1:
      currentRune = in[i-1]
      correctPrediction = END
    case i-1 >= len(in):
      currentRune = BLANK
      correctPrediction = BLANK
    default:
      currentRune = in[i-1]
      correctPrediction = in[i]
    }

    targetID := vocabIndex[correctPrediction]
    if i == 0 || i-1 >= len(in) {
      srcID := vocabIndex[currentRune]
      UnsafeLet(s.inVecs[i], S(srcID))
    }
    UnsafeLet(s.losses[i], S(targetID))

  }
  if s.vm == nil {
    s.vm = NewTapeMachine(s.g, BindDualValues())
  }
  s.vm.Reset()
  err = s.vm.RunAll()

  return
}
```

我们还需要一个`predict`函数，以便在我们的模型经过训练后，我们可以对其进行采样：

```go

func (s *seq2seq) predict(in []rune) (output []rune, err error) {
  g2 := s.g.SubgraphRoots(s.preds...)
  vm := NewTapeMachine(g2)
  if err = vm.RunAll(); err != nil {
    return
  }
  defer vm.Close()
  for _, pred := range s.predvals {
    log.Printf("%v", pred.Shape())
    id := sample(pred)
    if id >= len(vocab) {
      log.Printf("Predicted %d. Len(vocab) %v", id, len(vocab))
      continue
    }
    r := vocab[id]

    output = append(output, r)
  }
  return
}
```

在大型文本语料库上进行训练可能需要很长时间，因此有一种检查模型的方法非常有用，这样我们就可以在训练周期的任意点保存/加载模型：

```go

func (s *seq2seq) checkpoint() (err error) {
  learnables := s.learnables()
  var f io.WriteCloser
  if f, err = os.OpenFile("CHECKPOINT.bin", os.O_CREATE|os.O_TRUNC|os.O_WRONLY, 0644); err != nil {
    return
  }
  defer f.Close()
  enc := json.NewEncoder(f)
  for _, l := range learnables {
    t := l.Value().(*tensor.Dense).Data() // []float32
    if err = enc.Encode(t); err != nil {
      return
    }
  }

  return nil
}

func (s *seq2seq) load() (err error) {
  learnables := s.learnables()
  var f io.ReadCloser
  if f, err = os.OpenFile("CHECKPOINT.bin", os.O_RDONLY, 0644); err != nil {
    return
  }
  defer f.Close()
  dec := json.NewDecoder(f)
  for _, l := range learnables {
    t := l.Value().(*tensor.Dense).Data().([]float32)
    var data []float32
    if err = dec.Decode(&data); err != nil {
      return
    }
    if len(data) != len(t) {
      return errors.Errorf("Unserialized length %d. Expected length %d", len(data), len(t))
    }
    copy(t, data)
  }
  return nil
}
```

最后，我们可以定义`meta-training`循环。这是采用`s2s`网络、解算器、我们的数据和各种超参数的循环：

```go

func train(s *seq2seq, epochs int, solver Solver, data []string) (err error) {
  cost, err := s.build()
  if err != nil {
    return err
  }
  var costVal Value
  Read(cost, &costVal)

  model := NodesToValueGrads(s.learnables())
  for e := 0; e < epochs; e++ {
    shuffle(data)

    for _, sentence := range data {
      asRunes := []rune(sentence)
      if err = s.train(asRunes); err != nil {
        return
      }
      if err = solver.Step(model); err != nil {
        return
      }
    }
    // if e%100 == 0 {
    log.Printf("Cost for epoch %d: %1.10f\n", e, costVal)
    // }

  }

  return nil

}
```

在我们构建和执行我们的网络之前，我们将添加一个小型可视化工具，它将帮助我们进行任何需要进行的故障排除。在处理数据时，可视化是一个强大的工具，在我们的例子中，它允许我们窥视神经网络的内部，以便我们了解它在做什么。具体来说，我们将生成热图，以便在整个训练过程中跟踪网络权重的变化。通过这种方式，我们可以确保他们正在改变（也就是说，我们的网络正在学习）。

创建一个名为`heatmap.go`的文件：

```go
package main

import (
    "image/color"
    "math"

    "github.com/pkg/errors"
    "gonum.org/v1/gonum/mat"
    "gonum.org/v1/plot"
    "gonum.org/v1/plot/palette/moreland"
    "gonum.org/v1/plot/plotter"
    "gonum.org/v1/plot/vg"
    "gorgonia.org/tensor"
)

type heatmap struct {
    x mat.Matrix
}

func (m heatmap) Dims() (c, r int) { r, c = m.x.Dims(); return c, r }
func (m heatmap) Z(c, r int) float64 { return m.x.At(r, c) }
func (m heatmap) X(c int) float64 { return float64(c) }
func (m heatmap) Y(r int) float64 { return float64(r) }

type ticks []string

func (t ticks) Ticks(min, max float64) []plot.Tick {
    var retVal []plot.Tick
    for i := math.Trunc(min); i <= max; i++ {
        retVal = append(retVal, plot.Tick{Value: i, Label: t[int(i)]})
    }
    return retVal
}

func Heatmap(a *tensor.Dense) (p *plot.Plot, H, W vg.Length, err error) {
    switch a.Dims() {
    case 1:
        original := a.Shape()
        a.Reshape(original[0], 1)
        defer a.Reshape(original...)
    case 2:
    default:
        return nil, 0, 0, errors.Errorf("Can't do a tensor with shape %v", a.Shape())
    }

    m, err := tensor.ToMat64(a, tensor.UseUnsafe())
    if err != nil {
        return nil, 0, 0, err
    }

    pal := moreland.ExtendedBlackBody().Palette(256)
    // lum, _ := moreland.NewLuminance([]color.Color{color.Gray{0}, color.Gray{255}})
    // pal := lum.Palette(256)

    hm := plotter.NewHeatMap(heatmap{m}, pal)
    if p, err = plot.New(); err != nil {
        return nil, 0, 0, err
    }
    hm.NaN = color.RGBA{0, 0, 0, 0} // black
    p.Add(hm)

    sh := a.Shape()
    H = vg.Length(sh[0])*vg.Centimeter + vg.Centimeter
    W = vg.Length(sh[1])*vg.Centimeter + vg.Centimeter
    return p, H, W, nil
}

func Avg(a []float64) (retVal float64) {
    for _, v := range a {
        retVal += v
    }

    return retVal / float64(len(a))
}

```

我们现在可以把所有的部分放在一起，创建我们的`main.go`文件。在这里，我们将设置超参数，解析输入，并启动主训练循环：

```go
package main

import (
  "flag"
  "fmt"
  "io/ioutil"
  "log"
  "os"
  "runtime/pprof"

  . "gorgonia.org/gorgonia"
  "gorgonia.org/tensor"
)

var cpuprofile = flag.String("cpuprofile", "", "write cpu profile to file")
var memprofile = flag.String("memprofile", "", "write memory profile to this file")

const (
  embeddingSize = 20
  maxOut = 30

  // gradient update stuff
  l2reg = 0.000001
  learnrate = 0.01
  clipVal = 5.0
)

var trainiter = flag.Int("iter", 5, "How many iterations to train")

// various global variable inits
var epochSize = -1
var inputSize = -1
var outputSize = -1

var corpus string

func init() {
  buf, err := ioutil.ReadFile("shakespeare.txt")
  if err != nil {
    panic(err)
  }
  corpus = string(buf)
}

var dt tensor.Dtype = tensor.Float32

func main() {
  flag.Parse()
  if *cpuprofile != "" {
    f, err := os.Create(*cpuprofile)
    if err != nil {
      log.Fatal(err)
    }
    pprof.StartCPUProfile(f)
    defer pprof.StopCPUProfile()
  }

  hiddenSize := 100

  s2s := NewS2S(hiddenSize, embeddingSize, vocab)
  solver := NewRMSPropSolver(WithLearnRate(learnrate), WithL2Reg(l2reg), WithClip(clipVal), WithBatchSize(float64(len(sentences))))
  for k, v := range vocabIndex {
    log.Printf("%q %v", k, v)
  }

  // p, h, w, err := Heatmap(s2s.decoder.Value().(*tensor.Dense))
  // p.Save(w, h, "embn0.png")

  if err := train(s2s, 300, solver, sentences); err != nil {
    panic(err)
  }
  out, err := s2s.predict([]rune(corpus))
  if err != nil {
    panic(err)
  }
  fmt.Printf("OUT %q\n", out)

  p, h, w, err = Heatmap(s2s.decoder.Value().(*tensor.Dense))
  p.Save(w, h, "embn.png")
}
```

现在，让我们运行`go run *.go`并观察输出：

```go
2019/05/25 23:52:03 Cost for epoch 31: 250.7806701660
2019/05/25 23:52:19 Cost for epoch 32: 176.0116729736
2019/05/25 23:52:35 Cost for epoch 33: 195.0501556396
2019/05/25 23:52:50 Cost for epoch 34: 190.6829681396
2019/05/25 23:53:06 Cost for epoch 35: 181.1398162842
```

我们可以看到，在网络生命的早期，衡量网络优化程度的成本很高，而且波动很大。

在指定的历元数之后，将进行输出预测：

```go
OUT ['S' 'a' 'K' 'a' 'g' 'y' 'h' ',' '\x04' 'a' 'g' 'a' 't' '\x04' '\x04' ' ' 's' 'h' 'h' 'h' 'h' 'h' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']
```

现在，您可以尝试使用超参数和调整，例如使用 GRU 而不是 LSTM 单元，并探索偏差初始化，以进一步优化网络并生成更好的预测。

# 总结

在本章中，我们介绍了什么是 RNN 以及如何培训 RNN。我们已经看到，为了有效地建模长期依赖关系并克服培训挑战，有必要对标准 RNN 进行更改，包括 GRU/LSTM 单位提供的跨时间控制机制的附加信息。我们在戈尔戈尼亚建立了这样一个网络。

在下一章中，我们将学习如何构建 CNN 以及如何调整一些超参数。

# 进一步阅读

*   *训练回归神经网络*、*伊利亚·萨茨凯弗*，可在[上获得 http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)
*   *长短期记忆*、*Hochreiter*、*Sepp*和*Jurgen Schmidhuber*，可在[购买 https://www.researchgate.net/publication/13853244_Long_Short-term_Memory](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory)
*   *门控循环神经网络对序列建模的经验评估*，*Bengio 等人*，可在[获取 https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555)