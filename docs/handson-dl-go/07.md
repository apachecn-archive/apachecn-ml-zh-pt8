# 七、用深度 Q 网络求解迷宫

想象一下，您的数据不是一个离散的文本体，也不是组织数据仓库中经过仔细清理的一组记录。也许您希望培训一名代理在环境中导航。你将如何开始解决这个问题？到目前为止，我们所介绍的技术都不适用于这样的任务。我们需要考虑如何以完全不同的方式训练我们的模型，使这个问题变得容易处理。此外，在使用案例中，问题可以被定义为一个代理探索并从环境中获得回报，从游戏到个性化新闻推荐，**深度 Q 网络**（**DQNs**）是我们深度学习技术库中的有用工具。

**强化学习**（**RL**）已被 Yann LeCun（他在**卷积神经网络**（**CNNs**）的开发中发挥了重要作用）以及在撰写本文时 Facebook 人工智能研究总监描述为机器学习方法蛋糕上的樱桃。在这个类比中，无监督学习是蛋糕，监督学习是糖衣。对于我们来说，重要的是要理解，RL 只解决了一个非常具体的问题，尽管它提供了无模型学习的承诺，当您的模型成功优化到您指定的目标时，您只需提供一些标量奖励。

本章将提供一个简要的背景，说明为什么会出现这种情况，以及 RL 如何更普遍地融入到图片中。具体而言，我们将涵盖以下主题：

*   什么是 DQN？
*   关于 Q-学习算法的学习
*   学习如何培训 DQN
*   建立解迷宫的 DQN

# 什么是 DQN？

正如您将了解到的，DQN 与我们到目前为止所讨论的标准前馈和卷积网络没有太大区别。事实上，所有标准成分都存在：

*   数据的表示（在本例中，是迷宫的状态和试图在其中导航的代理）
*   处理迷宫表示的标准层，其中还包括这些层之间的标准操作，如`Tanh`激活功能
*   一个线性激活的输出层，可以提供预测

在这里，我们的预测代表了影响我们输入状态的可能动作。在解决迷宫的案例中，我们试图预测能够为玩家带来最大（和累积）预期回报的动作，最终导致迷宫退出。这些预测作为训练循环的一部分出现，其中学习算法使用*伽马*变量作为随时间衰减的变量，平衡环境状态空间的探索和通过建立行动、状态或奖励地图收集的知识的利用

让我们介绍一些新概念。首先，我们需要一个*m*x*n*矩阵，它表示给定*状态*（即一行）和*动作*（即一列）的奖励*R*。我们还需要一张*Q*表。这是一个矩阵（初始化为零值），表示代理的内存（即，我们的玩家试图通过迷宫找到自己的路），或状态、采取的行动及其奖励的历史。

这两个矩阵相互关联。我们可以使用以下公式确定我们代理人关于已知奖励表的记忆（*Q*表）：

*Q（状态，动作）=R（状态，动作）+Gamma*Max[Q（下一状态，所有动作）]*

在这里，我们的时代是一个**插曲**。我们的代理执行*动作*并从环境接收更新或奖励，直到系统状态为终端。在我们的例子中，这意味着陷入迷宫。

我们正在努力学习的是一项政策。此策略是一个函数或状态到操作的映射。这是一个巨大的*n*维优化行动表，给出了我们系统中每个可能的状态。

我们评估状态*S*的能力取决于假设它是一个**马尔可夫决策过程**（**MDP**。正如我们之前指出的，这本书更关注的是实现而不是理论；然而，MDP 是真正理解 RL 的基础，因此有必要对其进行详细介绍。

我们使用大写的*S*来表示系统的所有可能状态。在迷宫的情况下，这是迷宫边界内代理的每个可能位置。

我们使用小写的*s*来表示单个状态。这同样适用于所有动作*A*和单个动作*A*。

每对*（s**、a*产生奖励分配*R*。它还产生*P*，称为转移概率，其中对于给定的*（s，a）*，可能的下一状态的分布为*s（t+1）*。

我们还有一个超参数，它是贴现因子（*伽马*。一般来说，在超参数方面，这是我们自己设定的。这是指定给给定时间步长的预测奖励的相对值。例如，假设我们希望在下一个时间步骤中，而不是在三个时间步骤之后，为预测的奖励分配更大的值。我们可以在我们的目标中体现这一点，以便学习最佳政策；伪代码如下所示：

*最优策略=时间步 t*的最大值（总和（伽马 x 奖励）

进一步分解 DQN 的概念组件，我们现在可以讨论值函数。此函数表示给定状态的累积奖励。例如，在迷宫探索的早期，累积预期回报很低。这是因为我们的代理人可能采取或占据的行动或状态的数量。

# 增强学习

现在，我们来看看我们系统的真正内容：Q 值函数。这包括动作*a1*、*a2*和给定状态*s*的累积预期奖励。当然，我们对寻找最佳 Q 值函数感兴趣。这意味着我们不仅有一个给定的*（s，a）*，而且在我们训练网络的过程中，我们还可以修改或更新 DQN 中权重和偏差的可训练参数（乘积之和）。这些参数允许我们定义一个最佳策略，即应用于代理可用的任何给定状态和操作的函数。这就产生了一个最优的 Q 值函数，从理论上讲，它告诉我们的代理在任何一步的最佳行动过程是什么。一个糟糕的足球类比可能是当教练对着新秀经纪人的耳朵大喊指令时的 Q 值函数。

因此，当使用伪代码编写时，我们对最优策略的追求如下所示：

*最优策略=（状态、动作、θ）*

这里，*θ*是指我们的 DQN 的可训练参数。

那么，什么是 DQN？现在让我们详细检查一下我们的网络结构，更重要的是，它是如何使用的。这里，我们将引入我们的 Q 值函数，并使用我们的神经网络来计算给定状态的预期回报。

与我们目前所涉及的网络一样，我们预先设置了许多超参数：

*   Gamma（未来奖励的贴现系数，例如，0.95）
*   ε（勘探或开发，1.0，倾斜于勘探）
*   ε衰变（随着时间的推移，逐渐转向利用所学知识，例如 0.995）
*   ε衰减最小值（例如，0.01）
*   学习率（尽管使用了**自适应矩估计**（**Adam**），但仍默认设置）
*   州大小
*   动作大小
*   Batch size (in powers of two; start with 32 and tune your way from there)
*   剧集数

我们还需要一个固定的顺序内存用于体验重播功能，将其调整为 2000 个条目。

# 优化与网络架构

至于我们的优化方法，我们使用 Adam。您可能还记得[第 2 章](02.html)*什么是神经网络，我如何训练神经网络？*，Adam 解算器属于使用动态学习率的解算器类别。在 vanilla SGD 中，我们固定学习率。这里，学习速率是按参数设置的，在数据（向量）稀疏是一个问题的情况下，我们可以进行更多的控制。此外，我们使用根 MSE 传播与前一个梯度，了解优化曲面形状的变化率，并通过这样做改进网络处理数据中噪声的方式。

现在，让我们来谈谈神经网络的层次。我们的前两层是标准前馈网络，具有**整流线性单元**（**ReLU**激活：

*输出=激活（dotp（输入，权重）+偏差）*

第一个根据状态大小调整大小（即，系统中所有可能状态的向量表示）。

我们的输出层仅限于可能的操作数量。这些都是通过对第二个隐藏维度的输出应用线性激活来实现的。

我们的损失函数取决于我们拥有的任务和数据；通常，我们将使用 MSE 或交叉熵损失。

# 记住，行动，重播！

Beyond the usual suspects involved in our neural network, we need to define additional functions for our agent's memory. The remember function takes a number of inputs, as follows:

*   状态
*   行动
*   奖励
*   下一个州
*   完成

它将这些值附加到内存中（即，按顺序排列的列表）。

现在，我们定义代理如何在 act 函数中执行操作。这就是我们在探索状态空间和利用所学知识之间取得平衡的地方。以下是要遵循的步骤：

1.  它接受一个值，即`state`。
2.  From there, it applies `epsilon`; that is, if a random value between 0 and 1 is less than `epsilon`, then take a random action. Over time, our epsilon decays, reducing the randomness of the action!
3.  然后，我们将状态输入到我们的模型中，以预测要采取的行动。
4.  从这个函数中，我们返回`max(a)`。

The additional function we need is for the experience replay. The steps that this function take are as follows:

1.  从我们的 2000 单位内存中选择一个随机样本`batch_size`，该样本由前面的记忆函数定义和添加
2.  迭代`state`、`action`、`reward`、`next_state`和`isdone`输入，如下所示：
    1.  设置`target`=`reward`
    2.  如果未完成，则使用以下公式：

*预计未来报酬=当前报酬+（贴现因子（伽马）*调用下一状态模型（预计最大预期报酬）*

3.  将未来`reward`输入映射到模型（即当前状态的预测未来`reward`输入）
4.  最后，`replay`通过传递当前状态和目标未来奖励，对单个训练时期进行记忆
5.  使用`epsilon_decay`减少`epsilon`

这涵盖了 DQNs 和 Q-学习的理论；现在，是时候写一些代码了。

# 用 Gorgonia 中的 DQN 解迷宫

现在，是时候建立我们的迷宫解算器了！

使用 DQN 解决一个小 ASCII 迷宫有点像把推土机带到海滩为你的孩子们建造沙堡：这完全没有必要，但你可以玩一台大机器。然而，作为学习 DQN 的工具，迷宫是无价的。这是因为游戏中状态或动作的数量有限，约束的表示也很简单（例如我们的代理无法穿过的迷宫的*墙*。这意味着我们可以逐步完成我们的程序，轻松检查我们的网络正在做什么。

我们将遵循以下步骤：

1.  为这段代码创建一个`maze.go`文件
2.  导入库并设置数据类型
3.  定义我们的`Maze{}`
4.  编写一个`NewMaze()`函数来实例化这个`struct`

我们还需要定义我们的`Maze{}`助手函数。这些措施包括：

*   `CanMoveTo()`: Check whether a move is valid
*   `Move()`：将玩家移动到迷宫中的坐标处
*   `Value()`：返回给定动作的奖励
*   `Reset()`: Set player to start co-ordinates

让我们来看一下迷宫生成器代码的开始。这是一段摘录，代码的其余部分可以在本书的 GitHub 存储库中找到：

```go
...
type Point struct{ X, Y int }
type Vector Point

type Maze struct {
  // some maze object
  *mazegen.Maze
  repr *tensor.Dense
  iter [][]tile
  values [][]float32

  player, start, goal Point

  // meta

  r *rand.Rand
}
...
```

现在我们已经有了生成迷宫并与之交互所需的代码，我们需要定义简单的前馈、完全连接的网络。我们现在应该已经熟悉了这段代码。让我们创建`nn.go`：

```go
...
type NN struct {
  g *ExprGraph
  x *Node
  y *Node
  l []FC

  pred *Node
  predVal Value
}

func NewNN(batchsize int) *NN {
  g := NewGraph()
  x := NewMatrix(g, of, WithShape(batchsize, 4), WithName("X"), WithInit(Zeroes()))
  y := NewVector(g, of, WithShape(batchsize), WithName("Y"), WithInit(Zeroes()))
...
```

现在我们可以开始定义将使用该神经网络的 DQN。首先，让我们创建一个基本类型为`struct`的`memory.go`文件，用于捕获关于给定情节的信息：

```go
package main

type Memory struct {
  State Point
  Action Vector
  Reward float32
  NextState Point
  NextMovables []Vector
  isDone bool
}
```

我们将制作一个`[]Memories`的内存，并使用它来存储每场比赛的 X/Y 状态坐标、移动向量、预期奖励、下一步状态/可能的移动，以及迷宫是否解决。

现在我们可以编辑我们的`main.go`并将所有内容整合在一起。首先，我们定义了在代表迷宫的*m x n*矩阵中可能的移动：

```go
package main

import (
  "fmt"
  "log"
  "math/rand"
  "time"

  "gorgonia.org/gorgonia"
)

var cardinals = [4]Vector{
  Vector{0, 1}, // E
  Vector{1, 0}, // N
  Vector{-1, 0}, // S
  Vector{0, -1}, // W
}
```

接下来，我们需要将前面定义的神经网络、VM/解算器和特定于 DQN 的超参数连接到主`DQN{}`结构。我们还需要一个`init()`函数来构建嵌入式前馈网络以及`DQN`对象本身：

```go
type DQN struct {
  *NN
  gorgonia.VM
  gorgonia.Solver
  Memories []Memory // The Q-Table - stores State/Action/Reward/NextState/NextMoves/IsDone - added to each train x times per episode

  gamma float32
  epsilon float32
  epsDecayMin float32
  decay float32
}

func (m *DQN) init() {
  if _, err := m.NN.cons(); err != nil {
    panic(err)
  }
  m.VM = gorgonia.NewTapeMachine(m.NN.g)
  m.Solver = gorgonia.NewRMSPropSolver()
}
```

接下来是我们的体验`replay()`功能。在这里，我们首先创建一批内存，从中重新训练和更新我们的网络，逐步更新我们的 epsilon：

```go
func (m *DQN) replay(batchsize int) error {
  var N int
  if batchsize < len(m.Memories) {
    N = batchsize
  } else {
    N = len(m.Memories)
  }
  Xs := make([]input, 0, N)
  Ys := make([]float32, 0, N)
  mems := make([]Memory, N)
  copy(mems, m.Memories)
  rand.Shuffle(len(mems), func(i, j int) {
    mems[i], mems[j] = mems[j], mems[i]
  })

  for b := 0; b < batchsize; b++ {
    mem := mems[b]

    var y float32
    if mem.isDone {
      y = mem.Reward
    } else {
      var nextRewards []float32
      for _, next := range mem.NextMovables {
        nextReward, err := m.predict(mem.NextState, next)
        if err != nil {
          return err
        }
        nextRewards = append(nextRewards, nextReward)
      }
      reward := max(nextRewards)
      y = mem.Reward + m.gamma*reward
    }
    Xs = append(Xs, input{mem.State, mem.Action})
    Ys = append(Ys, y)
    if err := m.VM.RunAll(); err != nil {
      return err
    }
    m.VM.Reset()
    if err := m.Solver.Step(m.model()); err != nil {
      return err
    }
    if m.epsilon > m.epsDecayMin {
      m.epsilon *= m.decay
    }
  }
  return nil
}
```

当我们确定最佳可能移动（或具有最大预期回报的移动）时调用的`predict()`函数是下一步。它获取玩家在迷宫中的位置和单个移动，并返回我们的神经网络对该移动的预期奖励：

```go
func (m *DQN) predict(player Point, action Vector) (float32, error) {
  x := input{State: player, Action: action}
  m.Let1(x)
  if err := m.VM.RunAll(); err != nil {
    return 0, err
  }
  m.VM.Reset()
  retVal := m.predVal.Data().([]float32)[0]
  return retVal, nil
}
```

然后，我们为`n`集定义主要的训练循环，在迷宫中移动，并建立我们的 DQN 记忆：

```go
func (m *DQN) train(mz *Maze) (err error) {
  var episodes = 20000
  var times = 1000
  var score float32

  for e := 0; e < episodes; e++ {
    for t := 0; t < times; t++ {
      if e%100 == 0 && t%999 == 1 {
        log.Printf("episode %d, %dst loop", e, t)
      }

      moves := getPossibleActions(mz)
      action := m.bestAction(mz, moves)
      reward, isDone := mz.Value(action)
      score = score + reward
      player := mz.player
      mz.Move(action)
      nextMoves := getPossibleActions(mz)
      mem := Memory{State: player, Action: action, Reward: reward, NextState: mz.player, NextMovables: nextMoves, isDone: isDone}
      m.Memories = append(m.Memories, mem)
    }
  }
  return nil
}
```

我们还需要一个`bestAction()`函数，在给定一部分选项和迷宫实例的情况下，选择可能采取的最佳移动：

```go
func (m *DQN) bestAction(state *Maze, moves []Vector) (bestAction Vector) {
  var bestActions []Vector
  var maxActValue float32 = -100
  for _, a := range moves {
    actionValue, err := m.predict(state.player, a)
    if err != nil {
      // DO SOMETHING
    }
    if actionValue > maxActValue {
      maxActValue = actionValue
      bestActions = append(bestActions, a)
    } else if actionValue == maxActValue {
      bestActions = append(bestActions, a)
    }
  }
  // shuffle bestActions
  rand.Shuffle(len(bestActions), func(i, j int) {
    bestActions[i], bestActions[j] = bestActions[j], bestActions[i]
  })
  return bestActions[0]
}
```

最后，我们定义了一个`getPossibleActions()`函数来产生一个可能的动作片段，给出了我们的迷宫和我们的小`max()`辅助函数来寻找`float32s`片段中的最大值：

```go
func getPossibleActions(m *Maze) (retVal []Vector) {
  for i := range cardinals {
    if m.CanMoveTo(m.player, cardinals[i]) {
      retVal = append(retVal, cardinals[i])
    }
  }
  return retVal
}

func max(a []float32) float32 {
  var m float32 = -999999999
  for i := range a {
    if a[i] > m {
      m = a[i]
    }
  }
  return m
}
```

所有这些部分就绪后，我们可以编写`main()`函数来完成 DQN。我们首先设置`vars`，其中包括我们的ε。然后初始化`DQN{}`并实例化`Maze`：

然后，我们开始我们的训练循环，完成后，尝试解决我们的迷宫：

```go
func main() {
  // DQN vars

  // var times int = 1000
  var gamma float32 = 0.95 // discount factor
  var epsilon float32 = 1.0 // exploration/exploitation bias, set to 1.0/exploration by default
  var epsilonDecayMin float32 = 0.01
  var epsilonDecay float32 = 0.995

  rand.Seed(time.Now().UTC().UnixNano())
  dqn := &DQN{
    NN: NewNN(32),
    gamma: gamma,
    epsilon: epsilon,
    epsDecayMin: epsilonDecayMin,
    decay: epsilonDecay,
  }
  dqn.init()

  m := NewMaze(5, 10)
  fmt.Printf("%+#v", m.repr)
  fmt.Printf("%v %v\n", m.start, m.goal)

  fmt.Printf("%v\n", m.CanMoveTo(m.start, Vector{0, 1}))
  fmt.Printf("%v\n", m.CanMoveTo(m.start, Vector{1, 0}))
  fmt.Printf("%v\n", m.CanMoveTo(m.start, Vector{0, -1}))
  fmt.Printf("%v\n", m.CanMoveTo(m.start, Vector{-1, 0}))

  if err := dqn.train(m); err != nil {
    panic(err)
  }

  m.Reset()
  for {
    moves := getPossibleActions(m)
    best := dqn.bestAction(m, moves)
    reward, isDone := m.Value(best)
    log.Printf("\n%#v", m.repr)
    log.Printf("player at: %v best: %v", m.player, best)
    log.Printf("reward %v, done %v", reward, isDone)
    m.Move(best)
  }
}
```

现在，让我们执行程序并观察输出：

![](img/ec332bbe-2012-41eb-9c88-4328f20b0ace.png)

我们可以看到迷宫的尺寸，以及墙壁的简单表示（`1`）、清晰的路径（`o`）、我们的播放器（`2`）和迷宫出口（`3`）。下一行`{1 0} {9 20}`分别告诉我们玩家起点和迷宫出口的精确*（X，Y）*坐标。然后，我们通过动作向量进行循环，作为一种理智检查，并开始跨`n`集的训练跑。

我们的代理人现在正在穿越迷宫：

![](img/4413ba38-a2c7-4478-bd05-3597ff7882e7.png)

你可以尝试不同数量的情节（和情节长度），并生成更大更复杂的迷宫！

# 总结

在本章中，我们了解了 RL 的背景以及 DQN 是什么，包括 Q 学习算法。我们了解了 DQN 如何提供一种独特的（相对于我们目前讨论的其他架构）解决问题的方法。我们不提供传统意义上的*输出标签*，比如，我们的 CNN 来自[第 5 章](05.html)，*下一个单词预测与循环神经网络*，后者处理 CIFAR 图像数据。事实上，我们的输出标签是相对于环境状态的给定操作的累积奖励，因此您现在可以看到我们已经动态创建了输出标签。但这些标签并不是我们网络的最终目标，而是帮助虚拟代理在离散的可能性空间中做出智能决策。我们还研究了我们可以对奖励或行动做出哪些类型的预测。

现在，您可以考虑 DQN 的其他可能的应用程序，更一般地说，对于一些问题，您有某种简单的奖励，但没有为您的数据添加标签—典型的示例是某种环境中的代理。*代理*和*环境*应尽可能以最一般的方式定义，因为您不局限于玩 Atari 游戏或试图解决迷宫的一点数学知识。例如，您网站的用户可以被视为代理，而环境是一个空间，您可以在其中对内容进行某种基于功能的表示。您可以使用这种方法构建新闻推荐引擎。您可以参考*进一步阅读*部分，以获取您可能希望作为练习实施的论文链接。

在下一章中，我们将研究构建一个**变分自编码器**（**VAE**），并了解 VAE 相对于标准自编码器的优势。

# 进一步阅读

*   *通过深度强化学习玩雅达利*，可在[下载 https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)
*   *DRN：新闻推荐的深度强化学习框架*，可在[上找到 http://www.personal.psu.edu/~gjz5038/paper/www2018\u reinforceRec/www2018\u reinforceRec.pdf](http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf)