# 一、Go 深度学习导论

这本书将很快跳转到在 Go 中实现**深度神经网络**（**DNNs**）的实用性。简而言之，这本书的标题包含了它的目的。这意味着将有大量的技术细节、大量的代码和（不太多的）数学。当您最终关闭本书或关闭 Kindle 时，您将知道如何（以及为什么）实现现代化、可扩展的 DNN，并能够根据您所参与的任何行业或疯狂科学项目的需要重新调整它们的用途。

我们对 Go 的选择反映了为 DNN 执行的各种操作而构建的 Go 库的成熟。当然，在选择语言或库时会有很多关于取舍的争论，我们将在本章的一节讨论我们的观点，并为我们所做的选择进行辩论。

然而，没有上下文的代码是什么？为什么我们要关心线性代数、微积分、统计学和概率这些看似复杂的组合？为什么要使用计算机来识别图像中的事物或识别金融数据中的异常模式？也许最重要的是，这些任务的方法有什么共同点？本书的最初几节将尝试提供一些这方面的内容。

当科学努力被分解为代表其机构和行业专业化的学科时，就会受到进步理念的支配。我们指的是一种动力，一种朝着某个目标前进的动力。例如，医学的理想目标是能够识别和治疗任何疾病。物理学家的目标是完全理解自然的基本规律。这方面的进展趋势。科学本身就是一种优化方法。那么，**机器学习**（**ML**的最终目标是什么？

我们会在前面。我们认为这是**人工通用智能**（**AGI**的创造。这就是奖品：一台通用学习型计算机，它可以处理工作，把生活留给人们。当我们详细介绍**深度学习**（**DL**的历史时，我们将看到，顶级**人工智能**（**AI**实验室）的创始人同意 AGI 代表了当今世界许多复杂问题的*元解决方案*，从经济学到医学再到政府。

本章将涵盖以下主题：

*   为什么是 DL？
*   DL 历史应用程序
*   Go 中的 ML 概述
*   使用蛇舌草

# 介绍 DL

现在，我们将提供一个关于 DL 为何重要以及它如何适合 AI 讨论的高级视图。然后，我们将回顾 DL 的历史发展，以及当前和未来的应用

# 为什么是 DL？

那么，亲爱的读者，你是谁？你为什么对 DL 感兴趣？你对人工智能有自己的看法吗？或者你有更谦虚的东西吗？你的*起源故事*是什么？

在我们对同事、老师和聚会熟人的调查中，对机器有更正式兴趣的人的起源故事有几个共同特征。如果你从小就玩电脑游戏，一个有时会出故障的隐形敌人，或者你在上世纪 90 年代末 id 软件的《地震》中追击真实的机器人，这都没有多大关系；软硬件思维和独立行动相结合的想法在我们每个人的早期生活中都产生了影响。

然后，随着时间的推移，随着年龄的增长、教育程度的提高和对流行文化的接触，你的想法变得越来越精致，也许你最终成为了一名研究人员、工程师、黑客或业余爱好者，现在你想知道如何参与启动这台伟大的机器。

如果你的兴趣比较温和，比如说，你是一名数据科学家，希望了解尖端技术，但对所有这些关于感知软件和科幻小说的讨论持矛盾态度，那么在许多方面，你比大多数人更能为 2019 年的 ML 现实做好准备。我们每个人，不管我们的雄心壮志有多大，都必须理解代码的逻辑，并通过反复试验努力工作。谢天谢地，我们有非常快速的图形卡。

这些基本真理的结果是什么？现在，2019 年，DL 在许多方面对我们的生活产生了影响。棘手的问题正在得到解决。有些琐碎，有些不重要。是的，Netflix 有一个你最尴尬的电影偏好模型，但 Facebook 有针对视力受损者的自动图像注释。理解 DL 的潜在影响就像观察一个刚刚第一次看到爱人照片的人脸上喜悦的表情一样简单

# DL–历史

现在，我们将简要介绍 DL 的历史及其产生的历史背景，包括以下内容：

*   **AI**的理念
*   计算机科学/信息理论的开端
*   关于 DL 系统现状/未来的当前学术工作

虽然我们对 DL 特别感兴趣，但该领域并非从无到有。它是计算机科学的一个分支——ML 本身中的一组模型/算法。它形成了一种人工智能方法。另一种是所谓的**符号 AI**，它围绕着用代码编写的手工制作（而不是学习）的特征和规则，而不是包含从算法中提取的模式的加权模型。

在成为一门科学之前，思考机器的概念在很大程度上是一种始于古代的虚构。希腊的武器制造之神赫菲斯托斯（Hephaestus）用黄金和白银制造了机器人。它们满足了他的奇思妙想，是人类想象力的早期范例，自然地考虑复制自身的具体形式可能需要什么。

将历史向前推进几千年，20 世纪信息理论和计算机科学中的几个关键人物构建了一个平台，使人工智能成为一个独特的领域，包括我们将要介绍的 DL 领域的最新工作。

第一位主要人物，克劳德·香农，为我们提供了传播的一般理论。具体而言，他在其里程碑式的论文*中描述了一种数学计算理论*如何确保在不完美介质上传输信息时不丢失信息（比如说，使用真空管进行计算）。这一概念，特别是他的噪声信道编码定理，被证明对于可靠地处理任意大量的数据和算法至关重要，而不会将介质本身的错误引入通信信道。

艾伦·图灵在 1936 年描述了他的*图灵机器*，为我们提供了一个通用的计算模型。通过他描述的基本构造块，他定义了机器可能计算的极限。他受到约翰·冯·诺依曼的*存储程序*思想的影响。图灵工作的关键洞察是数字计算机可以模拟任何形式推理过程（图灵假设）。下图显示了图灵机器过程：

![](img/65ebbc44-c930-4920-a13f-f268400e5caf.png)

*那么，图灵先生，你的意思是告诉我们，计算机可能会被制造成理性的……就像我们一样？！*

约翰·冯·诺依曼本人也受到图灵 1936 年论文的影响。在晶体管发展之前，真空管是唯一可用的计算手段（在 ENIAC 及其衍生物等系统中），约翰·冯·诺依曼（John Von Neumann）发表了他的最后作品。它在他去世时仍然不完整，被命名为“计算机与大脑”。尽管仍不完整，但它很早就考虑了计算模型如何在大脑中像在机器中一样运作，包括早期神经科学对神经元和突触之间联系的观察。

自 1956 年人工智能最初被认为是一个独立的研究领域以来，随着 1959 年 ML 的诞生，该领域经历了一个备受讨论的高潮和低潮期，在这个时期，炒作和资金充足，私人部门资金不存在的时期，研究会议甚至不接受强调神经网络方法构建人工智能系统的论文。

Within AI itself, these competing approaches cannibalized research dollars and talent. Symbolic AI met its limitations in the sheer impossibility of handcrafting rules for advanced tasks such as image classification, speech recognition, and machine translation. ML sought to radically reconfigure this process. Instead of applying a bunch of human-written rules to data and hoping to get answers, human labor was, instead, to be spent on building a machine that could infer rules from data when the answers were known. This is an example of *supervised learning*, where the machine learns an essential *cat-ness* after processing thousands of example images with an associated *cat* label.

Quite simply, the idea was to have a system that could generalize. After all, the goal is AGI. Take a picture of your family's newest furry feline and the computer, using its understanding of *cat-ness*, correctly identifies a *cat*! An active area of research within ML, one thought essential for building a general AI, is *transfer learning*, where we might take the machine that understands *cat-ness* and plug it into a machine that, in turn, acts when *cat-ness* is identified. This is the approach many AI labs around the world are taking: building systems out of systems, augmenting algorithmic weakness in one area with statistical near certainty in another, and, hopefully, building a system that better serves human (or business) needs.

*服务于人类需求*的概念将我们带到了一个关于人工智能伦理的重要观点（以及我们将关注的 DL 方法）。媒体、学术界或产业界围绕这些制度的伦理含义进行了大量讨论。由于计算机视觉的进步，如果我们有了简单、自动化、广泛的监控，这对我们的社会意味着什么？那么自动化武器系统或制造业呢？想象一下那些没有脉搏的仓库，已经不再是一种奢望了。那么，那些曾经做过这些工作的人呢？

当然，对这些重要问题的充分考虑超出了本书的范围，但这是我们工作的背景。您将是少数能够构建这些系统并推动该领域向前发展的特权人员之一。尼克·博斯特罗姆（Nick Bostrom）管理的牛津大学**人类未来研究所**和麻省理工学院（MIT）物理学家马克斯·特马克（Max Tegmark）管理的**生命未来研究所**的工作就是围绕人工智能伦理问题展开学术辩论的两个例子。然而，这场辩论并不局限于学术界或非营利组织；DeepMind 是一家字母表公司，其目标是成为 AGI 的*阿波罗计划，于 2017 年 10 月启动了*DeepMind 伦理&协会*。*

这似乎与代码、CUDA 和神经网络识别 cat 图片的世界相去甚远，但随着进步，这些系统变得更先进，应用范围更广，我们的社会将面临真正的后果。作为研究人员和开发人员，我们应该有一些答案，或者至少有一些想法，当我们不得不面对这些挑战时，我们应该如何应对这些挑战。

# DL–炒作还是突破？

DL 和相关的宣传是一个相对较新的发展。大多数关于其*出现*的讨论集中在 2012 年的 ImageNet 基准测试上，其中深度卷积神经网络的错误率比前一年高出 9%，这是一个显著的改进，之前的获奖者通过在模型中使用手工制作的功能，最多只能进行增量改进。下图显示了这一改进：

![](img/1bcd2687-c8b6-4343-b5cf-6b58bc606a4c.png)

Despite the recent hype, the components that make DL work, which allow us to train deep models, have proven very effective in image classification and various other tasks. These were developed in the 1980s by Geoffrey Hinton and his group at the University of Toronto. Their early work took place during one of the *flow* periods discussed earlier in this chapter. Indeed, they were wholly dependent on funding from the **Canadian Institute for Advanced Research** (**CIFAR**).

随着 21 世纪的真正开始，2000 年 3 月破灭的科技泡沫开始再次膨胀，高性能 GPU 的可用性和计算能力的增长更普遍地意味着这些技术，几十年前开发的，但由于缺乏资金和行业兴趣而未使用的，突然变得可行。以前在图像识别、语音识别、自然语言处理和序列建模方面只有增量改进的基准测试都调整了它们的*y-*轴。

让我们走到这一步的不仅仅是硬件的巨大进步和旧算法。算法的进步使我们能够训练特别深入的网络。其中最著名的是 2015 年推出的批量标准化。它可以确保跨层的数值稳定，并可以防止梯度爆炸，大大减少训练时间。关于*为什么*批量规范化如此有效，仍然存在着激烈的争论。这方面的一个例子是 2018 年 5 月发表的一篇论文反驳了原论文的中心前提，即不是*内部共变异移位*被减少，而是*使优化景观更平滑*，即梯度可以更可靠地传播，学习率对训练时间和稳定性的影响更容易预测。

从古希腊神话的民间科学到信息理论、神经科学和计算机科学的真正突破，特别是在计算模型方面，它们结合起来产生了网络架构和训练它们所需的算法，能够很好地扩展到 2018 年解决许多基本人工智能任务，这些任务几十年来一直难以解决。

# Defining deep learning

现在，让我们后退一步，从一个简单的、工作的 DL 定义开始。当我们通过这本书工作时，我们对这个术语的理解将会发展，但是，现在，让我们考虑一个简单的例子。我们有一个人的形象。我们如何向计算机显示这张图片？我们如何才能*教*计算机将此图像与*人*一词相关联？

首先，我们计算出这个图像的一个表示，比如说图像中每个像素的 RGB 值。然后，我们将该值数组（以及几个可训练的参数）输入到一系列我们非常熟悉的操作（乘法和加法）中。这产生了一个新的表示，我们可以使用它与我们知道的映射到标签*person*的表示进行比较。我们自动执行这个比较过程，并在执行过程中更新参数值。

This description covers a simple, shallow ML system. We'll get into more detail in a later chapter devoted to neural networks but, for now, to make this system deep, we increase the number of operations on a greater number of parameters. This allows us to capture more information regarding the thing we're representing (the person's image). The biological model that influences the design of this system is the human nervous system, including neurons (the things we fill with our representations) and synapses (the trainable parameters).

下图显示了正在进行的 ML 系统：

![](img/128ef3d0-ba49-495c-a248-46d20f08cfd6.png)

因此，DL 只是 1957 年的*感知器上的一个进化扭曲，*是最简单、最原始的二元分类器。*这一转变，加上计算能力的大幅提高，是不工作的系统和允许汽车自动驾驶的系统之间的区别*

 *除了自动驾驶汽车之外，DL 和相关方法在农业、作物管理和卫星图像分析中也有许多应用。先进的计算机视觉为除草和减少农药使用的机器提供动力。我们有快速准确的近实时语音搜索。从食品生产到通讯，这是社会的基本要素。此外，我们还处于引人注目的实时视频和音频生成的关键时刻，这将使今天关于什么是*假新闻*的隐私辩论或戏剧看起来微不足道。

早在我们到达 AGI 之前，我们就可以利用一路上的发现来改善我们周围的世界。DL 就是这些发现之一。它将推动自动化程度的提高，只要随之而来的政治变革是支持性的，自动化程度就可以在许多行业中提供改进，这意味着商品和服务将变得更便宜、更快、更广泛。理想情况下，这意味着人们将越来越从他们祖先的日常生活中解放出来。

进步的黑暗面也不容忘记。可以识别受害者的机器视觉也可以识别目标。事实上，由斯蒂芬·霍金（Stephen Hawking）和埃隆·马斯克（Elon Musk）等科技名人支持的生命未来研究所（Future of Life Institute）关于自主武器的公开信（*自主武器：人工智能&机器人学研究人员*的公开信）就是学术部门、工业实验室、，政府也在思考什么是正确的进步。在我们的世界里，民族国家传统上控制着枪支和金钱。先进的人工智能可以被武器化，这是一场可能一组人赢而我们其余人输的比赛。

更具体地说，ML 领域正在以难以置信的速度发展。我们如何衡量这一点？2017 年，顶级 ML 会议**神经信息处理系统**（**NIPS**）的注册量是 2010 年的七倍多。

2018 年的注册更多地是以摇滚音乐会的方式进行的，而不是枯燥的技术会议，这反映在组织者自己发布的以下统计数据中：

![](img/7352c9fb-222f-43e5-b5f8-cd56732a7ce2.png)

*事实上的*ML 预印本中央存储库**arXiv**有一个这种极端情况的曲棍球棒生长图，其中出现了帮助研究人员跟踪所有新工作的工具。这方面的一个例子是特斯拉（Andrej Karpathy 的网站，arxiv sanity）（[）的人工智能总监 http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/) ）。该网站允许我们对论文进行分类/分组，并组织一个界面，通过该界面，我们可以相对轻松地从出版物流中提取我们感兴趣的研究

我们无法预测未来五年的进展速度。风险投资家和专家的专业猜测范围从指数级到*下一个人工智能冬天即将到来*。但是我们现在有了技术、库和计算能力*，知道如何在自然语言处理或计算机视觉任务中使用它们有助于解决现实世界的问题。*

 *这就是我们这本书的目的，告诉你怎么做。

# Go 中的 ML 概述

本节将介绍 Go 中的 ML 生态系统，首先讨论我们希望从库中获得的要素，然后依次评估每个主要的 Go ML 库。

Go 的 ML 生态系统在历史上非常有限。该语言是在 2009 年引入的，远远早于 DL 革命，DL 革命已经将许多新的程序员带进了这个领域。您可能会认为 Go 已经看到了其他语言所拥有的库和工具的增长。相反，历史决定了许多支持我们网络的数学运算的高级 API 都以 Python 库的形式出现（或者具有完整的 Python 绑定）。这方面有很多著名的例子，包括 PyTorch、Keras、TensorFlow、Theano 和 Caffe（你知道了）。

不幸的是，这些库没有 Go 绑定或绑定不完整。例如，TensorFlow 可以进行推理（*这是不是一只猫？*），但不能进行训练（*什么是猫？哦，好吧，我来看看这些例子，找出它*。从开发人员的角度来看，这充分利用了 Go 在部署方面的优势（编译到单二进制、编译器速度和低内存占用），但随后您不得不跨两种语言工作（Python 用于训练模型，Go 用于运行模型）。

除了在设计、实现或故障排除时切换语法的认知影响之外，您可能会面临的问题还扩展到环境和配置问题。这些问题包括以下问题：*我的 Go 环境是否正确配置了**我的 Python 2 二进制代码是否与 Python 进行了符号链接，还是 Python 3*？*TensorFlow GPU 是否正常工作*？如果我们的兴趣是设计最佳模型，并在最短的时间内对其进行培训和部署，那么 Python 或 Go 绑定库都不适用。

在这一点上，重要的是要问：那么，我们想从 Go 中的*DL 库*中得到什么？很简单，我们希望尽可能多地抽象掉不必要的复杂性，同时保持对模型的灵活性和控制，以及如何训练模型。

这在实践中意味着什么？以下列表概述了此查询的答案：

*   我们不想直接与**基本线性代数子程序**（**BLAS**接口来构造乘法、加法等基本运算。
*   我们不希望每次实现新网络时都定义张量类型和相关函数。
*   我们不想每次训练我们的网络时都从头开始编写一个**随机梯度下降**（**SGD**的实现。

以下是本书将介绍的一些内容：

*   **自动或符号微分**：我们的 DNN 正在尝试学习一些功能。它迭代地*解决了*问题*获取输入图像并输出标签 cat*的功能是什么？通过计算损失的梯度（*梯度下降优化*）（*我们的函数*有多大的错误？）。这使我们能够了解是否要改变网络中的权重，以及改变多少，具体的微分模式*打破了*这些梯度的计算（有效地使用链式规则），为我们提供了训练具有数百万参数的深层网络所需的性能。
*   **N****umerical stabilization** **function****s**: This is essential for DL, as we will explore in later sections of this book. A primary example of this is **Batch Normalization** or BatchNorm, as the attendant function is often called. It aims to put our data on the same scale to increase training speed, and it reduces the possibility of maximum values cascading through the layers and causing gradient explosion (something we will discuss in greater detail in [Chapter 2](02.html), *What is a Neural Network and How Do I Train One?*).
*   **激活函数**：这些数学运算将非线性引入我们神经网络的各个层，并帮助确定一层中的哪些神经元将被*激活*，将它们的值传递给网络中的下一层。示例包括 Sigmoid、**整流线性单元**（**ReLU**）和 Softmax。这些将在[第 2 章](02.html)*中更详细地讨论，什么是神经网络，如何训练神经网络？*
*   **梯度下降优化**：我们还将在[第 2 章](02.html)、*什么是神经网络以及如何训练神经网络中详细介绍这些内容？但是，作为 DNNs 的主要优化方法，我们认为这是任何图书馆损坏 DL 所必需的核心函数。*
*   **CUDA 支持**：Nvidia 的驱动程序允许我们将 DNN 中涉及的基本操作卸载到 GPU 中。GPU 对于涉及矩阵变换的并行工作负载非常有用（事实上，这是它们最初的目的：计算游戏的世界几何体），并且可以将训练模型所需的时间减少一个数量级或更多。可以说，CUDA 支持对于现代 DNN 实现至关重要，因此可以在前面描述的主要 Python 库中获得。
*   **部署工具**：正如我们将在[第 9 章](09.html)*构建深度学习管道*中所述，在有关 DL 的讨论中，通常会忽略部署用于培训或推理的模型。随着神经网络架构变得越来越复杂，以及大量数据的可用性，在 AWS GPU 上训练网络，或将训练后的模型部署到其他系统（例如，集成到新闻网站的推荐系统）是一个关键步骤。您将提高培训时间，并延长可使用的计算量。这意味着也可以用更复杂的模型进行实验。理想情况下，我们希望有一个库，可以轻松地与现有工具集成，或者有自己的工具。

既然我们已经对我们理想的图书馆有了一套合理的要求，让我们来看看一些社区中流行的选项。以下列表并非详尽无遗；但是，它涵盖了 GitHub 上与 ML 相关的大多数主要 Go 项目，从最窄的到最通用的。

# ML 库

现在我们将考虑每个主要的 ML 库，根据先前定义的标准来评估它们的实用性，包括任何负面的方面或缺点。

# Go 中的单词嵌入

**Go**中的单词嵌入是特定于任务的 ML 库的一个示例。它使用`Word2vec`和`GloVe`实现生成单词嵌入所需的两层神经网络。这是一个很好的实现，快速、干净。它以特定于通过`Word2vec`和`GloVe`生成单词嵌入任务的方式很好地实现了有限数量的功能。

这方面的一个例子是训练 DNN 所需的核心特性，一种称为 SGD 的优化方法。这在斯坦福大学的一个团队开发的`GloVe`模型中使用。但是，该代码专门与`GloVe`模型集成，并且`Word2Vec`中使用的额外优化方法（负采样和跳过 gram）对 DNN 没有用处。

这对于 DL 任务非常有用，例如，生成可用于**长短期记忆**（**LSTM**网络的文本语料库的嵌入层或密集向量表示，我们将在[第 5 章](05.html)、*下一个单词预测与循环神经网络*中介绍。然而，我们需要的所有高级功能（例如，梯度下降或反向传播）和模型特征（LSTM 单元本身）都不存在。

# Go 或 Golang 的朴素贝叶斯分类和遗传算法

这两个库构成了 Go 中另一组特定于任务的 ML 库示例。两者都编写得很好，并提供了特定于其功能的原语，但这些原语并不通用。在朴素贝叶斯分类器`lib`中，矩阵在使用之前是人工构建的，而传统的泛型算法根本不使用矩阵。在将它们纳入 GA 方面已经做了一些工作；然而，这项工作还没有进入我们在这里引用的 GA 库。

# 加油

GoLearn 是一个包含更多有用功能的库。虽然 DL 特定的特性在其愿望列表中，但它具有实现简单神经网络、随机森林、聚类和其他 ML 方法所必需的原语。它严重依赖 Gonum，这是一个库，提供`float64`和`complex128`矩阵结构的实现以及对它们的线性代数运算。

让我们从代码的角度来看这意味着什么，如下所示：

```go
type Network struct {
       origWeights *mat.Dense
       weights *mat.Dense // n * n
       biases []float64 // n for each neuron
       funcs []NeuralFunction // for each neuron
       size int
       input int
   }
```

这里，我们有戈尔恩对神经网络的基本定义。它包含使用 Gonum 的`mat`库创建权重作为密集矩阵的权重定义。它有偏差、功能、大小和输入，这些都是基本前馈网络的基本要素。（我们将在[第 3 章](03.html)中介绍前馈网络，*将超越基本神经网络–自编码器和 RBM*。

所缺乏的是在层内和层间轻松定义连接的能力（对于高级网络架构，如 RNN 及其衍生物，以及 DL 所必需的功能，如卷积操作和批处理规范化）。手工编写这些代码将为您的项目增加大量的开发时间，更不用说优化其性能所需的时间了。

另一个缺失的功能是 CUDA 支持，用于培训和扩展 DL 中使用的网络架构。我们将在[第 4 章](04.html)、*CUDA–GPU 加速培训*中介绍 CUDA，但如果没有这种支持，我们将局限于不使用大量数据的简单模型，即我们在本书中感兴趣的那种。

# Golang 的机器学习库

该库的不同之处在于它实现了自己的矩阵运算，不依赖于 Gonum。它实际上是一组实现，包括以下内容：

*   线性回归
*   逻辑回归
*   神经网络
*   协同过滤
*   异常检测系统的高斯多元分布

就个人而言，这些都是强大的工具；事实上，线性回归通常被描述为数据科学家工具包中最重要的工具之一，但就我们的目的而言，我们实际上只关心库中的神经网络部分。在这里，我们看到了与 GoLearn 类似的限制，例如激活功能有限，缺乏用于内部和层间连接的工具（例如，LSTM 单元）。

作者还有一个实现 CUDA 矩阵运算的附加库；然而，这个库和`go_ml`库本身已经四年没有更新了（在撰写本文时），因此这不是一个可以直接导入并开始构建神经网络的项目。

# 戈布雷恩

Another library that is not under active development is GoBrain. You might then ask: why bother reviewing it? Briefly, it is of interest because it is the only other library apart from Gorgonia that attempts to implement primitives from more advanced network architectures. Specifically, it extends its primary network, which is a basic feedforward neural network, to become something new, an **Elman recurrent neural network**, or **SRN**.

1990 年引入，这是第一个包含循环的网络架构，连接网络的隐藏层和相邻的*上下文*单元。这使得网络能够学习序列依赖性，比如单词的*上下文*，或者潜在的人类语言的语法和句法。SRN 开创性地提供了*愿景，即这些单元可能是在语音流*的潜在结构上运行的学习过程的紧急后果。

SRN 已经让位给了更现代的循环神经网络，我们将在[第 5 章](05.html)、*下一个单词使用循环神经网络进行预测*中详细介绍。然而，在 GoBrain 中，我们有一个有趣的例子，其中包含了我们工作所需的开始。

# Go 编程语言的一组数字库

可能对 DL 有用的功能最完整的库（除了 Gorgonia，我们将在后面的章节中介绍）是 Gonum。最简单的描述是 Gonum 试图模仿 Python 中著名的科学计算库的许多功能，即 NumPy 和 SciPy。

让我们来看一个代码示例，用来构造一个矩阵，我们可以用它来表示 DNN 的输入。

Initialize a matrix and back it with some numbers, as follows:

```go
// Initialize a matrix of zeros with 3 rows and 4 columns.
d := mat.NewDense(3, 4, nil)
fmt.Printf("%v\n", mat.Formatted(d))
// Initialize a matrix with pre-allocated data. Data has row-major storage.
data := []float64{
    6, 3, 5,
   -1, 9, 7,
    2, 3, 4,}
d2 := mat.NewDense(3, 3, data)
fmt.Printf("%v\n", mat.Formatted(d2))
```

对矩阵执行操作，如以下代码所示：

```go
a := mat.NewDense(2, 3, []float64{
   3, 4, 5,
   1, 2, 3,
})

b := mat.NewDense(3, 3, []float64{   1, 1, 8,
   1, 2, -3,
   5, 5, 7,
})
fmt.Println("tr(b) =", mat.Trace(b))

c := mat.Dense{}
c.Mul(a, b)
c.Add(c, a)
c.Mul(c, b.T())
fmt.Printf("%v\n", mat.Formatted(c))
```

在这里，我们可以看到 Gonum 为我们提供了操作 DNN 中层间交换矩阵所需的原语，即`c.Mul`和`c.Add`。

When we decide to scale up our design ambitions, this is when we run into the limitations of Gonum. There are no GRU/LSTM cells and there is no SGD with backpropagation. If we are to reliably and efficiently construct DNNs that we want to carry all of the way through to production, we need to look elsewhere for a more complete library.

# 使用蛇舌草

在写这本书的时候，有两个库通常会被考虑用于 Go 中的 DL，TensorFlow 和 Gorgonia。然而，尽管 TensorFlow 肯定受到好评，并且在 Python 中有一个功能齐全的 API，但在 Go 中并非如此。如前所述，TensorFlow 的 Go 绑定只适合加载已经在 Python 中创建的模型，而不适合从头创建模型。

Gorgonia 从一开始就是一个 Go 库，它能够训练 ML 模型并执行推理。这是一个特别有价值的属性，特别是如果您有一个现有的 Go 应用程序或者您正在寻找构建一个 Go 应用程序。Gorgonia 允许您在现有的 Go 环境中开发、培训和维护 DL 模型。在本书中，我们将专门使用 Gorgonia 来构建模型。

在我们继续构建模型之前，让我们先了解一下 Gorgonia 的一些基础知识，并学习如何在其中构建简单的方程。

# The basics of Gorgonia

**Gorgonia**是一个较低级别的库，这意味着我们需要自己构建模型的方程和架构。这意味着没有内置的 DNN 分类器函数可以神奇地创建具有多个隐藏层的整个模型，并立即准备应用于数据集。

Gorgonia 作为一个库简化了使用多维数组的工作，从而简化了 DL。它通过提供大量的操作符来实现这一点，这样您就可以构建构成 DL 模型中各层的基本数学方程。然后，我们可以继续在模型中使用这些层。

Gorgonia 的另一个重要特征是性能。通过消除对如何优化张量运算的思考，我们可以专注于构建模型并确保架构是正确的，而不是担心我们的模型是否会有性能。

由于 Gorgonia 的级别略低于典型的 ML 库，因此构建模型需要更多的步骤。然而，这并不意味着在 Gorgonia 中构建模型是困难的。它需要以下三个基本步骤：

1.  创建一个计算图
2.  输入数据
3.  Execute the graph

Wait, what's a computation graph? A **computation graph** is a directed graph where each of the nodes is either an operation or a variable. Variables can be fed into operations, which will then produce a value. This value can then be fed into another operation. In more familiar terms, a graph is like a function that takes all of the variables and then produces a result.

A variable can be anything; we can make it a single scalar value, a vector (that is, an array), or a matrix. In DL, we typically work with a more generalized structure called a tensor; a tensor can be thought of as something similar to an *n*-dimensional matrix.

以下屏幕截图显示了*n*维张量的视觉表示：

![](img/906774f8-7b4b-4e6f-aabd-7b0e57cf7d16.png)

我们将方程表示为图形，因为这使我们更容易优化模型的性能。这是因为，通过将每个节点放在一个有向图中，我们可以很好地了解它的依赖关系是什么。因为我们将每个节点建模为一段独立的代码，所以我们知道它需要执行的只是它的依赖项（可以是其他节点或其他变量）。此外，在遍历图时，我们可以知道哪些节点彼此独立，并并发运行这些节点。

例如，以下图为例：

![](img/f71250c6-0790-48c1-8d43-48ace79d54e3.png)

As **A**, **B**, and **C** are independent, we can easily compute these concurrently. The computation of **V** requires both **A** and **B** to be ready. However, **W** only requires **B** to be ready. This follows into the next level, and so on, up until we are ready to compute the final output in **Z**.

# 简单示例-添加

理解这一切是如何结合在一起的最简单方法是构建一个简单的示例。

首先，让我们实现一个简单的图形，将两个数字基本相加，这将是：*c=a+b*：

1.  首先，让我们导入一些最重要的库 Gorgonia，如下所示：

```go
package main
import (
     "fmt"
     "log"
     . "gorgonia.org/gorgonia"
 )
```

2.  然后，让我们开始我们的主要功能，如下所示：

```go
func main() {
  g := NewGraph()
}
```

3.  在此基础上，我们添加标量，如下所示：

```go
a = NewScalar(g, Float64, WithName("a"))
b = NewScalar(g, Float64, WithName("b"))
```

4.  然后，非常重要的是，让我们定义我们的操作节点，如下所示：

```go
c, err = Add(a, b)
if err != nil {
              log.Fatal(err)
              }
```

Note that `c` will not actually have a value now; we've just defined a new node of our computation graph, so we need to execute it before it will have a value.

5.  要执行它，我们需要创建一个虚拟机对象，让它在其中运行，如下所示：

```go
machine := NewTapeMachine(g)
```

6.  然后，设置初始值`a`和`b`，继续让机器执行我们的图形，如下所示：

```go
Let(a, 1.0)
Let(b, 2.0)
if machine.RunAll() != nil {
                           log.Fatal(err)
                           }
```

完整代码如下：

```go
package main

import (
         "fmt"
         "log"

         . "gorgonia.org/gorgonia"
)

func main() {
         g := NewGraph()

         var a, b, c *Node
         var err error

         // define the expression
         a = NewScalar(g, Float64, WithName("a"))
         b = NewScalar(g, Float64, WithName("b"))
         c, err = Add(a, b)
         if err != nil {
                  log.Fatal(err)
         }

         // create a VM to run the program on
         machine := NewTapeMachine(g)

         // set initial values then run
         Let(a, 1.0)
         Let(b, 2.0)
         if machine.RunAll() != nil {
                  log.Fatal(err)
         }

         fmt.Printf("%v", c.Value())
         // Output: 3.0
}
```

现在，我们已经在 Gorgonia 中构建了我们的第一个计算图并执行了它！

# 向量与矩阵

当然，能够增加数字并不是我们在这里的原因；我们在这里研究张量，以及最终的 DL 方程，让我们朝着更复杂的方向迈出第一步。

现在，我们的目标是创建一个图表，用于计算以下简单等式：

*z=Wx*

注意，*W*是一个*n*x*n*矩阵，*x*是一个大小为*n*的向量。在本例中，我们将使用*n=2*.1957。

同样，我们从相同的基本主函数开始，如下所示：

```go
package main

import (
        "fmt"
        "log"

        G "gorgonia.org/gorgonia"
        "gorgonia.org/tensor"
)

func main() {
        g := NewGraph()
}
```

您会注意到，我们选择将 Gorgonia 包别名为`G`。

然后我们创建第一个张量，矩阵，`W`，如下所示：

```go
matB := []float64{0.9,0.7,0.4,0.2}
matT := tensor.New(tensor.WithBacking(matB), tensor.WithShape(2, 2))
mat := G.NewMatrix(g,
        tensor.Float64,
        G.WithName("W"),
        G.WithShape(2, 2),
        G.WithValue(matT),
)

```

您会注意到，这次我们做的事情有点不同，如下所示：

1.  我们首先声明了一个数组，其中包含我们想要在矩阵中使用的值
2.  然后我们从这个矩阵中创建了一个形状为 2x2 的张量，我们想要一个 2x2 矩阵
3.  所有这些之后，我们在图中为矩阵创建了一个新节点，命名为`W`，并用张量值初始化它

然后我们以同样的方式创建第二个张量和输入节点，向量`x`，如下所示：

```go
vecB := []float64{5,7}

vecT := tensor.New(tensor.WithBacking(vecB), tensor.WithShape(2))

vec := G.NewVector(g,
        tensor.Float64,
        G.WithName("x"),
        G.WithShape(2),
        G.WithValue(vecT),
)
```

就像上次一样，我们随后添加了一个操作符节点`z`，它将使二者相乘（而不是加法操作）：

```go
z, err := G.Mul(mat, vec)
```

然后，与上次一样，创建一台新的磁带机并运行它，如图所示，然后打印结果：

```go
machine := G.NewTapeMachine(g)
if machine.RunAll() != nil {
        log.Fatal(err)
}
fmt.Println(z.Value().Data())
// Output: [9.4 3.4]
```

# 可视化图形

在许多情况下，可视化图形也是非常有用的；您可以通过在导入中添加`io`或`ioutil`并在代码中添加以下行来轻松完成此操作：

```go
ioutil.WriteFile("simple_graph.dot", []byte(g.ToDot()), 0644)
```

这将产生一个点文件；您可以在 GraphViz 中打开它，或者更方便地将其转换为 SVG。通过安装 GraphViz 并在命令行中输入以下命令，您可以在大多数现代浏览器中查看它：

```go
dot -Tsvg simple_graph.dot -O
```

This will produce `simple_graph.dot.svg`; you can open this in your browser to see a rendering of the graph, as follows:

![](img/0514dde0-b38c-440b-84a4-4b1df0ffcacc.png)

你可以看到，在我们的图中，我们有两个输入，`W`和`x`，然后输入到我们的运算符中，这是一个矩阵乘以一个向量，得到的结果也是另一个向量。

# 构建更复杂的表达式

当然，我们主要讨论了如何建立简单的方程；但是，如果您的方程式稍微复杂一点，例如，如下所示，会发生什么情况：

*z=Wx+b*

We can also very easily do this by changing our code a bit to add the following line:

```go
b := G.NewScalar(g,
        tensor.Float64,
        G.WithName("b"),
        G.WithValue(3.0)
)
```

然后，我们可以稍微更改`z`的定义，如下所示：

```go
a, err := G.Mul(mat, vec)
if err != nil {
        log.Fatal(err)
}

z, err := G.Add(a, b)
if err != nil {
        log.Fatal(err)
}
```

如您所见，我们创建了一个乘法运算符节点，然后在其上创建了一个加法运算符节点。

或者，您也可以在一行中完成，如下所示：

```go
z, err := G.Add(G.Must(G.Mul(mat, vec)), b)
```

注意，我们在这里使用`Must`来抑制错误对象；我们在这里这样做只是为了方便，因为我们知道将这个节点添加到图中的操作会起作用。请务必注意，您可能希望重新构造此代码，以分别创建用于添加的节点，以便可以对每个步骤进行错误处理。

如果现在继续构建和执行代码，您将发现它将生成以下内容：

```go
// Output: [12.4 6.4]
```

计算图现在看起来像以下屏幕截图：

![](img/bac6b03b-238f-406a-9adf-57a510a7b559.png)

您可以看到，`W`和`x`都输入第一个运算（我们的乘法运算），然后，它输入我们的加法运算以产生结果。

这是使用 Gorgonia 的入门！正如你现在所希望看到的，这是一个包含必要的原语的库，它将允许我们在接下来的章节中构建第一个简单，然后是更复杂的神经网络。

# Summary

本章简要介绍了 DL 的历史和应用。接下来讨论了为什么 Go 是 DL 的优秀语言，并演示了 Gorgonia 中使用的库与 Go 中其他库的比较。

下一章将介绍使神经网络和 DL 工作的魔力，包括激活函数、网络结构和训练算法。**