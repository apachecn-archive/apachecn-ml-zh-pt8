# 十、扩展部署

现在我们已经介绍了一种管理数据管道的工具，现在是时候完全隐藏起来了。我们的模型最终运行在我们在[第 5 章](05.html)*中提到的硬件上，使用循环神经网络*进行下一个单词预测，通过多层软件进行抽象，直到我们可以使用`go build --tags=cuda`这样的代码。

我们在厚皮动物上构建的图像识别管道是在本地部署的。我们这样做的方式在功能上与将其部署到云资源中的方式相同，但没有深入了解具体情况。这个细节现在将是我们的重点。

在本章结束时，您应该能够执行以下操作：

*   识别并理解云资源，包括特定于我们的平台示例（AWS）的资源
*   了解如何将本地部署迁移到云
*   了解 Docker 和 Kubernetes 是什么以及他们是如何工作的
*   了解计算与成本的权衡

# 在云中失物招领

拥有一台带有 GPU 和 Ubuntu 构建的强大桌面机器对于原型设计和研究来说是非常好的，但是当需要将您的模型投入生产，以及实际进行用例所需的日常预测时，您需要具有高可用性和可伸缩性的计算资源。这到底是什么意思？

假设您以我们的**卷积神经网络**（**CNN**）为例，调整模型并根据您自己的数据进行训练，并创建一个简单的 REST API 前端来调用模型。您希望围绕为客户提供一项服务来建立一项小业务，通过该服务，客户可以支付一些费用，获得一个 API 密钥，并可以向端点提交一个映像，并获得一个声明该映像包含内容的回复。图像识别服务！这听起来好吗？

我们如何确保我们的服务始终可用且快速？毕竟，人们给了你很多钱，即使是一次小的停机或可靠性下降也可能导致你的客户流失给你的竞争对手。传统上，解决方案是购买一批昂贵的*服务器级*硬件，通常是一台机架安装的服务器，具有多个电源和网络接口，以确保硬件故障时的服务连续性。您需要检查各个级别的冗余选项，从磁盘或存储一直到网络，甚至互联网连接。

经验法则是，你需要每样东西中的两样，而这一切都付出了相当大的代价，甚至让人望而却步。如果你是一家资金充足的大型初创公司，你有很多选择，但当然，随着资金曲线的下降，你的选择也随之下降。自托管成为托管托管托管是不可避免的（并非总是如此，但对于大多数小型或初创使用案例而言），这反过来成为存储在其他人数据中心的标准化计算层，以至于您根本不需要关心底层硬件或基础设施。

当然，在现实中，情况并非总是如此。像 AWS 这样的云服务提供商把大部分枯燥、痛苦（但必要）的事情，如硬件更换和一般维护，都排除在外。您不会丢失磁盘或成为故障网线的牺牲品，如果您决定（*嘿，这一切都很好*）每天服务 100000 个客户，那么您可以推动一个简单的基础架构规范更改。无需致电主机提供商、协商停机或前往计算机硬件商店。

This is an incredibly powerful idea; the literal nuts and bolts of your solution—the mix of silicon and gadgetry that your model will use to make predictions—can almost be treated as an afterthought, at least compared to a few short years ago. The skill set, or approach, that is generally required to maintain cloud infrastructure is called **DevOps**. This means that an individual has feet in two (or more!) camps. They understand what all these AWS resources are meant to represent (servers, switches, and load balancers), and how to write the code necessary to specify and manage them.

一个不断发展的角色是*机器学习工程师*。这是传统的 DevOps 技能集，但随着更多的*Ops*端变得自动化或抽象化，个人也可以专注于模型训练或部署，甚至是扩展。让工程师参与整个堆栈是有益的。了解模型的可并行性，了解特定模型可能具有的内存需求，如何构建大规模执行推理所需的分布式基础设施，所有这些都会导致一个模型服务基础设施，其中各种设计元素不是领域专门化的产物，而是一个集成的整体。

# 构建部署模板

We will now put together the various templates required to deploy and train our model at scale. These templates include:

*   **AWS 云形成模板**：虚拟实例及相关资源
*   **Kubernetes 或 KOPS 配置**：K8s 集群管理
*   **Docker 模板或 Makefile**：创建映像以部署到我们的 K8s 集群上

我们在这里选择了一条特殊的道路。AWS 有**弹性容器服务**（**ECS**）和**弹性 Kubernetes 服务**（**EKS**等服务，可通过简单的 API 调用访问。我们在这里的目的是处理细节，以便您能够就如何扩展您自己用例的部署做出明智的选择。现在，您可以更好地控制容器选项和处理的分布方式，以及在将容器部署到普通 EC2 实例时如何调用模型。这些服务也很昂贵，我们将在后面的一节中看到，在做出这些决策时，在成本和性能之间进行权衡。

# 高级步骤

我们的小型 CI/CD 管道包括以下任务：

1.  创建或将培训或推理 Docker 映像推送到 AWS ECS。
2.  在 EC2 实例上创建或部署带有 Kubernetes 集群的 AWS 堆栈，以便我们执行下一步。
3.  训练一个模型或者做一些预测！

现在，我们将依次介绍这些步骤的每个细节。

# 创建或推送 Docker 图像

Docker 无疑是一个吸引了大量炒作的工具。除了人类时尚之外，Docker 的主要原因是简化了依赖关系管理和模型集成等工作，允许可复制、可广泛部署的构建。我们可以预先从操作系统定义我们需要的东西，并将它们全部打包到一个时间点，在这个时间点上我们知道依赖关系是新的，这样我们所有的调整和故障排除就不会白费。

我们需要两样东西来创建我们的形象，并使其达到我们想要的目的：

*   **Dockerfile**：这定义了我们的映像、Linux 版本、要运行的命令以及启动容器时要运行的默认命令
*   **Makefile**：创建镜像并推送到 AWS ECS

让我们首先看看 Dockerfile：

```go
FROM ubuntu:16.04

ARG DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
 curl \
 git \
 pkg-config \
 rsync \
 awscli \
 wget \
 && \
 apt-get clean && \
 rm -rf /var/lib/apt/lists/*

RUN wget -nv https://storage.googleapis.com/golang/go1.12.1.linux-amd64.tar.gz && \
 tar -C /usr/local -xzf go1.12.1.linux-amd64.tar.gz

ENV GOPATH /home/ubuntu/go

ENV GOROOT /usr/local/go

ENV PATH $PATH:$GOROOT/bin

RUN /usr/local/go/bin/go version && \
 echo $GOPATH && \
 echo $GOROOT

RUN git clone https://github.com/PacktPublishing/Hands-On-Deep-Learning-with-Go

RUN go get -v gorgonia.org/gorgonia && \
 go get -v gorgonia.org/tensor && \
 go get -v gorgonia.org/dawson && \
 go get -v github.com/gogo/protobuf/gogoproto && \
 go get -v github.com/golang/protobuf/proto && \
 go get -v github.com/google/flatbuffers/go && \
 go get -v .

WORKDIR /

ADD staging/ /app

WORKDIR /app

CMD ["/bin/sh", "model_wrapper.sh"]
```

我们可以通过查看每行开头的大写声明来辨别一般方法：

1.  使用`FROM`选择基本操作系统映像。
2.  用`ARG`设置开机。
3.  使用`RUN`运行一系列命令，使 Docker 映像进入所需状态。然后`ADD`一个`staging`数据目录，挂载到`/app`。
4.  更改为新的`WORKDIR`。
5.  执行`CMD`命令，我们的容器将运行。

我们现在需要一个 Makefile。这个文件包含的命令将构建我们刚才在 Dockerfile 中定义的映像，并将它们推送到 Amazon 的容器托管服务 ECS。

这是我们的 Makefile：

```go
cpu-image:
 mkdir -p staging/
 cp model_wrapper.sh staging/
 docker build --no-cache -t "ACCOUNTID.dkr.ecr.ap-southeast-2.amazonaws.com/$(MODEL_CONTAINER):$(VERSION_TAG)" .
 rm -rf staging/

cpu-push: cpu-image
 docker push "ACCOUNTID.dkr.ecr.ap-southeast-2.amazonaws.com/$(MODEL_CONTAINER):$(VERSION_TAG)"

```

与我们已经介绍过的其他例子一样，我们使用的是`sp-southeast-2`区域；但是，请随意指定您自己的。您还需要包括您自己的 12 位 AWS 帐户 ID。

从这个目录（到时候，不是现在！）我们现在可以创建和推送 Docker 图像。

# 准备您的 AWS 帐户

您将看到 API 访问 AWS 的通知，以便 KOP 管理您的 EC2 和相关计算资源。与此 API 密钥关联的帐户也需要以下 IAM 权限：

*   AmazonEC2FullAccess
*   AmazonRoute53 完全访问
*   AmazonS3FullAccess
*   AmazonVPCFullAccess

您可以通过进入 AWS 控制台并执行以下步骤来启用编程或 API 访问：

1.  单击 IAM
2.  从左侧菜单中，选择用户，然后选择您的用户
3.  选择安全凭据。然后您将看到访问密钥部分
4.  单击“创建访问密钥”，然后按照说明进行操作

生成的密钥和密钥 ID 将在您的`~/.aws/credentials`文件中使用，或作为外壳变量导出，以便与 KOP 及相关部署和集群管理工具一起使用。

# 创建或部署 Kubernetes 群集

我们的 docker 图像必须运行在某些东西上，那么为什么不收集 Kubernetes 吊舱呢？这就是分布式云计算的神奇之处。在我们的例子中，使用一个中央数据源 AWS S3，许多用于训练或推理的微实例被旋转起来，从而最大限度地提高 AWS 资源利用率，为您节省资金，并为您提供企业级机器学习应用程序所需的稳定性和性能。

首先，导航到这些章节附带的存储库中的`/k8s/`目录。

我们将首先创建部署集群所需的模板。在我们的例子中，我们将为`kubectl`使用一个前端，这是与主 API 交互的默认 Kubernetes 命令。

# 库伯内特斯

让我们看看我们的`k8s_cluster.yaml`文件：

```go
apiVersion: kops/v1alpha2
kind: Cluster
metadata:
  creationTimestamp: 2018-05-01T12:11:24Z
  name: $NAME
spec:
  api:
    loadBalancer:
      type: Public
  authorization:
    rbac: {}
  channel: stable
  cloudProvider: aws
  configBase: $KOPS_STATE_STORE/$NAME
  etcdClusters:
  - etcdMembers:
    - instanceGroup: master-$ZONE
      name: b
    name: main
  - etcdMembers:
    - instanceGroup: master-$ZONE
      name: b
    name: events
  iam:
    allowContainerRegistry: true
    legacy: false
  kubernetesApiAccess:
  - 0.0.0.0/0
  kubernetesVersion: 1.9.3
  masterInternalName: api.internal.$NAME
  masterPublicName: api.hodlgo.$NAME
  networkCIDR: 172.20.0.0/16
  networking:
    kubenet: {}
  nonMasqueradeCIDR: 100.64.0.0/10
  sshAccess:
  - 0.0.0.0/0
  subnets:
  - cidr: 172.20.32.0/19
    name: $ZONE
    type: Public
    zone: $ZONE
  topology:
    dns:
      type: Public
    masters: public
    nodes: public
```

让我们看看我们的`k8s_master.yaml`文件：

```go
apiVersion: kops/v1alpha2
kind: InstanceGroup
metadata:
  creationTimestamp: 2018-05-01T12:11:25Z
  labels:
    kops.k8s.io/cluster: $NAME
  name: master-$ZONE
spec:
  image: kope.io/k8s-1.8-debian-jessie-amd64-hvm-ebs-2018-02-08
  machineType: $MASTERTYPE
  maxSize: 1
  minSize: 1
  nodeLabels:
    kops.k8s.io/instancegroup: master-$ZONE
  role: Master
  subnets:
  - $ZONE
```

让我们看看我们的`k8s_nodes.yaml`文件：

```go
apiVersion: kops/v1alpha2
kind: InstanceGroup
metadata:
  creationTimestamp: 2018-05-01T12:11:25Z
  labels:
    kops.k8s.io/cluster: $NAME
  name: nodes-$ZONE
spec:
  image: kope.io/k8s-1.8-debian-jessie-amd64-hvm-ebs-2018-02-08
  machineType: $SLAVETYPE
  maxSize: $SLAVES
  minSize: $SLAVES
  nodeLabels:
    kops.k8s.io/instancegroup: nodes-$ZONE
  role: Node
  subnets:
  - $ZONE
```

这些模板将被输入 Kubernetes，以加速我们的集群。我们将用于部署集群和相关 AWS 资源的工具是*KOPS*。在编写本文时，此工具的当前版本为 1.12.1，所有部署都已使用此版本进行了测试；早期版本可能存在兼容性问题。

首先，我们需要安装 KOPS。与前面的所有示例一样，这些步骤也适用于 macOS。我们使用自制工具来管理依赖关系，并使安装本地化和正常：

```go
#brew install kops
==> Installing dependencies for kops: kubernetes-cli
==> Installing kops dependency: kubernetes-cli
==> Downloading https://homebrew.bintray.com/bottles/kubernetes-cli-1.14.2.mojave.bottle.tar.gz
==> Downloading from https://akamai.bintray.com/85/858eadf77396e1acd13ddcd2dd0309a5eb0b51d15da275b491
######################################################################## 100.0%
==> Pouring kubernetes-cli-1.14.2.mojave.bottle.tar.gz
==> Installing kops
==> Downloading https://homebrew.bintray.com/bottles/kops-1.12.1.mojave.bottle.tar.gz
==> Downloading from https://akamai.bintray.com/86/862c5f6648646840c75172e2f9f701cb590b04df03c38716b5
######################################################################## 100.0%
==> Pouring kops-1.12.1.mojave.bottle.tar.gz
==> Caveats
Bash completion has been installed to:
 /usr/local/etc/bash_completion.d

zsh completions have been installed to:
 /usr/local/share/zsh/site-functions
==> Summary
 /usr/local/Cellar/kops/1.12.1: 5 files, 139.2MB
==> Caveats
==> kubernetes-cli
Bash completion has been installed to:
 /usr/local/etc/bash_completion.d

zsh completions have been installed to:
 /usr/local/share/zsh/site-functions
==> kops
Bash completion has been installed to:
 /usr/local/etc/bash_completion.d

zsh completions have been installed to:
 /usr/local/share/zsh/site-functions

```

我们可以看到已经安装了 KOPS 和`kubectl`，后者是默认的 K8s 集群管理工具，直接与 API 交互。请注意，Homebrew 经常会发出有关命令完成的警告类型消息，可以安全地忽略这些消息；但是，如果您在配置符号链接时遇到错误，请按照说明解决与`kubectl`任何现有本地安装的冲突。

# 群集管理脚本

我们还需要编写一些脚本来设置环境变量，并根据需要启动或关闭 Kubernetes 集群。在这里，我们将汇集我们编写的模板、KOPS 或`kubectl`以及我们在前面章节中完成的 AWS 配置。

Let's look at our `vars.sh` file:

```go
#!/bin/bash

# AWS vars
export BUCKET_NAME="hodlgo-models"
export MASTERTYPE="m3.medium"
export SLAVETYPE="t2.medium"
export SLAVES="2"
export ZONE="ap-southeast-2b"

# K8s vars
export NAME="hodlgo.k8s.local"
export KOPS_STATE_STORE="s3://hodlgo-cluster"
export PROJECT="hodlgo"
export CLUSTER_NAME=$PROJECT

# Docker vars
export VERSION_TAG="0.1"
export MODEL_CONTAINER="hodlgo-model"
```

我们可以在这里看到，主要变量是容器名称、K8s 集群详细信息，以及我们想要启动的 AWS 资源类型的一系列规范（以及放置它们的区域）。您需要用自己的值替换这些值。

现在我们可以制作一个相应的脚本来取消 shell 中的变量设置，这是部署或管理完 K8s 集群后清理的一个重要部分。

让我们看看我们的`unsetvars.sh`文件：

```go
#!/bin/bash

# Unset them vars

unset BUCKET_NAME
unset MASTERTYPE
unset SLAVETYPE
unset SLAVES
unset ZONE

unset NAME
unset KOPS_STATE_STORE

unset PROJECT
unset CLUSTER_NAME

unset VERSION_TAG
unset MODEL_CONTAINER
```

启动集群的脚本现在将使用这些变量来确定调用集群的内容、它有多少节点以及应该部署在哪里。您将看到，我们使用了一个小技巧，在一行中将环境变量传递到 Kubernetes 模板或 KOP 中；在未来的版本中，这可能不是必需的，但现在它是一个可用的解决方案。

让我们看看我们的`cluster-up.sh`文件：

```go
#!/bin/bash

## Bring up the cluster with kops

set -e

echo "Bringing up Kubernetes cluster"
echo "Using Cluster Name: ${CLUSTER_NAME}"
echo "Number of Nodes: ${SLAVES}"
echo "Using Zone: ${ZONE}"
echo "Bucket name: ${BUCKET_NAME}"

export PARALLELISM="$((4 * ${SLAVES}))"

# Includes ugly workaround because kops is unable to take stdin as input to create -f, unlike kubectl
cat k8s_cluster.yaml | envsubst > k8s_cluster-edit.yaml && kops create -f k8s_cluster-edit.yaml
cat k8s_master.yaml | envsubst > k8s_master-edit.yaml && kops create -f k8s_master-edit.yaml
cat k8s_nodes.yaml | envsubst > k8s_nodes-edit.yaml && kops create -f k8s_nodes-edit.yaml

kops create secret --name $NAME sshpublickey admin -i ~/.ssh/id_rsa.pub
kops update cluster $NAME --yes

echo ""
echo "Cluster $NAME created!"
echo ""

# Cleanup from workaround
rm k8s_cluster-edit.yaml
rm k8s_master-edit.yaml
rm k8s_nodes-edit.yaml
```

相应的`down`脚本将杀死我们的集群，并确保相应地清理所有 AWS 资源。

Let's look at our `cluster-down.sh` file:

```go
#!/bin/bash

## Kill the cluster with kops

set -e

echo "Deleting cluster $NAME"
kops delete cluster $NAME --yes
```

# 构建和推送 Docker 容器

现在，我们已经完成了准备所有模板和脚本的艰苦工作，我们可以开始实际制作 Docker 映像，并在完整集群部署之前将其推送到 ECR。

First, we export the AWS credentials we generated earlier in this chapter:

```go
export AWS_DEFAULT_REGION=ap-southeast-2
export AWS_ACCESS_KEY_ID="<your key here>"
export AWS_SECRET_ACCESS_KEY="<your secret here>"
```

然后我们获得容器存储库登录。这是必要的，以允许我们将创建的 Docker 映像推送到 ECR，而 ECR 将在模型训练或推理时被 Kubernetes 节点拉下来。请注意，此步骤假定已安装 AWS CLI：

```go
aws ecr get-login --no-include-email
```

此命令的输出应类似于以下内容：

```go
docker login -u AWS -p xxxxx https://ACCOUNTID.dkr.ecr.ap-southeast-2.amazonaws.com
```

然后我们可以执行`make cifarcnn-image`和`make cifarcnn-push`这将构建我们在 Dockerfile 中指定的 docker 映像，并将其推送到 AWS 的容器存储服务

# 在 K8s 群集上运行模型

您现在可以编辑我们之前创建的`vars.sh`文件，并使用您最喜欢的命令行文本编辑器设置适当的值。您还需要创建 k8s 存储集群信息的 bucket。

完成此操作后，可以启动 Kubernetes 群集：

```go
source vars.sh
./cluster-up.sh
```

KOPS 现在通过`kubectl`与 Kubernetes 交互，以加速运行集群的 AWS 资源，然后在这些资源上配置 K8s 本身。在继续之前，您需要验证您的群集是否已成功启动：

```go
kops validate cluster
Validating cluster hodlgo.k8s.local

INSTANCE GROUPS
NAME ROLE MACHINETYPE MIN MAX SUBNETS
master-ap-southeast-2a Master c4.large 1 1 ap-southeast-2
nodes Node t2.medium 2 2 ap-southeast-2

NODE STATUS
NAME ROLE READY
ip-172-20-35-114.ec2.internal node True
ip-172-20-49-22.ec2.internal master True
ip-172-20-64-133.ec2.internal node True
```

一旦所有 K8s 主机返回`Ready`，您就可以继续在集群的节点上部署您的模型了！

执行此操作的脚本很简单，并调用`kubectl`以与我们的`cluster_up.sh`脚本相同的方式应用模板。

让我们看看我们的`deploy-model.sh`文件：

```go
#!/bin/bash

# envsubst doesn't exist for OSX. needs to be brew-installed
# via gettext. Should probably warn the user about that.
command -v envsubst >/dev/null 2>&1 || {
  echo >&2 "envsubst is required and not found. Aborting"
  if [[ "$OSTYPE" == "darwin"* ]]; then
    echo >&2 "------------------------------------------------"
    echo >&2 "If you're on OSX, you can install with brew via:"
    echo >&2 " brew install gettext"
    echo >&2 " brew link --force gettext"
  fi
  exit 1;
}

cat ${SCRIPT_DIR}/model.yaml | envsubst | kubectl apply -f -

```

# 总结

现在，我们已经向您介绍了 Kubernetes、Docker 和 AWS 如何在您的钱包允许的范围内为您的模型投入尽可能多的资源的秘密细节，您可以采取一些步骤根据您的用例定制这些示例，或者进一步提高您的知识水平：

*   将此方法集成到 CI 或 CD 工具（Bambol、CircleCI、Puppet 等）中
*   Integrate Pachyderm into your Docker, Kubernetes, or AWS solution
*   使用 parameter server 进行实验，以执行分布式梯度下降等操作，并进一步优化模型管道