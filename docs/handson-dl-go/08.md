# 八、使用变分自编码器的生成模型

在上一章中，我们研究了 DQN 是什么，以及我们可以围绕奖励或行动做出哪些类型的预测。在本章中，我们将探讨如何构建 VAE 以及 VAE 相对于标准自编码器的优势。我们还将研究不同的潜在空间维度对网络的影响。

让我们看看另一个自编码器。在[第 3 章](03.html)*中，我们已经研究过一次自编码器，超越了基本的神经网络——自编码器和 RBM*，其中有一个简单的例子，生成 MNIST 数字。现在我们来看看如何将它用于一个非常不同的任务，即生成新的数字。

本章将介绍以下主题：

*   **变分自编码器**（**VAEs**简介）
*   Building a VAE on MNIST
*   评估结果并改变潜在维度

# VAEs 简介

VAE 在本质上与更基本的自编码器极为相似；它学习如何对输入简化表示的数据进行编码，然后能够基于该编码在另一端重新创建数据。不幸的是，标准的自编码器通常仅限于去噪等任务。使用标准自编码器生成是有问题的，因为标准自编码器中的潜在空间不适合此用途。它们产生的编码可能不是连续的，它们可能聚集在非常特定的部分周围，并且可能难以对其执行插值。

然而，由于我们想要构建一个更具生成性的模型，并且我们不想复制我们放入的相同图像，因此我们需要对输入进行更改。如果我们试图用标准的自编码器来实现这一点，那么最终结果很可能是荒谬的，特别是当输入与训练集相差相当大时。

标准自编码器结构看起来有点像这样：

![](img/f2dddc46-b9a4-446f-a80f-c940504c54b8.png)

我们已经建立了这个标准的自编码器；但是，VAE 的编码方式略有不同，这使其看起来更像下图：

![](img/a3e4423b-cb5c-4eeb-8931-01f9cb94f9b9.png)

A VAE is different from the standard autoencoder; it has a continuous latent space by design, making it easier for us to do random sampling and interpolation. It does this by encoding its data into two vectors: one to store its estimate of means, and another to store its estimate of the standard deviation.

使用这些平均值和标准偏差，我们对编码进行采样，然后将其传递给解码器。然后，解码器对采样编码进行处理以生成结果。因为我们在采样过程中插入了大量的随机噪声，所以每次实际的编码都会略有不同。

通过允许这种变化发生，解码器不限于特定编码；相反，它可以跨潜在空间中更大的区域发挥作用，因为在训练过程中，它不仅会受到数据变化的影响，还会受到编码变化的影响。

为了确保编码在潜在空间上彼此接近，我们在训练期间将一种称为**Kullback-Leibler**（**KL**）发散的度量纳入我们的损失函数。KL 散度度量两个概率函数之间的差异。在这种情况下，通过最小化这种差异，我们可以奖励模型靠近编码，反之亦然，奖励模型试图通过在编码之间创建更多距离来欺骗。

在 VAEs 中，我们根据标准正态分布（均值为 0，标准偏差为 1 的高斯分布）测量 KL 散度。我们可以使用以下公式进行计算：

*klLoss = 0.5 * sum(mean^2 + exp(sd) - (sd + 1))*

Unfortunately, just using KL divergence is insufficient, as all we are doing is ensuring that the encodings are not spread too far apart; we still need to ensure that the encodings are meaningful, and not just mixed with one another. As such, for optimizing a VAE, we also add another loss function to compare the input with the output. This will cause the encodings for similar objects (or, in the case of MNIST, handwritten digits) to cluster closer together. This will enable the decoder to reconstruct the input better and allow us, via manipulation of the input, to produce different results along the continuous axis.

# 在 MNIST 上构建 VAE

熟悉 MNIST 数据集以及普通自编码器的结果，是您未来工作的良好起点。您可能还记得，MNIST 由许多手写数字图像组成，每个图像的大小为 28 x 28 像素。

# 编码

由于这是一个自编码器，因此第一步是构建编码部分，其外观如下所示：

![](img/968aa964-0fea-47d4-a355-033352f268b4.png)

首先，我们有两个完全连接的层：

```go
w0 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(784, 256), gorgonia.WithName("w0"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))

w1 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(256, 128), gorgonia.WithName("w1"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))
```

我们为每层提供一个 ReLU 激活：

```go
// Set first layer to be copy of input
l0 = x
log.Printf("l0 shape %v", l0.Shape())

// Encoding - Part 1
if c1, err = gorgonia.Mul(l0, m.w0); err != nil {
   return errors.Wrap(err, "Layer 1 Convolution failed")
}
if l1, err = gorgonia.Rectify(c1); err != nil {
    return errors.Wrap(err, "Layer 1 activation failed")
}
log.Printf("l1 shape %v", l1.Shape())

if c2, err = gorgonia.Mul(l1, m.w1); err != nil {
    return errors.Wrap(err, "Layer 1 Convolution failed")
}
if l2, err = gorgonia.Rectify(c2); err != nil {
    return errors.Wrap(err, "Layer 1 activation failed")
}
log.Printf("l2 shape %v", l2.Shape())
```

然后，我们将这些连接到平均值和标准偏差层：

```go
estMean := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(128, 8), gorgonia.WithName("estMean"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))

estSd := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(128, 8), gorgonia.WithName("estSd"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))
```

这些层按原样使用，因此不需要特定的激活功能：

```go
if l3, err = gorgonia.Mul(l2, m.estMean); err != nil {
    return errors.Wrap(err, "Layer 3 Multiplication failed")
}
log.Printf("l3 shape %v", l3.Shape())

if l4, err = gorgonia.HadamardProd(m.floatHalf, gorgonia.Must(gorgonia.Mul(l2, m.estSd))); err != nil {
    return errors.Wrap(err, "Layer 4 Multiplication failed")
}
log.Printf("l4 shape %v", l4.Shape())
```

# 取样

现在，VAEs 背后的一个魔法是：采样以创建编码，我们将输入解码器。作为参考，我们正在构建一个类似这样的东西：

![](img/554e13bb-17a8-4630-95d9-76435785ee56.png)

如果您还记得本章前面的内容，我们需要在采样过程中添加一些噪声，我们将此噪声称为`epsilon`。这将反馈到我们的采样编码中；在 Gorgonia 中，我们可以使用平均值为`0`的`GaussianRandomNode`和标准偏差为`1`作为输入参数来实现这一点：

```go
epsilon := gorgonia.GaussianRandomNode(g, dt, 0, 1, 100, 8)
```

然后，我们将其输入到公式中，以创建采样编码：

```go
if sz, err = gorgonia.Add(l3, gorgonia.Must(gorgonia.HadamardProd(gorgonia.Must(gorgonia.Exp(l4)), m.epsilon))); err != nil {
    return errors.Wrap(err, "Layer Sampling failed")
}
log.Printf("sz shape %v", sz.Shape())
```

前面的代码可能很难阅读。简单来说，我们正在做的是：

```go
sampled = mean + exp(sd) * epsilon
```

这为我们提供了一个使用均值和标准偏差向量加上噪声分量的采样编码。这确保了每次的结果并不完全相同。

# 解码

在获得采样编码后，我们将其馈送到解码器，解码器的结构与编码器基本相同，但相反。这种安排看起来有点像这样：

![](img/65f816b1-012b-47b4-ac5f-bba260f295ac.png)

Gorgonia 中的实际实现如下所示：

```go
// Decoding - Part 3
if c5, err = gorgonia.Mul(sz, m.w5); err != nil {
    return errors.Wrap(err, "Layer 5 Convolution failed")
}
if l5, err = gorgonia.Rectify(c5); err != nil {
    return errors.Wrap(err, "Layer 5 activation failed")
}
log.Printf("l6 shape %v", l1.Shape())

if c6, err = gorgonia.Mul(l5, m.w6); err != nil {
    return errors.Wrap(err, "Layer 6 Convolution failed")
}
if l6, err = gorgonia.Rectify(c6); err != nil {
    return errors.Wrap(err, "Layer 6 activation failed")
}
log.Printf("l6 shape %v", l6.Shape())

if c7, err = gorgonia.Mul(l6, m.w7); err != nil {
    return errors.Wrap(err, "Layer 7 Convolution failed")
}
if l7, err = gorgonia.Sigmoid(c7); err != nil {
    return errors.Wrap(err, "Layer 7 activation failed")
}
log.Printf("l7 shape %v", l7.Shape())
```

我们将`Sigmoid`激活放在最后一层，因为我们希望输出比 ReLU 通常提供的更连续。

# 损失或成本函数

如本章第一部分所述，我们针对两种不同的损失源进行了优化。

我们优化的第一个损失是输入图像和输出图像之间的实际差异；如果差异最小，这对我们来说是理想的。为此，我们公开输出层，然后计算输入的差异。对于这个例子，我们使用的是输入和输出之间的平方误差之和，没有什么特别之处。在伪代码中，如下所示：

```go
valueLoss = sum(squared(input - output))
```

在 Gorgonia 中，我们可以按如下方式实施：

```go
m.out = l7
valueLoss, err := gorgonia.Sum(gorgonia.Must(gorgonia.Square(gorgonia.Must(gorgonia.Sub(y, m.out)))))
if err != nil {
    log.Fatal(err)
}
```

我们的另一个损失分量是 KL 发散度量，其伪代码如下所示：

```go
klLoss = sum(mean^2 + exp(sd) - (sd + 1)) / 2
```

我们在 Gorgonia 中的实现更加详细，大量使用了`Must`：

```go
valueOne := gorgonia.NewScalar(g, dt, gorgonia.WithName("valueOne"))
valueTwo := gorgonia.NewScalar(g, dt, gorgonia.WithName("valueTwo"))
gorgonia.Let(valueOne, 1.0)
gorgonia.Let(valueTwo, 2.0)

ioutil.WriteFile("simple_graph_2.dot", []byte(g.ToDot()), 0644)
klLoss, err := gorgonia.Div(
    gorgonia.Must(gorgonia.Sum(
        gorgonia.Must(gorgonia.Sub(
            gorgonia.Must(gorgonia.Add(
                gorgonia.Must(gorgonia.Square(m.outMean)),
                gorgonia.Must(gorgonia.Exp(m.outVar)))),
            gorgonia.Must(gorgonia.Add(m.outVar, valueOne)))))),
    valueTwo)
if err != nil {
    log.Fatal(err)
}

```

现在，剩下的就是一点家务活，把所有的东西都绑在一起。我们将使用 Adam 的`solver`进行此示例：

```go
func (m *nn) learnables() gorgonia.Nodes {
    return gorgonia.Nodes{m.w0, m.w1, m.w5, m.w6, m.w7, m.estMean, m.estSd}
}

vm := gorgonia.NewTapeMachine(g, gorgonia.BindDualValues(m.learnables()...))
solver := gorgonia.NewAdamSolver(gorgonia.WithBatchSize(float64(bs)), gorgonia.WithLearnRate(0.01))
```

现在让我们评估一下结果。

# 评估结果

You'll notice that the results of our VAE model are a fair bit fuzzier than our standard autoencoder:

![](img/6597464d-58b8-4720-8092-27a5015cac42.png)

在某些情况下，它在几个不同的数字之间似乎也不确定，如在下面的示例中，它似乎接近于解码为 7 而不是 9：

![](img/042e54bc-4e38-429e-b706-8a98276d0119.png)

这是因为我们已经明确地要求分布彼此接近。如果我们试图在二维图上形象化这一点，它将看起来有点像以下内容：

![](img/1db7260f-8158-4d64-8cef-518d1d2f8779.png)

您可以从最后一个示例中看到，它可以生成每个手写数字的几个不同变体，并且在不同数字之间的某些区域中，它似乎在几个不同的数字之间变形。

# 改变潜在维度

MNIST 上的 VAE 在经历了足够多的时期后，通常在两个维度上表现得相当不错，但要确定这一点，最好的方法是测试该假设并尝试其他一些尺寸。

对于本书中描述的实现，这是一个相当快的变化：

```go
w0 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(784, 256), gorgonia.WithName("w0"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))
w1 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(256, 128), gorgonia.WithName("w1"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))

w5 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(8, 128), gorgonia.WithName("w5"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))
w6 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(128, 256), gorgonia.WithName("w6"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))
w7 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(256, 784), gorgonia.WithName("w7"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))

estMean := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(128, 8), gorgonia.WithName("estMean"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))
estSd := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(128, 8), gorgonia.WithName("estSd"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))

floatHalf := gorgonia.NewScalar(g, dt, gorgonia.WithName("floatHalf"))
gorgonia.Let(floatHalf, 0.5)

epsilon := gorgonia.GaussianRandomNode(g, dt, 0, 1, 100, 8)
```

这里的基本实现是八个维度；要使其在二维上工作，我们所要做的就是将`8`的所有实例更改为`2`，结果如下：

```go
w0 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(784, 256), gorgonia.WithName("w0"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))
w1 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(256, 128), gorgonia.WithName("w1"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))

w5 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(2, 128), gorgonia.WithName("w5"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))
w6 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(128, 256), gorgonia.WithName("w6"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))
w7 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(256, 784), gorgonia.WithName("w7"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))

estMean := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(128, 2), gorgonia.WithName("estMean"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))
estSd := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(128, 2), gorgonia.WithName("estSd"), gorgonia.WithInit(gorgonia.GlorotU(1.0)))

floatHalf := gorgonia.NewScalar(g, dt, gorgonia.WithName("floatHalf"))
gorgonia.Let(floatHalf, 0.5)

epsilon := gorgonia.GaussianRandomNode(g, dt, 0, 1, 100, 2)
```

现在我们所要做的就是重新编译代码，然后运行它，这样我们就可以看到当我们尝试使用更多维度的潜在空间时会发生什么。

正如我们所看到的，很明显二维空间处于不利地位，但当我们向上移动时，情况就不那么清楚了。您可以看到，平均而言，20 维生成的结果明显更清晰，但实际上，5 维版本的模型对于大多数用途来说已经足够了：

![](img/ba71c845-ea42-4efc-b9c6-3b3ef61f3b43.png)

# 总结

现在，您已经了解了如何构建 VAE 以及使用 VAE 优于标准自编码器的优点。您还了解了不同的潜在空间维度对网络的影响。

作为练习，您应该尝试将此模型训练为在 CIFAR-10 数据集上工作，并使用卷积层而不是简单的完全连接层。

在下一章中，我们将了解什么是数据管道，以及为什么我们使用 Pachydrm 来构建或管理它们。

# 进一步阅读

*   *自编码变分 Bayes、**Diederik P.Kingma*和*Max Wlling*
*   *关于**变分自编码器的教程，**卡尔·多尔什*
*   *厄尔博手术：另一种划分变化证据下限的方法，**马修·D·霍夫曼*和*马修·J·约翰逊*
*   *潜在对齐和变化注意；**邓云田*、*金妍*、*邱斯廷*、*郭德美*、*亚历山大·M·拉什*