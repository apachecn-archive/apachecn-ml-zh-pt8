# 四、CUDA——GPU 加速训练

本章将介绍深度学习的硬件方面。首先，我们将了解 CPU 和 GPU 如何满足我们构建**深层神经网络**（**DNN**）的计算需求，它们之间的区别以及它们的优势。GPU 提供的性能改进对于深度学习的成功至关重要。

我们将学习如何让 Gorgonia 与我们的 GPU 一起工作，以及如何使用**CUDA**：NVIDIA 的软件库加速 Gorgonia 模型，以方便 GPU 加速深度学习模型的构建和执行。我们还将学习如何在 Gorgonia 中构建一个使用 GPU 加速操作的模型，然后将这些模型的性能与 CPU 的性能进行对比，以确定哪一个是不同任务的最佳选择。

本章将介绍以下主题：

*   CPU 与 GPU
*   了解 Gorgonia 和 CUDA
*   使用 CUDA 在 Gorgonia 中构建模型
*   用于训练和推理的 CPU 与 GPU 模型的性能基准测试

# CPU 与 GPU

在这一点上，我们已经涵盖了神经网络的许多基本理论和实践，但是我们没有考虑到运行它们的处理器。因此，让我们从编码中休息一下，然后深入了解正在实际工作的硅的小切片。

30000 英尺的视图显示，CPU 最初设计用于支持按顺序执行的标量操作，而 GPU 设计用于并行执行的向量操作。神经网络在一个层内执行大量独立的计算（例如，每个神经元乘以其权重），因此它们是一种处理工作量，适合于支持大规模并行的芯片设计。

让我们通过一个利用每个操作的性能特征的操作类型的示例，使这一点更具体一些。取[1,2,3]和[4,5,6]的两行向量。如果我们对这些元素执行元素矩阵乘法，它将如下所示：

```go
CPU, 2ns per operation (higher per-core clock than GPU, fewer cores):

1 * 4
2 * 5
3 * 6
     = [4, 10, 18]

Time taken: 6ns

GPU, 4ns per operation (lower per-core clock than CPU, more cores):

1 * 4 | 2 * 5 | 3 *6
     = [4, 10, 18]

Time taken: 4ns
```

如您所见，CPU 按顺序执行计算，而 GPU 并行执行。这导致 GPU 比 CPU 花费更少的时间来完成计算。这是我们关心的与 DNN 相关的工作负载的两种处理器之间的根本区别。

# 计算工作量与芯片设计

从处理器本身的实际设计来看，这种差异是如何体现的？此图取自 NVIDIA 自己的 CUDA 文档，说明了这些差异：

![](img/0bdb4464-70b6-4df5-a217-e1d027a3a3c9.png)

控制或缓存单元减少，而核心或 ALU 的数量显著增加。这将导致性能提高一个数量级（或更多）。需要注意的是，GPU 的效率在内存、计算和功耗方面远远不够完美。这就是为什么许多公司竞相为 DNN 工作负载从头开始设计处理器，优化缓存单元/ALU 的比率，并改进将数据拉入内存然后送入计算单元的方式。目前，内存是 GPU 的瓶颈，如下图所示：

![](img/bfcece11-f4c4-4dc6-9a86-012ddebb635f.png)

ALU 只有在有工作的情况下才能工作。如果我们耗尽了片上内存，我们必须进入二级缓存，这在 GPU 中比在 CPU 中更快，但仍然需要比片上一级内存更长的时间来访问。我们将在后面的一章中结合新的和竞争性的芯片设计讨论这些缺点。目前，需要了解的重要一点是，理想情况下，我们希望有尽可能多的 ALU，尽可能多的片上缓存，以适当的比例填充到芯片中，并在处理器和内存之间进行快速通信。对于这个过程，CPU 确实可以工作，但 GPU 要好得多。目前，它们是最适合机器学习的硬件，消费者可以广泛使用。

# gpu 中的内存访问

到目前为止，您应该很清楚，在进行深度学习时，快速和本地内存是我们卸载到处理器上的各种工作负载性能的关键。然而，重要的不仅仅是内存的数量和接近程度，还有如何访问内存。考虑硬盘上的顺序访问和随机访问性能，因为原理是相同的。

为什么这对 DNN 很重要？简言之，它们是高维结构，最终必须嵌入到 1D 空间中，用于为我们的 ALU 提供内存。为图形工作负载而构建的现代（矢量）GPU 假设它们将访问相邻内存，即 3D 场景的一部分将存储在相关部分（帧中的相邻像素）旁边。因此，它们针对这一假设进行了优化。我们的网络不是 3D 场景。他们的数据布局是稀疏的，并且依赖于网络（以及图形）结构和他们所持有的信息。

下图显示了这些不同工作负载的内存访问模式：

![](img/b7d762a5-eb4d-4f84-bb1b-6ea46e2e761e.png)

对于 DNN，我们希望在编写操作时尽可能接近**跨步**内存访问模式。毕竟，矩阵乘法恰好是 DNN 中更常见的操作之一。

# Real-world performance

为了了解真实世界的性能差异，让我们比较一下最适合神经网络工作负载的 CPU 之一 Intel Xeon Phi 和 2015 年的 NVIDIA Maxwell GPU。

# Intel Xeon Phi CPU

以下是一些硬性能数据：

*   该芯片的计算单元能够达到 2400 千兆次/秒，从 DRAM 中提取 88 千兆字/秒，比率为 27/1
*   这意味着从内存中提取的每个字有 27 个浮点操作

# NVIDIA Maxwell GPU

现在，这里是参考 NVIDIA GPU 的数字。特别注意比率的变化：

*   6100 千兆次/秒
*   84 千兆字节/秒
*   比率为 72/1

因此，就每个内存块的原始操作而言，GPU 具有明显的优势。

A full detour into microprocessor design is of course outside the scope of this book, but it is useful to think about the processor's distribution of memory and compute units. The design philosophy for modern chips can be summed up as *cram as many floating-point units onto the chip as possible to achieve the m**aximum computation relative to the power required/heat generated*.

这样做的目的是让这些 ALU 尽可能地满，从而最大限度地减少它们在内存被填满时处于空闲状态的时间。

# 了解 Gorgonia 和 CUDA

在我们进入 Gorgonia 如何与 CUDA 合作之前，让我们快速向您介绍 CUDA 以及它是什么。

# 库达

CUDA 是 NVIDIA 的 GPU 编程语言。这意味着您的 AMD 卡不支持 CUDA。在不断增长的深度学习图书馆、语言和工具领域，这是一个事实上的标准。C 实现是免费提供的，但当然，它只与 NVIDIA 自己的硬件兼容。

# Basic Linear Algebra Subprograms

正如我们在迄今为止构建的网络中所看到的，张量运算是机器学习的基础。GPU 设计用于这些类型的向量或矩阵运算，但我们的软件也需要设计为利用这些优化。进入**BLAS**！

BLAS provide the building blocks for linear algebra operations, commonly used in graphics programming as well as machine learning. BLAS libraries are low level, originally written in Fortran, and group the functionality they offer into three *levels*, defined by the types of operations covered, as follows:

*   **级别 1**：跨步阵列上的向量运算、点积、向量范数和广义向量加法
*   **二级**：广义矩阵向量乘法，涉及三角矩阵的线性方程组求解器
*   **三级**：矩阵运算，包括**一般矩阵乘法**（**GEMM**）

Level 3 operations are what we're really interested in for deep learning. Here's an example from the CUDA-fied convolution operation in Gorgonia.

# 戈尔戈尼亚的库达

Gorgonia 已经实施了对 NVIDIA 的 CUDA 的支持，作为其`cu`包的一部分。它抽象出了几乎所有的复杂性，因此我们所要做的只是在构建时指定`--tags=cuda`标志，并确保我们调用的操作实际上存在于 Gorgonia API 中。

当然，并非所有可能的操作都得到了实施。重点放在从并行执行中受益的操作上，这些操作适合 GPU 加速。正如我们将在[第 5 章](05.html)、*使用循环神经网络进行下一个单词预测*中所述，**卷积神经网络**（**CNN**中涉及的许多操作都符合这一标准。

那么，有什么可用的？以下列表概述了这些选项：

*   一维或二维卷积（用于 CNN）
*   2D 最大池（也用于 CNN！）
*   Dropout (kill some neurons!)
*   ReLU（在[第 2 章](02.html)、*什么是神经网络以及如何训练神经网络？*中的回忆激活函数）
*   批量标准化

现在，我们将依次研究每种方法的实施情况。

查看`gorgonia/ops/nn/api_cuda.go`，我们看到 2D 卷积的函数如下：

```go
func Conv2d(im, filter *G.Node, kernelShape tensor.Shape, pad, stride, dilation []int) (retVal *G.Node, err error) {
    var op *convolution
    if op, err = makeConvolutionOp(im, filter, kernelShape, pad, stride, dilation); err != nil {
        return nil, err
    }
    return G.ApplyOp(op, im, filter)
}
```

下面的 1D 卷积函数返回`Conv2d()`的一个实例，这是一种为我们提供两个选项的简洁方式：

```go
func Conv1d(in, filter *G.Node, kernel, pad, stride, dilation int) (*G.Node, error) {
    return Conv2d(in, filter, tensor.Shape{1, kernel}, []int{0, pad}, []int{1, stride}, []int{1, dilation})
}
```

接下来是`MaxPool2D()`函数。在 CNN 中，最大池层是特征提取过程的一部分。在传递到后续的卷积层之前，输入的维数被降低。

在这里，我们创建一个携带我们的`XY`参数的`MaxPool`实例，并返回在输入节点上运行`ApplyOp()`的结果，如下代码所示：

```go
func MaxPool2D(x *G.Node, kernel tensor.Shape, pad, stride []int) (retVal *G.Node, err error) {
    var op *maxpool
    if op, err = newMaxPoolOp(x, kernel, pad, stride); err != nil {
        return nil, err
    }
    return G.ApplyOp(op, x)
}
```

`Dropout()`是一种正则化技术，用于防止网络过度拟合。我们希望尽可能学习输入数据的最通用表示形式，辍学帮助我们做到这一点。

`Dropout()`的结构现在应该已经很熟悉了。这是另一个可以在层内并行的操作，如下所示：

```go
func Dropout(x *G.Node, prob float64) (retVal *G.Node, err error) {
    var op *dropout
    if op, err = newDropout(x, prob); err != nil {
        return nil, err
    }

    // states := &scratchOp{x.Shape().Clone(), x.Dtype(), ""}
    // m := G.NewUniqueNode(G.WithType(x.Type()), G.WithOp(states), G.In(x.Graph()), G.WithShape(states.shape...))

    retVal, err = G.ApplyOp(op, x)
    return
}
```

我们在[第 2 章](02.html)、*中介绍的标准 ReLU 函数什么是神经网络？如何训练神经网络？*，也可使用，如图所示：

```go
func Rectify(x *G.Node) (retVal *G.Node, err error) {
 var op *activation
 if op, err = newRelu(); err != nil {
 return nil, err
 }
 retVal, err = G.ApplyOp(op, x)
 return
}
```

`BatchNorm()` is slightly more complicated. Looking back at the original paper that described batch normalization, by Szegedy and Ioffe (2015), we see how, for a given batch, we normalize the output of the previous layer by subtracting the mean of the batch and dividing by the standard deviation. We can also observe the addition of two parameters that we will train with SGD.

现在，我们可以看到 CUDA 的 Gorgonia 实现，如下所示。首先，让我们执行函数定义和数据类型检查：

```go
func BatchNorm(x, scale, bias *G.Node, momentum, epsilon float64) (retVal, γ, β *G.Node, op *BatchNormOp, err error) {
    dt, err := dtypeOf(x.Type())
    if err != nil {
        return nil, nil, nil, nil, err
    }
```

然后，它需要创建一些临时变量，以允许 VM 分配备用内存：

```go
channels := x.Shape()[1]
H, W := x.Shape()[2], x.Shape()[3]
scratchShape := tensor.Shape{1, channels, H, W}

meanScratch := &gpuScratchOp{scratchOp{x.Shape().Clone(), dt, "mean"}}
varianceScratch := &gpuScratchOp{scratchOp{x.Shape().Clone(), dt, "variance"}}
cacheMeanScratch := &gpuScratchOp{scratchOp{scratchShape, dt, "cacheMean"}}
cacheVarianceScratch := &gpuScratchOp{scratchOp{scratchShape, dt, "cacheVariance"}}
```

然后，我们在计算图中创建等效变量：

```go
g := x.Graph()

dims := len(x.Shape())

mean := G.NewTensor(g, dt, dims, G.WithShape(scratchShape.Clone()...), G.WithName(x.Name()+"_mean"), G.WithOp(meanScratch))

variance := G.NewTensor(g, dt, dims, G.WithShape(scratchShape.Clone()...), G.WithName(x.Name()+"_variance"), G.WithOp(varianceScratch))

cacheMean := G.NewTensor(g, dt, dims, G.WithShape(scratchShape.Clone()...),      G.WithOp(cacheMeanScratch))

cacheVariance := G.NewTensor(g, dt, dims, G.WithShape(scratchShape.Clone()...), G.WithOp(cacheVarianceScratch))
```

然后，在应用函数并返回结果之前，我们在图中创建比例和偏差变量：

```go
if scale == nil {
    scale = G.NewTensor(g, dt, dims, G.WithShape(scratchShape.Clone()...), G.WithName(x.Name()+"_γ"), G.WithInit(G.GlorotN(1.0)))
}

if bias == nil {
    bias = G.NewTensor(g, dt, dims, G.WithShape(scratchShape.Clone()...), G.WithName(x.Name()+"_β"), G.WithInit(G.GlorotN(1.0)))
}

op = newBatchNormOp(momentum, epsilon)

retVal, err = G.ApplyOp(op, x, scale, bias, mean, variance, cacheMean, cacheVariance)

return retVal, scale, bias, op, err
```

接下来，让我们来看看如何在 Gorgonia 构建一个利用 CUDA 的模型。

# 使用 CUDA 支持在 Gorgonia 中构建模型

在 Gorgonia 中使用 CUDA 支持构建模型，我们首先要做一些事情。我们需要将 Gorgonia 的`cu`接口安装到 CUDA，然后准备好一个模型进行训练！

# 为 Gorgonia 安装 CUDA 支持

要使用 CUDA，您需要一台带有 NVIDIA 制造的 GPU 的计算机。不幸的是，将 CUDA 设置为与 Gorgonia 一起使用是一个稍微复杂一些的过程，因为它需要设置一个与 Go 一起使用的 C 编译器环境，以及一个与 CUDA 一起使用的 C 编译器环境。NVIDIA 已善意地确保其编译器与每个平台的通用工具链协同工作：Windows 上的 Visual Studio、macOS 上的 Clang LLVM 和 Linux 上的 GCC。

安装 CUDA 并确保一切正常运行需要相当多的工作。我们将研究如何在 Windows 和 Linux 上实现这一点。由于苹果已经有好几年没有生产出具有 NVIDIA GPU 的电脑了（在撰写本文时），我们将不介绍如何在 macOS 上实现这一点。您仍然可以通过将外部 GPU 连接到 macOS 来使用 CUDA，但这是一个相当复杂的过程，苹果公司（在撰写本文时）没有官方支持的 NVIDIA GPU 设置。

# Linux

正如我们所讨论的，一旦 CUDA 被很好地设置好，在 GPU 上运行 Gorgonia 代码就像在构建它时添加`-tags=cuda`一样简单。但是，我们如何才能做到这一点呢？让我们看看。

本指南要求您安装标准的 Ubuntu 18.04。NVIDIA 在[提供独立于发行版的说明（和故障排除步骤）https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) 。

在较高级别上，您需要安装以下软件包：

*   英伟达司机
*   库达
*   cuDNN
*   libcupti 开发公司

首先，您需要确保安装了 NVIDIA 的专有（非开源默认）驱动程序。检查您是否正在运行它的一种快速方法是执行`nvidia-smi`。您应该会看到类似于以下内容的输出，其中指示了驱动程序版本号和有关 GPU 的其他详细信息：

![](img/34e90c45-3eef-4f47-9c8c-9ffed01c4adf.png)

如果您获得了`command not found`，那么根据您正在运行的 Linux 发行版，您有两个选择。最新的 Ubuntu 发行版允许您从默认存储库安装 CUDA 的大部分依赖项（包括专有的 NVIDIA 驱动程序）。这可以通过执行以下操作来完成：

```go
sudo apt install nvidia-390 nvidia-cuda-toolkit libcupti-dev
```

或者，您可以按照官方 NVIDIA 指南（前面链接）中的步骤手动安装各种依赖项。

Once the installation has completed and you have rebooted your system, confirm that the drivers are installed by running `nvidia-smi` again. You also need to verify that the CUDA C compiler (part of the `nvidia-cuda-toolkit` package) is installed by executing `nvcc --version`. The output should look similar to the following:

![](img/f3df211d-9e0b-4cfd-aac2-f5b616ae25b5.png)

一旦 CUDA 本身安装完毕，您还需要执行一些额外的步骤，以确保 Gorgonia 已经编译了必要的 CUDA 库并可供使用：

1.  确保正在生成的模块的目标目录存在。如果不是，请使用以下命令创建它：

```go
mkdir $GOPATH/src/gorgonia.org/gorgonia/cuda\ modules/target
```

2.  运行`cudagen`构建模块如下：

```go
cd $GOPATH/src/gorgonia.org/gorgonia/cmd/cudagen
go run main.go
```

3.  程序执行后，验证`/target`目录中是否填充了表示 CUDA-fied 操作的文件，我们将在构建网络时使用这些文件，如以下屏幕截图所示：

![](img/3c52c837-75d7-4f5b-a77e-993d56bb3dcf.png)

4.  现在，准备工作已经结束，让我们使用以下命令测试一切是否正常：

```go
go install gorgonia.org/cu/cmd/cudatest cudatest
cd $GOPATH/src/gorgonia.org/cu/cmd/cudatest
go run main.go
```

您应该看到类似于以下内容的输出：

![](img/0e17b690-3ae4-41f0-a06d-453389811d1b.png)

现在，您可以充分利用 GPU 提供的所有计算能力了！

# 窗户

Windows 的设置非常类似，但您还需要提供 Go 和 CUDA 所需的 C 编译器。以下步骤概述了此设置：

1.  安装 GCC 环境；在 Windows 上执行此操作的最简单方法是安装 MSYS2。您可以从[下载 MSYS2https://www.msys2.org/](https://www.msys2.org/) 。
2.  安装 MSYS2 后，使用以下命令更新安装：

```go
pacman -Syu
```

3.  重新启动 MSYS2 并再次运行以下操作：

```go
pacman -Su
```

4.  按如下方式安装 GCC 包：

```go
pacman -S mingw-w64-x86_64-toolchain
```

5.  安装 Visual Studio 2017 以获得与 CUDA 兼容的编译器。在撰写本文时，您可以从[下载此内容 https://visualstudio.microsoft.com/downloads/](https://visualstudio.microsoft.com/downloads/) 。社区版运行良好；如果您有任何其他版本的许可证，它们也可以。
6.  安装 CUDA。从 Nvidia 网站下载：https://developer.nvidia.com/cuda-downloads 。根据我的经验，网络安装程序不如本地安装程序可靠，因此如果无法让网络安装程序正常工作，请尝试本地安装程序。
7.  接下来，您还应该从 NVIDIA 安装 cuDNN:[https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn) 。安装过程实际上是一个复制粘贴操作，非常简单。
8.  设置环境变量，以便 GO 和 NVIDIA CUDA 编译器驱动程序（AutoT0.）知道在哪里可以找到相关编译器。在适当的情况下，应该将路径替换为 CUDA、MSYS2 和 Visual Studio 的安装位置。需要添加的项目和相关变量名称如下：

```go
C_INCLUDE_PATH
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\include

LIBRARY_PATH
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\lib\x64

PATH
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\bin
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\libnvvp
C:\msys64\mingw64\bin
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\x86_amd64
```

9.  现在应该正确设置您的环境，以编译支持 CUDA 的 Go 二进制文件。

Now, for Gorgonia, you need to do a few things first, as outlined in the following steps:

1.  首先，确保您将要构建的模块存在以下`target`目录：

```go
$GOPATH/src/gorgonia.org/gorgonia/cuda\ modules/target
```

2.  接下来，运行`cudagen`构建如下模块：

```go
cd $GOPATH/src/gorgonia.org/gorgonia/cmd/cudagen
go run main.go
```

3.  现在您已经准备好了所有东西，您应该安装`cudatest`，如下所示：

```go
go install gorgonia.org/cu/cmd/cudatest cudatest
```

4.  如果您现在运行`cudatest`并且一切正常，您将获得类似于以下输出的结果：

```go
CUDA version: 9020
CUDA devices: 1
Device 0
========
Name : "GeForce GTX 1080"
Clock Rate: 1835000 kHz
Memory : 8589934592 bytes
Compute : 6.1
```

# 用于训练和推理的 CPU 与 GPU 模型的性能基准测试

Now that we've done all that work, let's explore some of the advantages of using a GPU for deep learning. First, let's go through how to actually get your application to use CUDA, and then we'll go through some of the CPU and GPU speeds.

# 如何使用 CUDA

If you've completed all the previous steps to get CUDA working, then using CUDA is a fairly simple affair. You simply need to compile your application with the following:

```go
go build -tags='cuda'
```

这将使用 CUDA 支持构建可执行文件，并使用 CUDA 而不是 CPU 来运行深度学习模型。

为了说明这一点，让我们使用一个我们已经熟悉的示例—一个具有权重的神经网络：

```go
w0 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(784, 300), gorgonia.WithName("w0"), gorgonia.WithInit(gorgonia.GlorotN(1.0)))

w1 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(300, 100), gorgonia.WithName("w1"), gorgonia.WithInit(gorgonia.GlorotN(1.0)))

w2 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(100, 10), gorgonia.WithName("w2"), gorgonia.WithInit(gorgonia.GlorotN(1.0)))
```

这只是我们在 MNIST 数据集上构建的简单前馈神经网络。

# CPU 结果

通过运行代码，我们得到的输出告诉我们何时开始每个历元，以及上一次执行的大致成本函数值。对于这个特定任务，我们只运行了 10 个阶段，结果如下所示：

```go
2018/07/21 23:48:45 Batches 600
2018/07/21 23:49:12 Epoch 0 | cost -0.6898460176511779
2018/07/21 23:49:38 Epoch 1 | cost -0.6901109698353116
2018/07/21 23:50:05 Epoch 2 | cost -0.6901978951202982
2018/07/21 23:50:32 Epoch 3 | cost -0.6902410983814113
2018/07/21 23:50:58 Epoch 4 | cost -0.6902669350941992
2018/07/21 23:51:25 Epoch 5 | cost -0.6902841232197489
2018/07/21 23:51:52 Epoch 6 | cost -0.6902963825164774
2018/07/21 23:52:19 Epoch 7 | cost -0.6903055672849466
2018/07/21 23:52:46 Epoch 8 | cost -0.6903127053988457
2018/07/21 23:53:13 Epoch 9 | cost -0.690318412509433
2018/07/21 23:53:13 Run Tests
2018/07/21 23:53:19 Epoch Test | cost -0.6887220522190024
```

我们可以看到，在这台英特尔核心 i7-2700K 的 CPU 上，每一个历元大约需要 26-27 秒。

# GPU 结果

对于可执行文件的 GPU 构建，我们也可以这样做。这使我们能够比较一个历元在模型中训练所需的时间。由于我们的模型并不复杂，因此我们不希望看到太大的差异：

```go
2018/07/21 23:54:31 Using CUDA build
2018/07/21 23:54:32 Batches 600
2018/07/21 23:54:56 Epoch 0 | cost -0.6914807096357707
2018/07/21 23:55:19 Epoch 1 | cost -0.6917470871356043
2018/07/21 23:55:42 Epoch 2 | cost -0.6918343739257966
2018/07/21 23:56:05 Epoch 3 | cost -0.6918777292080605
2018/07/21 23:56:29 Epoch 4 | cost -0.6919036464362168
2018/07/21 23:56:52 Epoch 5 | cost -0.69192088335746
2018/07/21 23:57:15 Epoch 6 | cost -0.6919331749749763
2018/07/21 23:57:39 Epoch 7 | cost -0.691942382545885
2018/07/21 23:58:02 Epoch 8 | cost -0.6919495375223687
2018/07/21 23:58:26 Epoch 9 | cost -0.691955257565567
2018/07/21 23:58:26 Run Tests
2018/07/21 23:58:32 Epoch Test | cost -0.6896057773382677
```

在这个 GPU（NVIDIA Geforce GTX960）上，我们可以看到，对于这个简单的任务，这稍微快一点，时间为 23–24 秒。

# 总结

In this chapter, we had a look at the hardware side of deep learning. We also had a look at how CPUs and GPUs serve our computational needs. We also looked at how CUDA, NVIDIA's software,  facilitates GPU-accelerated deep learning that is implemented in Gorgonia, and finally, we looked at how to build a model that uses the features implemented by CUDA Gorgonia. 

在下一章中，我们将研究普通 RNN 以及 RNN 涉及的问题。我们还将学习如何在 Gorgonia 中构建 LSTM。