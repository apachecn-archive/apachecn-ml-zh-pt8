# 十、自然语言处理实战

自然语言处理是解析、分析和重构自然语言的科学(和艺术)，如书面或口头英语、法语或德语。这不是一项容易的任务；**自然语言处理** ( **NLP** )是一个完整的研究领域，拥有充满活力的学术研究社区和来自主要科技公司的大量资金支持。每当谷歌、苹果、亚马逊和微软等公司投资其谷歌助手、Siri、Alexa 和 Cortana 产品时，NLP 领域都会获得更多一点的资金。简而言之，自然语言处理就是为什么你可以和你的手机通话，你的手机也可以和你通话。

Siri 远不止是 NLP。我们作为消费者，当我们的**人工智能** ( **AI** )助手出了可笑的问题时，喜欢批评他们。但是他们真的是工程学的奇迹，事实上他们做对了任何事情都是一个奇迹！

如果我看着手机说:*好的谷歌，给我指路到 7-Eleven* ，我的手机会自动唤醒并回复我，*好的，去主大街 7-Eleven，右转*。让我们想一想需要做些什么:

*   我的睡眠手机正在监控我预先训练好的谷歌标语。
*   音频缓冲区在我训练的 OK 谷歌声波上获得一个音频散列匹配，并唤醒手机。
*   手机开始捕捉音频，这只是一个数字的时间序列向量(代表声波强度)。
*   语音音频被解码成音素或语音的文本表示。为每个话语生成几个候选词。

*   候选音素被组合在一起，试图组成单词。该算法使用最大似然或其他估计器来计算出哪些组合最有可能是将在当前上下文中使用的真实句子。
*   生成的句子必须经过意义解析，因此要进行多种类型的预处理，每个单词都用其可能的**词性** ( **POS** )进行标记。
*   给定短语的主语、宾语和动词，学习系统(通常是人工神经网络)将试图确定意图。
*   实际意图必须通过子程序来实现。
*   必须制定对用户的响应。在无法编写响应脚本的情况下，它必须通过算法生成。
*   文本到语音算法将响应解码成音素，然后必须合成听起来自然的语音，然后通过手机扬声器播放。

恭喜你，你正在去拿思乐冰的路上！你的体验是由几个人工神经网络、各种自然语言处理工具的多种不同用途、庞大的数据量以及数百万工程师小时的构建和维护工作提供动力的。这一经历也解释了 NLP 和 ML 之间的密切关系——它们不是一回事，但它们是在技术前沿合作的。

显然，自然语言处理不仅仅是我们可以在 25 页中涵盖的主题。这一章的目的不是全面的；它的目的是让您熟悉在解决涉及自然语言的 ML 问题时将采用的最常见的策略。我们将对七个与自然语言处理相关的概念进行一次旋风式的参观:

*   测量弦长
*   TF-IDF 指标
*   标记文本
*   词干
*   语音学
*   词性标注
*   Word2vec 嵌入文字

如果这些话题看起来令人望而生畏，不要担心。我们将依次介绍每一个，并展示许多例子。NLP 涉及很多行话，也有很多边缘案例，所以这个话题乍一看似乎不可接近。但话题终究是*自然语言*:我们每天都在说！一旦我们学会了行话，话题就变得相当直观，因为我们都对语言有非常强的直观理解。

我们从一个简单的问题开始讨论:如何测量*退出*和*引用*之间的距离？我们已经知道，我们可以测量空间中两点之间的距离，所以现在让我们来看看测量两个单词之间的距离。

# 字符串距离

能够测量两点之间的某种形式的距离总是很方便的。在前面的章节中，我们使用了点之间的距离来帮助聚类和分类。对于自然语言处理中的单词和段落，我们也可以这样做。当然，问题是单词是由字母组成的，距离是由数字组成的——那么我们如何用两个单词组成一个数字呢？

输入 Levenshtein 距离 *—* 一个简单的度量标准，用于衡量将一个字符串转换为另一个字符串所需的单字符编辑次数。Levenshtein 距离允许插入、删除和替换。莱文斯坦距离的一个修改，叫做**达梅罗-莱文斯坦距离**，也允许互换，或者两个相邻字母的交换。

为了用一个例子来说明这个概念，让我们试着把单词**板条箱**转换成单词**盘子**:

*   将 **r** 换成 **l** 得到**卡顿**
*   将 **c** 换成 **p** 得到**板**

因此板条箱和盘子之间的距离是 2。

**板**与**激光器**之间的距离为 3:

*   删除 **p** 获得**迟到**
*   插入一个 **r** 获得**稍后**
*   将 **t** 换成 **s** 获得**激光**

让我们用代码来确认这些例子。创建一个名为`Ch10-NLP`的新目录，并添加以下`package.json`文件:

```js
{
  "name": "Ch10-NLP",
  "version": "1.0.0",
  "description": "ML in JS Example for Chapter 10 - NLP",
  "main": "src/index.js",
  "author": "Burak Kanber",
  "license": "MIT",
  "scripts": {
    "start": "node src/index.js"
  },
  "dependencies": {
    "compromise": "^11.7.0",
    "natural": "^0.5.6",
    "wordnet-db": "^3.1.6"
  }
}
```

然后从命令行发出`yarn install`安装依赖项。这个`package.json`文件与前几章中的文件有点不同，因为`wordnet-db`依赖与 Browserify bundler 不兼容。因此，我们将不得不省略本章中的一些高级 JavaScript 特性。

创建一个名为`src`的目录，并向其中添加一个`index.js`文件，您将向其中添加以下内容:

```js
const compromise = require('compromise');
const natural = require('natural');
```

您将在本章的剩余部分使用这些导入，因此将它们保存在`index.js`文件中。然而，我们在本章中使用的其余代码将是可替换的；如果您愿意，您可以在本章的示例中删除旧的不相关的代码。

让我们使用`natural.js`库来看看 Levenshtein 距离:

```js
[
    ['plate', 'laser'],
    ['parachute', 'parasail'],
    ['parachute', 'panoply']
]
    .forEach(function(pair) {
        console.log("Levenshtein distance between '"+pair[0]+"' and '"+pair[1]+"': "
            + natural.LevenshteinDistance.apply(null, pair)
        );
    });
```

从命令行运行`yarn start`，您将看到以下输出:

```js
Levenshtein distance between 'plate' and 'laser': 3
Levenshtein distance between 'parachute' and 'parasail': 5
Levenshtein distance between 'parachute' and 'panoply': 7
```

试着尝试几对单词，看看你是否能计算出你大脑中的距离，从而对它有一个直观的感觉。

勒温斯坦距离有许多用途，因为它是一种度量，而不是任何特定的工具。其他系统，如拼写检查器、暗示器和模糊匹配器，使用 Levenshtein，或者在自己的算法中编辑距离度量。

让我们来看一个更高级的指标:TF-IDF 分数，它表示一个特定的单词在一组文档中有多有趣或多重要。

# 术语频率-反向文档频率

搜索相关性、文本挖掘和信息检索中最常用的指标之一是**术语频率-反向文档频率** ( **TF-IDF** )分数。本质上，TF-IDF 衡量一个词对特定文档的重要性。因此，TF-IDF 度量仅在文档中某个单词的上下文中有意义，该单词是更大的文档语料库的一部分。

假设你有一个文档语料库，比如关于不同主题的博客文章，你想让它变得可搜索。应用的最终用户运行*时尚风格*的搜索查询。然后，如何找到匹配的文档并根据相关性对它们进行排名？

TF-IDF 分数由两个独立但相关的部分组成。第一个是*术语频率*，或者给定文档中特定术语的相对频率。如果一篇 100 字的博文中有 4 次出现*时尚*这个词，那么该文档中*时尚*这个词的出现频率为 4%。

Note that term frequency only requires a single term and a single document as parameters; the full corpus of documents is not required for the term frequency component of TF-IDF.

然而，词频本身不足以确定相关性。像 *this* 和 *the* 这样的词在大多数文本中出现的频率很高，但是这些词通常与任何搜索都不相关。

因此，我们在计算中引入了第二个指标:反向文档频率。这个指标本质上是给定单词出现在文档中的百分比的倒数。如果你有 1000 篇博文，其中 50 篇出现*时尚*这个词，这个词的(非逆)文档频率是 5%。逆文档频率是这个概念的扩展，通过取逆文档频率的对数给出。

如果 n <sub>fashion</sub> 是包含单词 *fashion* 的文档数，而 *N* 是文档总数，那么逆文档频率由 *log 给出(N / n <sub>fashion</sub> )* 。在我们的例子中，*时尚*这个词的逆文档频率大概是 1.3。

如果我们现在考虑*这个词，它可能出现在 90%的文档中，我们发现*这个*的逆文档频率是 0.0451，比我们得到的*时装*的 1.3 要小得多。因此，反向文档频率衡量给定单词在一组文档中的稀有性或唯一性；更高的价值意味着这个词更罕见。计算反向文档频率所需的参数是术语本身和文档语料库(与术语频率不同，术语频率只需要一个文档)。*

 *TF-IDF 分数通过将术语频率和反向文档频率相乘来计算。结果是一个单一的度量标准，它概括了一个术语对于一个特定的文档有多重要或多有趣，在您所看到的所有文档中都考虑到了这一点。像**这样的词，在任何一个文档中*都可能有很高的术语出现频率，但是因为它们在所有文档中都很普遍，所以它们的 TF-IDF 总得分会很低。仅存在于文档子集的单词，如*时尚*，将具有更高的 TF-IDF 分数。当比较两个都包含单词*时尚*的单独文档时，更经常使用它的文档将具有更高的 TF-IDF 分数，因为两个文档的反向文档频率部分是相同的。*

 *在对搜索结果进行相关性评分时，最常见的方法是计算搜索查询中每个术语和语料库中每个文档的 TF-IDF 评分。每个查询词的单个 TF-IDF 分数可以加在一起，所得的总和可以称为该特定文档的**相关性分数**。一旦所有匹配的文档都以这种方式评分，您就可以按相关性进行排序，并以这种顺序显示它们。大多数全文搜索系统，如 Lucene 和 Elasticsearch，都使用这种方法进行相关性评分。

让我们在实践中看到这一点，使用`natural.js` TF-IDF 工具。在`index.js`中增加以下内容:

```js
const fulltextSearch = (query, documents) => {
    const db = new natural.TfIdf();
    documents.forEach(document => db.addDocument(document));
    db.tfidfs(query, (docId, score) => {
        console.log("DocID " + docId + " has score: " + score);
    });
};

fulltextSearch("fashion style", [
    "i love cooking, it really relaxes me and makes me feel at home",
    "food and restaurants are basically my favorite things",
    "i'm not really a fashionable person",
    "that new fashion blogger has a really great style",
    "i don't love the cinematic style of that movie"
]);
```

该代码定义了一个`fulltextSearch`函数，该函数接受一个搜索查询和一组要搜索的文档。每个文档都被添加到 TF-IDF 数据库对象中，在那里它被`natural.js`自动标记化。使用`yarn start`运行程序，您将看到以下输出:

```js
DocID 0 has score: 0
DocID 1 has score: 0
DocID 2 has score: 0
DocID 3 has score: 3.4271163556401456
DocID 4 has score: 1.5108256237659907
```

前两个文档与时尚或风格无关，返回零分。这些文档中*样式*和*样式*的术语频率分量为零，因此总得分为零。第三个文档的得分也是零。本文档确实提到了时尚，但是，由于没有执行词干处理，标记器无法将单词*时尚*与*时尚*协调起来。我们将在本章的后面部分深入讨论标记化和词干化，但是现在只要知道*词干化*是将单词简化为其词根形式的操作就足够了。

文件三和文件四的分数不为零。文件三得分较高，因为它同时包含了*时尚*和*风格*这两个词，而文件四只包含了*风格*这两个词。这个简单的度量在获取相关性方面做了令人惊讶的出色工作，这就是它被如此广泛使用的原因。

让我们更新代码，添加一个词干操作。在对文本应用词干后，我们期望文档二也具有非零的相关性分数，因为词干师应该将*时髦的*转变为*时髦的*。在`index.js`中添加以下代码:

```js
const stemmedFulltextSearch = (query, documents) => {
    const db = new natural.TfIdf();
    const tokenizer = new natural.WordTokenizer();
    const stemmer = natural.PorterStemmer.stem;
    const stemAndTokenize = text => tokenizer.tokenize(text).map(token => stemmer(token));

    documents.forEach(document => db.addDocument(stemAndTokenize(document)));
    db.tfidfs(stemAndTokenize(query), (docId, score) => {
        console.log("DocID " + docId + " has score: " + score);
    });
};

stemmedFulltextSearch("fashion style", [
    "i love cooking, it really relaxes me and makes me feel at home",
    "food and restaurants are basically my favorite things",
    "i'm not really a fashionable person",
    "that new fashion blogger has a really great style",
    "i don't love the cinematic style of that movie"
]);
```

我们添加了一个`stemAndTokenize`助手方法，并将其应用于添加到数据库和搜索查询中的文档。用`yarn start`运行代码，您将看到更新后的输出:

```js
DocID 0 has score: 0
DocID 1 has score: 0
DocID 2 has score: 1.5108256237659907
DocID 3 has score: 3.0216512475319814
DocID 4 has score: 1.5108256237659907
```

不出所料，文档二现在有一个非零的分数，因为词干分析器能够将单词*时髦*转换成*时髦*。文档二和文档四的分数相同，但只是因为这是一个非常简单的例子；使用更大的语料库，我们不会期望术语*时尚*和*风格*的反向文档频率相等。

搜索相关性和排名并不是 TF-IDF 的唯一应用。这种度量广泛用于许多用例和问题领域。TF-IDF 的一个有趣的用途是文章摘要。在文章总结中，目标是将一篇书面文章减少到只有几个能有效总结文章的句子。

解决文章摘要问题的一种方法是将文章中的每个句子或段落视为一个单独的文档。在为每个句子建立 TF-IDF 索引后，您可以评估每个单词的 TF-IDF 分数，并使用该分数对每个句子进行整体评分。挑出前三五句，按原文顺序展示，你就会有一个像样的总结。

让我们看看这个动作，使用`natural.js`和`compromise.js`。将以下代码添加到`index.js`:

```js
const summarize = (article, maxSentences = 3) => {
    const sentences = compromise(article).sentences().out('array');
    const db = new natural.TfIdf();
    const tokenizer = new natural.WordTokenizer();
    const stemmer = natural.PorterStemmer.stem;
    const stemAndTokenize = text => tokenizer.tokenize(text).map(token => stemmer(token));
    const scoresMap = {};

    // Add each sentence to the document
    sentences.forEach(sentence => db.addDocument(stemAndTokenize(sentence)));

    // Loop over all words in the document and add that word's score to an overall score for each sentence
    stemAndTokenize(article).forEach(token => {
        db.tfidfs(token, (sentenceId, score) => {
            if (!scoresMap[sentenceId]) scoresMap[sentenceId] = 0;
            scoresMap[sentenceId] += score;
        });
    });

    // Convert our scoresMap into an array so that we can easily sort it
    let scoresArray = Object.entries(scoresMap).map(item => ({score: item[1], sentenceId: item[0]}));
    // Sort the array by descending score
    scoresArray.sort((a, b) => a.score < b.score ? 1 : -1);
    // Pick the top maxSentences sentences
    scoresArray = scoresArray.slice(0, maxSentences);
    // Re-sort by ascending sentenceId
    scoresArray.sort((a, b) => parseInt(a.sentenceId) < parseInt(b.sentenceId) ? -1 : 1);
    // Return sentences
    return scoresArray
        .map(item => sentences[item.sentenceId])
        .join('. ');

};
```

上述`summarize`方法执行以下程序:

*   使用`compromise.js`从文章中提取句子
*   将每个句子添加到 TF-IDF 数据库中
*   对于文章中的每个单词，计算其在每个句子中的 TF-IDF 分数
*   将每个单词的 TF-IDF 分数添加到每个句子的总分数列表中(`scoresMap`对象)
*   将`scoresMap`转换为数组，使排序更容易
*   按相关性得分递减排序`scoresArray`
*   除去得分最高的句子
*   按照句子的时间顺序重新排序`scoresArray`
*   通过将最重要的句子连接在一起来构建总结

让我们在代码中添加一篇简单的文章，并尝试三句话和五句话的总结。在这个例子中，我将使用本节的前几段，但是您可以用您喜欢的任何内容替换文本。在`index.js`中增加以下内容:

```js
const summarizableArticle = "One of the most popular metrics used in search relevance, text mining, and information retrieval is the term frequency - inverse document frequency score, or tf-idf for short. In essence, tf-idf measures how significant a word is to a particular document. The tf-idf metric therefore only makes sense in the context of a word in a document that's part of a larger corpus of documents. Imagine you have a corpus of documents, like blog posts on varying topics, that you want to make searchable. The end user of your application runs a search query for fashion style. How do you then find matching documents and rank them by relevance? The tf-idf score is made of two separate but related components. The first is term frequency, or the relative frequency of a specific term in a given document. If a 100-word blog post contains the word fashion four times, then the term frequency of the word fashion is 4% for that one document. Note that term frequency only requires a single term and a single document as parameters; the full corpus of documents is not required for the term frequency component of tf-idf. Term frequency by itself is not sufficient to determine relevance, however. Words like this and the appear very frequently in most text and will have high term frequencies, but those words are not typically relevant to any search.";

console.log("3-sentence summary:");
console.log(summarize(summarizableArticle, 3));
console.log("5-sentence summary:");
console.log(summarize(summarizableArticle, 5));
```

当您使用`yarn start`运行代码时，您将看到以下输出:

```js
3-sentence summary:
 the tf idf metric therefore only makes sense in the context of a word in a document that's part of a larger corpus of documents. if a 100-word blog post contains the word fashion four times then the term frequency of the word fashion is 4% for that one document. note that term frequency only requires a single term and a single document as parameters the full corpus of documents is not required for the term frequency component of tf idf

 5-sentence summary:
 one of the most popular metrics used in search relevance text mining and information retrieval is the term frequency inverse document frequency score or tf idf for short. the tf idf metric therefore only makes sense in the context of a word in a document that's part of a larger corpus of documents. the first is term frequency or the relative frequency of a specific term in a given document. if a 100-word blog post contains the word fashion four times then the term frequency of the word fashion is 4% for that one document. note that term frequency only requires a single term and a single document as parameters the full corpus of documents is not required for the term frequency component of tf idf
```

这些总结的质量说明了`tf-idf metric`的力量和灵活性，同时也强调了一个事实，即你并不总是需要先进的 ML 或 AI 算法来完成有趣的任务。TF-IDF 还有许多其他用途，因此当您需要单词或术语的相关性时，您应该考虑使用该指标，因为它与语料库中的文档相关。

在本节中，我们使用了标记器和词干分析器，但没有正式介绍它们。这些都是 NLP 中的核心概念，现在正式介绍一下。

# 标记化

标记化是将输入字符串(如句子、段落，甚至是电子邮件等对象)转换为单个*标记*的行为。一个非常简单的标记器可能会将一个句子或段落用空格分割，从而生成单个单词的标记。但是，标记不一定需要是单词，输入字符串中的每个单词也不需要由标记器返回，标记器生成的每个标记也不需要出现在原始文本中，标记也不需要只表示一个单词。因此，我们使用术语*标记*而不是*单词*来描述标记器的输出，因为标记不总是单词。

在用最大似然算法处理文本之前标记文本的方式对算法的性能有很大影响。许多 NLP 和 ML 应用使用*单词包*方法，其中只有单词或标记重要，但它们的顺序不重要，正如我们在[第 5 章](05.html)、*分类算法*中探索的朴素贝叶斯分类器。然而，生成*二元模型*的标记器，或者彼此相邻的词对，实际上将保留原始文本的一些位置和语义意义，即使与词包算法一起使用。

有许多方法可以标记文本。如上所述，最简单的方法是用空格分割一个句子，生成单个单词的*标记流*。然而，这种简单的方法有许多问题。首先，算法会将大写的单词视为不同于小写的单词；水牛和水牛被认为是两个独立的词或标志。有时这是可取的，有时却不是。过于简化的标记化也将把诸如*不会*这样的缩写与*不会*这样的词区分开来，这两个词将被分成两个独立的标记，*会而不是*。

在大多数情况下，也就是说，在 80%的应用中，应该考虑的最简单的标记化是将所有文本转换为小写、删除标点和换行符、删除 HTML 等格式和标记，甚至删除 *stopwords* 或常见单词，如 *this* 或 *the* 。在其他情况下，需要更高级的令牌化，在某些情况下，需要更简单的令牌化。

在本节中，我将标记化的行为描述为一个复合过程，包括大小写转换、删除非字母数字字符和停用词过滤。但是，对于标记器的角色和职责，标记器库各有各的看法。您可能需要将库的标记化工具与其他工具相结合，以达到所需的效果。

首先，让我们构建自己的简单标记器。此标记器将字符串转换为小写，删除非字母数字字符，并删除长度少于三个字符的单词。将以下内容添加到您的`index.js`文件中，替换 Levenshtein 距离代码或在其下方添加:

```js
const tokenizablePhrase = "I've not yet seen 'THOR: RAGNAROK'; I've heard it's a great movie though. What'd you think of it?";

const simpleTokenizer = (text) =>
    text.toLowerCase()
        .replace(/(\w)'(\w)/g, '$1$2')
        .replace(/\W/g, ' ')
        .split(' ')
        .filter(token => token.length > 2);

console.log(simpleTokenizer(tokenizablePhrase));
```

这个`simpleTokenizer`会把字符串转换成小写，去掉一个单词中间的撇号(这样*就不会*变成*不会*，用空格替换掉其他所有非单词字符过滤掉。然后，它按空格字符拆分字符串，返回一个数组，最后删除少于三个字符的任何项。

运行`yarn start`会看到如下内容:

```js
[ 'ive', 'not', 'yet', 'seen', 'thor',
 'ragnarok', 'ive', 'heard', 'its',
 'great', 'movie', 'though',
 'whatd', 'you', 'think' ]
```

然后，这个令牌流可以以有序或无序的方式提供给算法。分类器，如朴素贝叶斯，将忽略顺序，并分析每个单词，就像它是独立的一样。

让我们将我们的简单标记器与`natural.js`和`compromise.js`提供的两个标记器进行比较。将以下内容添加到您的`index.js`文件中:

```js
console.log("Natural.js Word Tokenizer:");
console.log((new natural.WordTokenizer()).tokenize(tokenizablePhrase));
```

用`yarn start`运行代码将产生以下输出:

```js
Natural.js Word Tokenizer:
 [ 'I', 've', 'not', 'yet', 'seen',
 'THOR', 'RAGNAROK', 'I', 've',
 'heard', 'it', 's', 'a', 'great', 'movie',
 'though', 'What', 'd', 'you', 'think',
 'of', 'it' ]
```

如你所见，简短的单词被保留了下来，缩写，比如*我已经*被拆分成了单独的代币。此外，保留了大写字母。

让我们试试另一个`natural.js`标记器:

```js
console.log("Natural.js WordPunct Tokenizer:");
console.log((new natural.WordPunctTokenizer()).tokenize(tokenizablePhrase));
```

这将导致:

```js
Natural.js WordPunct Tokenizer:
 [ 'I', '\'', 've', 'not', 'yet', 'seen',
 '\'', 'THOR', ': ', 'RAGNAROK', '\'', '; ',
 'I', '\'', 've', 'heard', 'it', '\'', 's',
 'a', 'great', 'movie', 'though', '.', 'What',
 '\'', 'd', 'you', 'think', 'of',
 'it', '?' ]
```

这个标记器继续在标点上拆分，但是标点本身被保留下来。在标点符号很重要的应用中，这可能是需要的。

其他标记器库，如`compromise.js`中的标记器库，采取更智能的方法，甚至执行词性标注，以便在标记时解析和理解句子。让我们尝试一些`compromise.js`标记技术:

```js
console.log("Compromise.js Words:");
console.log(compromise(tokenizablePhrase).words().out('array'));
console.log("Compromise.js Adjectives:");
console.log(compromise(tokenizablePhrase).adjectives().out('array'));
console.log("Compromise.js Nouns:");
console.log(compromise(tokenizablePhrase).nouns().out('array'));
console.log("Compromise.js Questions:");
console.log(compromise(tokenizablePhrase).questions().out('array'));
console.log("Compromise.js Contractions:");
console.log(compromise(tokenizablePhrase).contractions().out('array'));
console.log("Compromise.js Contractions, Expanded:");
console.log(compromise(tokenizablePhrase).contractions().expand().out('array'));
```

使用`yarn start`运行新代码，您将看到以下内容:

```js
Compromise.js Words:
 [ 'i\'ve', '', 'not', 'yet', 'seen',
 'thor', 'ragnarok', 'i\'ve', '', 'heard',
 'it\'s', '', 'a', 'great', 'movie', 'though',
 'what\'d', '', 'you', 'think', 'of', 'it' ]
 Compromise.js Adjectives:
 [ 'great' ]
 Compromise.js Nouns:
 [ 'thor', 'ragnarok', 'movie' ]
 Compromise.js Questions:
 [ 'what\'d you think of it' ]
 Compromise.js Contractions:
 [ 'i\'ve', 'i\'ve', 'it\'s', 'what\'d' ]
 Compromise.js Contractions, Expanded:
 [ 'i have', 'i have', 'it is', 'what did' ]
```

`words()`标记器不会像`natural.js`标记器那样将收缩分开。此外，`compromise.js`使您能够从文本中提取特定的实体类型。我们可以分别提取形容词、名词、动词、疑问、缩略词(甚至有扩展缩略词的能力)；我们还可以使用`compromise.js`提取日期、标签、列表、子句和数值。

也不要求您的令牌必须直接映射到输入文本中的单词和短语。例如，在为电子邮件系统开发垃圾邮件过滤器时，您可能会发现在令牌流中包含来自电子邮件头的一些数据可以极大地提高准确性。电子邮件是否通过 SPF 和 DKIM 检查可能是您的垃圾邮件过滤器的一个非常强烈的信号。你可能还会发现区分正文和主题行也是有好处的；也可能是作为超链接出现的单词是比明文更强的信号。

通常，标记这种半结构化数据的最简单方法是在标记前加上一个字符或一组字符，这通常是标记器所不允许的。例如，电子邮件主题行中的标记可以以`_SUBJ:`为前缀，超链接中出现的标记可以以`_LINK:`为前缀。为了说明这一点，这里有一个电子邮件令牌流的示例:

```js
['_SPF:PASS',
 '_DKIM:FAIL',
 '_SUBJ:buy',
 '_SUBJ:pharmaceuticals',
 '_SUBJ:online',
 '_LINK:pay',
 '_LINK:bitcoin',
 'are',
 'you',
 'interested',
 'buying',
 'medicine',
 'online']
```

即使一个朴素贝叶斯分类器以前从未见过药品的引用，它可能会看到大多数垃圾邮件都没有通过 DKIM 检查，并且仍然将此邮件标记为垃圾邮件。或者你和会计部门密切合作，他们经常收到关于付款的电子邮件，但几乎从未收到外部网站超链接中带有`pay`字样的合法电子邮件；出现在明文中的*支付*代币与出现在超链接中的`_LINK:pay`代币的区别可能会对电子邮件是否被归类为垃圾邮件产生影响。

事实上，最早的垃圾邮件过滤突破之一是由 Y Combinator fame 的保罗·格拉厄姆开发的，他使用这种带有注释的电子邮件令牌的方法来标记早期垃圾邮件过滤器的准确性的显著提高。

另一种标记化方法是 *n-gram* 标记化，它将输入字符串分成 N 个大小的相邻标记组。事实上，所有的标记化都是 n-gram 标记化，但是，在前面的示例中，N 被设置为 1。更典型地，n-gram 标记化指的是 N > 1 的方案。最常见的是，你会遇到*二元模型*和*三元模型*符号化。

二元模型和三元模型标记化的目的是保留单个单词周围的一些上下文。一个与情感分析相关的例子是一个简单的可视化。短语*我不爱电影*将被标记为*我*、*爱了*、*不爱了*、*爱了*、*电影*、*电影*。当使用词袋算法时，如朴素贝叶斯，该算法会看到单词 *love* 并猜测句子有积极的情绪，因为词袋算法不考虑词之间的关系。

另一方面，二元模型标记器可以欺骗一个简单的算法来考虑单词之间的关系，因为每对单词都变成了一个标记。前面的短语，经过 bigram tokenizer 处理，会变成*我做了*、*没有*、*不爱*、*爱*、*电影*。即使每个令牌由两个单独的单词组成，算法也会对令牌进行操作，因此对待*不爱*和*我爱*会有所不同。因此，情感分析器将在每个单词周围有更多的上下文，并且能够区分否定短语(*不爱*)和肯定短语。

让我们在前面的例句中尝试一下`natural.js`二元模型标记器。将以下代码添加到`index.js`:

```js
console.log("Natural.js bigrams:");
console.log(natural.NGrams.bigrams(tokenizablePhrase));
```

用`yarn start`运行代码会产生:

```js
Natural.js bigrams:
 [ [ 'I', 've' ],
 [ 've', 'not' ],
 [ 'not', 'yet' ],
 [ 'yet', 'seen' ],
 [ 'seen', 'THOR' ],
 [ 'THOR', 'RAGNAROK' ],
 [ 'RAGNAROK', 'I' ],
 [ 'I', 've' ],
 [ 've', 'heard' ],
 [ 'heard', 'it' ],
 [ 'it', 's' ],
 [ 's', 'a' ],
 [ 'a', 'great' ],
 [ 'great', 'movie' ],
 [ 'movie', 'though' ],
 [ 'though', 'What' ],
 [ 'What', 'd' ],
 [ 'd', 'you' ],
 [ 'you', 'think' ],
 [ 'think', 'of' ],
 [ 'of', 'it' ] ]
```

n-gram 标记化的最大问题是它显著增加了数据域的熵。在 n-grams 上训练一个算法时，你不仅要确保算法学习所有重要的单词，还要学习所有重要的单词对。单词对比唯一单词多得多，因此 n-gram 标记化只有在您有非常大且全面的训练集时才会起作用。

解决 n-gram 熵问题的一个聪明方法，特别是在情感分析中处理否定的方法，是在否定之后立即转换标记，就像我们处理电子邮件标题和主题行一样。比如*不爱*这句话可以令牌化为*不*、 *_NOT:爱*，或者*不*、*！爱*，甚至只是*！爱*(丢弃*而不是*作为个体表征)。

在这个方案下，我不爱电影这个短语将被标记为*我*、*做了*、*而不是*、 *_NOT:爱*、*电影*、*电影*。这种方法的优点是上下文否定仍然得到保留，但一般来说，我们仍然使用可以用较小数据集训练的低熵单幅图。

文本标记有多种方法，每种方法都有其优缺点。像往常一样，您选择的方法将取决于手头的任务、可用的训练数据以及问题域本身。

在接下来的几节中，请记住标记化主题，因为这些主题也可以应用于标记化过程。例如，您可以在标记化后对单词进行词干处理，以进一步降低熵，或者您可以根据它们的 TF-IDF 分数来过滤您的标记，因此只使用文档中最有趣的单词。

为了继续我们关于熵的讨论，让我们花点时间来讨论一下*词干*。

# 堵塞物

词干是一种可以应用于单个单词的转换类型，尽管词干操作通常在标记化之后立即发生。标记化后的词干是如此普遍，以至于`natural.js`提供了一种可以附加到`String`类原型的`tokenizeAndStem`方便方法。

具体来说，词干将一个单词简化为它的词根形式，例如通过将 *running* 转换为 *run* 。标记化后对文本进行词干化可以显著降低数据集的熵，因为它本质上消除了意义相似但时态或屈折变化不同的单词的重复。你的算法不需要分别学习*运行*、*运行*、*运行*和*运行*这几个词，因为它们都会转化为*运行*。

最流行的词干算法，即*波特* *词干器*，是一种启发式算法，它为转换定义了许多阶段规则。但是，本质上，它归结为从词尾去掉标准的动词和名词屈折变化，并处理出现的特定边缘情况和常见的不规则形式。

从某种意义上说，词干是一种压缩算法，它丢弃了关于屈折变化和特定单词形式的信息，但保留了词根留下的概念信息。因此，词干不应用于语言本身的词形变化或形式很重要的情况。

出于同样的原因，词干擅长于概念信息比形式更重要的情况。主题提取就是一个很好的例子:如果有人在写自己作为跑步者的经历和他们观看田径比赛的经历，这并不重要——他们仍然在写跑步。

因为词干减少了数据熵，所以当数据集很小或不大时，它被非常有效地使用。然而，堵塞不能随便使用。如果不必要地使用词干，非常大的数据集可能会导致精度损失。当你阻止文本时，你就破坏了信息，而具有非常大的训练集的模型可能已经能够使用这些额外的信息来生成更好的预测。

在实践中，你永远不应该去猜测你的模型在有或没有词干的情况下是否会表现得更好:你应该尝试两种方法，看看哪个表现得更好。我不能告诉你*什么时候*要用炮泥，我只能告诉你为什么用，为什么有时候不用。

让我们尝试一下`natural.js`波特词干分析器，我们将把它和我们之前的标记化结合起来。在`index.js`中增加以下内容:

```js
console.log("Tokenized and stemmed:");
console.log(
    (new natural.WordTokenizer())
        .tokenize(
            "Writing and write, lucky and luckies, part parts and parted"
        )
        .map(natural.PorterStemmer.stem)
```

使用`yarn start`运行代码，您将看到以下内容:

```js
Tokenized and stemmed:
 [ 'write', 'and', 'write',
 'lucki', 'and', 'lucki',
 'part', 'part', 'and', 'part' ]
```

这个简单的例子说明了不同形式的单词是如何简化成它们的概念意义的。它还说明了不能保证词干分析器会创建*真实的*单词(你在字典里找不到`lucki`，只能保证它会为一组相似构造的单词减少熵。

还有其他词干分析算法试图从更语言学的角度来处理这个问题。那种词干叫做**引理化**，与词干类似的叫做**引理**，或者一个单词的字典形式。本质上，词干分析器是一个词干分析器，它首先确定单词的词性(通常需要字典，如 *WordNet* ，然后对特定的词性应用深入的规则，可能涉及更多的查找表。举个例子 *better* 这个词通过词干没有变化，但是通过引理化的方式转化为 *good* 这个词。引理化在大多数日常任务中是不必要的，但是当你的问题需要更精确的语言规则或极大地减少熵时，它可能是有用的。

我们在讨论自然语言处理或语言学时，不能不讨论最常见的交流方式:言语。一个语音到文本或文本到语音的系统实际上是如何知道如何说出英语中几十万个定义的单词，加上任意数量的名字的？答案是*语音学*。

# 语音学

语音检测，例如语音到文本系统中使用的语音检测，是一个令人惊讶的难题。说话风格、发音、方言和口音有如此多的变化，节奏、音调、速度和演讲也有如此多的变化，再加上音频是简单的一维时域信号，所以即使是今天最先进的智能手机技术也是*好，而不是很棒*也就不足为奇了。

虽然现代语音到文本的转换比我在这里介绍的要深入得多，但我想向您展示*语音算法*的概念。这些算法将一个单词转换成类似语音散列的东西，这样就很容易识别出听起来彼此相似的单词。

*隐喻*算法就是这样一种语音算法。它的目的是将一个单词简化为一种简化的语音形式，最终目标是能够索引相似的发音。隐喻使用由 16 个字符组成的字母表:0BFHJKLMNPRSTWXY。0 字符代表*第*音， *X* 代表一个 *sh* 或 *ch* 音，其他字母照常发音。几乎所有的元音信息都在转换中丢失了，尽管如果它们是一个单词的第一个音，有些会被保留下来。

一个简单的例子说明了语音算法的用处。想象一下，你负责一个搜索引擎，人们不停地搜索*知识就是力量，法国就是培根*。熟悉艺术史的你会明白，其实是弗兰西斯·培根说*知识就是力量*，你的用户只是听错了这句话。你想添加一个*你的意思是:**弗朗西斯·培根*** 链接到你的搜索结果，但不知道如何处理这个问题。

让我们看看隐喻算法是如何将术语`France is Bacon`和`Francis Bacon`音素化的。在`index.js`中增加以下内容:

```js
console.log(
    (new natural.WordTokenizer())
        .tokenize("Francis Bacon and France is Bacon")
        .map(t => natural.Metaphone.process(t))
);
```

当您使用`yarn start`运行代码时，您将看到以下内容:

```js
[ 'FRNSS', 'BKN', 'ANT', 'FRNS', 'IS', 'BKN' ]
```

弗朗西斯变成了`FRNSS`，法国变成了`FRNS`，培根变成了`BKN`。直观地说，这些字符串代表了用来发音的最明显的声音。

在音素化之后，我们可以使用莱文斯坦距离来衡量两个单词之间的相似性。如果忽略空格， *FRNSS BKN* 和 *FRNS IS BKN* 之间只有一个 Levenshtein 距离(增加了*I*)；因此，这两个短语听起来非常相似。您可以使用这些信息，结合搜索词的其余部分和反向查找，来确定`France is Bacon`是`Francis Bacon`的似是而非的错误发音，并且`Francis Bacon`实际上是要在您的搜索结果中呈现的正确主题。语音拼写错误和误解，如`France is Bacon`，非常常见，我们甚至在一些拼写检查工具中使用它们。

语音转文本系统中也使用了类似的方法。录音系统尽最大努力捕捉你发出的特定元音和辅音，并使用语音索引(映射到各种词典单词的语音反向查找)来得出一组候选单词。典型地，神经网络然后将考虑语音形式的置信度和结果语句的语义意义或无意义来确定哪个是最可能的单词组合。最有意义的一组词是呈现给你的东西。

`natural.js`库还提供了一种方便的方法来比较两个单词，如果它们听起来相似，则返回 *true* 。尝试以下代码:

```js
console.log(natural.Metaphone.compare("praise", "preys"));
console.log(natural.Metaphone.compare("praise", "frays"));
```

运行时，这将返回`true`，然后返回`false`。

当你的问题涉及发音或处理发音相似的单词和短语时，你应该考虑使用语音算法。这通常仅限于更专业的领域，但是语音到文本和文本到语音系统变得非常流行，如果用户将来开始通过语音与您的服务进行交互，您可能会发现自己需要更新语音类声音的搜索算法。

说到语音系统，现在让我们来看看词性标注，以及如何使用它从短语中提取语义信息——比如你可能向智能手机助手发出的命令。

# 词性标注

A **词性** ( **词性**)标记者分析一段文本，如一个句子，并确定每个单词在句子上下文中的词性。实现这一点的唯一方法是使用字典查找，因此它不是一个可以仅从第一性原理开发的算法。

词性标注的一个很好的用例是从命令中提取意图。例如，当你说 *Siri 时，请从约翰的披萨店*给我点一个披萨，AI 系统会用词类标记命令，以便从命令中提取主语、动词、宾语和任何其他相关细节。

此外，词性标注经常被用作其他自然语言处理操作的支持工具。例如，主题提取大量使用词性标注，以便将人物、地点和主题与动词和形容词分开。

请记住，词性标注从来都不是完美的，特别是由于英语的歧义性。许多单词既可以用作名词，也可以用作动词，因此许多词性标注者会返回给定单词的候选词性列表。执行词性标注的库具有广泛的复杂性，从简单的试探法到字典查找，再到试图基于上下文确定词性的高级模型。

`compromise.js`库具有灵活的 POS 标记器和匹配/提取系统。`compromise.js`图书馆的独特之处在于，它旨在成为足够好的*，但并不全面；它只在英语中最常见的单词上进行训练，这足以在大多数情况下给出 80-90%的准确率，同时仍然是一个快速而小的库。*

 *让我们看看`compromise.js`词性标注和匹配在行动。将以下代码添加到`index.js`:

```js
const siriCommand = "Hey Siri, order me a pizza from John's pizzeria";
const siriCommandObject = compromise(siriCommand);

console.log(siriCommandObject.verbs().out('array'));
console.log(siriCommandObject.nouns().out('array'));
```

使用`compromise.js`允许我们从命令中只提取动词，或者只提取名词(和其他词类)。用`yarn start`运行代码会产生:

```js
[ 'order' ]
[ 'siri', 'pizza', 'john\'s pizzeria' ]
```

POS tagger 已经将`order`确定为句子中唯一的动词；然后，这些信息可以用来加载 Siri 人工智能系统内置的正确的订单子程序。然后，提取的名词可以被发送到子程序，以确定订单的类型和来源。

令人印象深刻的是，POS tagger 还将`John's pizzeria`确定为单个名词，而不是将`John's`和`pizzeria`这两个词视为单独的名词。标记者已经理解`John's`是所有格，因此适用于其后的单词。

我们也可以使用`compromise.js`为常用命令编写解析和提取规则。让我们试一试:

```js
console.log(
    compromise("Hey Siri, order me a pizza from John's pizzeria")
        .match("#Noun [#Verb me a #Noun+ *+ #Noun+]").out('text')
);

console.log(
    compromise("OK Google, write me a letter to the congressman")
        .match("#Noun [#Verb me a #Noun+ *+ #Noun+]").out('text')
);
```

用`yarn start`运行代码会产生:

```js
order me a pizza from John's
write me a letter to the congressman
```

相同的匹配选择器能够捕获这两个命令，通过匹配组(用`[]`表示)忽略命令的收件人(Siri 或 Google)。因为两个命令都遵循动词-名词-名词模式，所以两个命令都将匹配选择器。

当然，这个选择器本身不足以构建一个完整的 AI 系统，比如 Siri 或者 Google Assistant。该工具将在人工智能系统过程开始时使用，以便根据预定义但灵活的命令格式确定用户的总体意图。你可以设计一个系统来回应一些短语，比如*打开我的#名词*，这里的名词可以是`calendar`或`email`或`Spotify`，或者*给#名词*写一封电子邮件，等等。这个工具可以作为建立自己的语音或自然语言命令系统的第一步，也可以用于各种主题提取应用。

在本章中，我们讨论了自然语言处理中使用的基础工具。许多高级自然语言处理任务使用人工神经网络作为学习过程的一部分，但是对于许多新手来说，不清楚单词和自然语言应该如何被发送到人工神经网络的输入层。在下一节中，我们将讨论*单词嵌入*，特别是单词 2vec 算法，该算法可用于将单词输入人工神经网络和其他系统。

# 单词嵌入和神经网络

在本章中，我们讨论了各种自然语言处理技术，特别是关于文本预处理的技术。在许多用例中，我们需要与人工神经网络交互来执行最终的分析。分析的类型与本节无关，但是假设您正在开发一个情绪分析人工神经网络。您适当地标记和标记您的训练文本，然后，当您尝试在预处理文本上训练您的人工神经网络时，您意识到您不知道如何将单词输入神经网络。

最简单的方法是将网络中的每个输入神经元映射到一个单独的唯一单词。处理文档时，您可以将输入神经元的值设置为文档中该单词的词频(或绝对计数)。你会有一个网络，其中一个输入神经元对“时尚”这个词做出反应，另一个神经元对“T2”科技做出反应，另一个神经元对“T4”食物做出反应，等等。

这种方法可行，但有几个缺点。人工神经网络的拓扑必须预先定义，因此在开始训练网络之前，您必须知道您的训练集中有多少个唯一的单词；这将成为输入层的大小。这也意味着你的网络在经过训练后是没有能力学习新单词的。要给网络添加一个新词，你必须从头开始构建和训练一个新的网络。

此外，在整个文档语料库中，您可能会遇到成千上万个独特的单词。这对人工神经网络的效率有巨大的负面影响，因为你需要一个输入层，比如说，有 10，000 个神经元。这将大大增加网络所需的培训时间以及系统的内存和处理要求。

每个神经元一个单词的方法也直观地感觉效率低下。虽然您的语料库包含 10，000 个独特的单词，但其中大多数将是罕见的，并且只出现在少数文档中。对于大多数文档，只有几百个输入神经元会被激活，其他的被设置为零。这相当于所谓的**稀疏矩阵**或**稀疏向量**，或其中大多数值为零的向量。

因此，当自然语言与人工神经网络交互时，需要一种更进化的方法。一系列称为*单词嵌入*的技术可以分析一个文本语料库，并将每个单词转换为固定长度的数值向量。向量是单词的固定长度表示，就像散列(如 md5 或 sha1)是任意数据的固定长度表示一样。

单词嵌入有几个优点，特别是当与人工神经网络一起使用时。由于词向量的长度是固定的，网络的拓扑可以预先决定，也可以在初始训练后处理新词的出现。

单词向量也是*密集向量*，意思是你的网络中不需要 10000 个输入神经元。单词向量的大小(以及输入层的大小)的一个很好的值是在 100-300 个项目之间。仅这一个因素就显著降低了人工神经网络的维数，并允许更快的模型训练和收敛。

有许多单词嵌入算法可供选择，但目前最先进的选择是在谷歌开发的单词 2vec 算法。这种特殊的算法还有另一个令人满意的特点:相似的单词将按照它们的向量表示法被聚集在一起。

在本章的前面，我们看到我们可以使用字符串距离来测量两个单词之间的印刷距离。我们还可以使用单词的两个语音表示之间的字符串距离来衡量它们听起来有多相似。使用 Word2vec 时，可以测量两个词向量之间的距离，得到两个词之间的*概念*距离。

Word2vec 算法本身是一个浅层神经网络，它在你的文本语料库上训练自己。该算法使用 n-grams 来发展单词之间的上下文感。如果*时尚*和*博主*这两个词在你的语料库中经常相邻出现，Word2vec 会给这两个词分配相似的向量。如果*时尚*和*数学*很少出现在一起，它们的合成向量会相隔一段距离。因此，两个词向量之间的距离代表它们的概念和上下文距离，或者两个词在语义内容和上下文方面有多相似。

Word2vec 算法的这一特性也赋予了最终处理数据的人工神经网络自身的效率和准确性优势，因为单词向量将为相似的单词激活相似的输入神经元。Word2vec 算法不仅降低了问题的维数，还在单词嵌入中加入了上下文信息。这些额外的上下文信息正是人工神经网络非常擅长捕捉的信号类型。

以下是涉及自然语言和人工神经网络的常见工作流示例:

*   标记和阻止所有文本
*   从文本中删除停用词
*   确定合适的人工神经网络输入层大小；将此值用于输入层和 Word2vec 维度
*   使用 Word2vec 为文本生成单词嵌入
*   用嵌入这个词来训练你任务中的人工神经网络
*   评估新文档时，在将文档传递给人工神经网络之前，对其进行标记化、词干化和矢量化

使用诸如 Word2vec 这样的单词嵌入算法不仅会提高模型的速度和内存性能，而且由于 Word2vec 算法保留的上下文信息，它还可能提高模型的准确性。还应该注意的是，像 n-gram 标记化一样，Word2vec 是一种可能的方法来欺骗一个简单的单词包算法来考虑单词上下文，因为 Word2vec 算法本身使用 n-gram 来开发嵌入。

虽然单词嵌入主要用于自然语言处理，但同样的方法也可以用于其他领域，如遗传学和生物化学。在这些领域中，有时能够对蛋白质或氨基酸序列进行载体化是有利的，这样类似的结构将具有类似的载体嵌入。

# 摘要

自然语言处理是一个内容丰富的研究领域，在最大似然语言处理、计算语言学和人工智能方面有许多先进的技术和广泛的应用。然而，在这一章中，我们将重点放在日常 ML 任务中最流行的特定工具和策略上。

本章介绍的技术是可以混合和匹配的构建模块，以实现许多不同的结果。仅使用本章中的信息，您就可以构建一个简单的全文搜索引擎、口头或书面命令的意图提取器、文章摘要器以及许多其他令人印象深刻的工具。然而，当这些技术与高级学习模型(如人工神经网络和神经网络)相结合时，自然语言处理最令人印象深刻的应用就会出现。

特别是，您学习了单词度量，例如字符串距离和 TF-IDF 相关性评分；预处理和降维技术，如标记化和词干化；语音算法，如隐喻算法；词性提取和短语分析；以及使用单词嵌入算法将单词转换成向量。

通过大量的例子，您还被介绍到了两个优秀的 JavaScript 库，`natural.js`和`compromise.js`，它们可以用来轻松完成大多数与 ML 相关的 NLP 任务。你甚至可以用 20 行代码写一篇总结文章！

在下一章中，我们将讨论如何在一个实时的、面向用户的 JavaScript 应用中整合您到目前为止所学的一切。***