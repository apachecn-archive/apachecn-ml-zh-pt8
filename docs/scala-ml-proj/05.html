<html><head/><body>


    
        <title>Topic Modeling - A Better Insight into Large-Scale Texts</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">主题建模——对大规模文本的更好理解</h1>
                
            
            
                
<p>主题建模是一种广泛用于从大量文档中挖掘文本的技术。然后，这些主题可以用于总结和组织包含主题术语及其相对权重的文档。这个项目将使用的数据集只是普通的非结构化文本格式。</p>
<p>我们将看到如何有效地使用<strong>潜在狄利克雷分配</strong> ( <strong> LDA </strong>)算法在数据中寻找有用的模式。我们将比较其他TM算法和LDA的可伸缩性。此外，我们将利用<strong>自然语言处理</strong> ( <strong> NLP </strong>)库，比如斯坦福NLP。</p>
<p>简而言之，在这个端到端项目中，我们将学习以下主题:</p>
<ul>
<li>主题建模和文本聚类</li>
<li>LDA算法是如何工作的？</li>
<li>使用LDA、Spark MLlib和标准NLP进行主题建模</li>
<li>其他主题模型和LDA的可扩展性测试</li>
<li>模型部署</li>
</ul>


            

            
        
    






    
        <title>Topic modeling and text clustering</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">主题建模和文本聚类</h1>
                
            
            
                
<p>在TM中，主题由词的簇定义，簇中的每个词具有给定主题的出现概率，并且不同的主题具有它们各自的词簇以及相应的概率。不同的主题可以共享一些单词，一个文档可以有多个相关的主题。简而言之，我们有一个文本数据集的集合，也就是一组文本文件。现在具有挑战性的部分是使用LDA找到有用的数据模式。</p>
<p>有一种流行的基于LDA的TM方法，其中每个文档被认为是主题的混合，并且文档中的每个单词被认为是从文档的主题中随机抽取的。主题被认为是隐藏的，必须通过分析联合分布来揭示，以计算隐藏变量(主题)的条件分布，给定文档中观察到的变量和单词。TM技术广泛用于从大量文档中挖掘文本的任务中。这些主题可用于总结和组织包含主题术语及其相对权重的文档(参见<em>图1 </em>):</p>
<div><img height="379" width="709" class="alignnone size-full wp-image-529 image-border" src="img/998c2c26-e90d-46c5-8029-8874910126df.png"/></div>
<p>图1:简明的TM(资料来源:Blei，D.M .等人，概率主题模型，ACM通讯，55(4(，77-84，2012))</p>
<p>由于上图中可以看到的主题数量远小于与文档集合相关联的词汇表，因此主题空间表示也可以视为一个降维过程:</p>
<div><img height="356" width="601" class="alignnone size-full wp-image-530 image-border" src="img/9557f8d8-4641-4391-9691-bae70f289da5.png"/></div>
<p>图2: TM与文本聚类</p>
<p>与TM相反，在文档聚类中，基本思想是基于众所周知的相似性度量将文档分组到不同的组中。为了执行分组，每个文档由表示分配给文档中单词的权重的向量来表示。</p>
<p>通常使用术语频率-逆文档频率来执行加权(也称为<strong> TF-IDF </strong>方案)。聚类的最终结果是一个聚类列表，每个文档都出现在其中一个聚类中。下图说明了TM和文本聚类之间的基本区别:</p>


            

            
        
    






    
        <title>How does LDA algorithm work?</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">LDA算法是如何工作的？</h1>
                
            
            
                
<p>LDA是一个主题模型，它从一组文本文档中推断出主题。LDA可以被认为是一种聚类算法，其中主题对应于聚类中心，文档对应于数据集中的示例(行)。主题和文档都存在于特征空间中，其中特征向量是单词计数(单词包)的向量。LDA不是使用传统的距离来估计聚类，而是使用基于文本文档如何生成的统计模型的函数(参见图3 ):</p>
<div><img height="385" width="575" class="alignnone size-full wp-image-531 image-border" src="img/87d01601-71f3-4c34-9fbc-cf468d483985.png"/></div>
<p>图LDA算法在一组文档上的工作原理</p>
<p>特别是，我们想从大量的文本中讨论人们谈论最多的话题。自从Spark 1.3发布以来，MLlib支持LDA，这是在文本挖掘和NLP领域最成功使用的TM技术之一。</p>
<p>而且LDA也是第一个采用Spark GraphX的MLlib算法。在我们正式开始TM应用之前，以下术语值得了解:</p>
<ul>
<li>词汇的一个元素</li>
<li><kbd>"token"</kbd>:出现在文档中的术语的实例</li>
<li><kbd>"topic"</kbd>:代表一些概念的词的多项式分布</li>
</ul>
<p>Spark中开发的基于RDD的LDA算法是为文本文档设计的主题模型。它基于LDA的原始论文(期刊版):Blei，Ng和Jordan，<em>潜在狄利克雷分配</em>，JMLR，2003年。</p>
<p>这个实现通过<kbd>setOptimizer</kbd>函数支持不同的推理算法。<kbd>EMLDAOptimizer</kbd>使用对似然函数的<strong>期望最大化</strong> ( <strong> EM </strong>)来学习聚类，并产生综合结果，而<kbd>OnlineLDAOptimizer</kbd>使用迭代小批量采样进行在线变分推断，并且通常是记忆友好的。</p>
<p>EM是一种逼近最大似然函数的迭代方法。在实际中，当输入数据不完整，有缺失数据点，或者有隐藏的潜在变量时，ML估计可以找到<kbd>best fit</kbd>模型。</p>
<p>LDA将文档集合作为字数和以下参数的向量(使用构建器模式设置):</p>
<ul>
<li><kbd>K</kbd>:主题数(即聚类中心)(默认为10)。</li>
<li><kbd>ldaOptimizer</kbd>:用于学习LDA模型的优化器，可以是<kbd>EMLDAOptimizer</kbd>或<kbd>OnlineLDAOptimizer</kbd>(默认为<kbd>EMLDAOptimizer</kbd>)。</li>
<li><kbd>Seed</kbd>:再现性随机种子(可选)。</li>
<li><kbd>docConcentration</kbd> : Drichilet参数，用于主题上的优先文档分布。值越大，推断分布越平滑(默认值为- <kbd>Vectors.dense(-1)</kbd>)。</li>
<li><kbd>topicConcentration</kbd> : Drichilet参数，用于先验主题在术语(词)上的分布。较大的值确保推断的分布更平滑(默认值为-1)。</li>
<li><kbd>maxIterations</kbd>:迭代次数限制(默认为20)。</li>
<li><kbd>checkpointInterval</kbd>:如果使用检查点(在Spark配置中设置)，该参数指定创建检查点的频率。如果<kbd>maxIterations</kbd>很大，使用检查点可以帮助减少磁盘上的随机文件大小，并有助于故障恢复(默认为10)。</li>
</ul>
<div><img height="393" width="414" class="alignnone size-full wp-image-532 image-border" src="img/b1acc9cd-72c8-45e3-a84b-a2184a5e5e0e.png"/></div>
<p>图4:主题分布及其外观</p>
<p>让我们看一个例子。假设篮子里有<em> n </em>个球，它们有<em> w </em>种不同的颜色。现在还假设词汇表中的每个术语都有一种颜色。现在还假设词汇项分布在<em> m </em>个主题中。现在，篮子中每种颜色的出现频率与主题中相应术语的权重<em> φ </em>成正比。</p>
<p>然后，LDA算法通过使每个球的大小与其对应项的权重成比例来结合项加权方案。在<em>图4 </em>中，<em> n </em>项拥有一个主题的总权重，例如，主题0到3。图4 显示了随机生成的推文文本的主题分布。</p>
<p>现在我们已经看到，通过使用TM，我们可以在非结构化的文档集合中找到结构。一旦<strong>发现</strong>构造，如图<em>图4 </em>所示，我们可以回答如下几个问题:</p>
<ul>
<li>文档X是关于什么的？</li>
<li>文档X和Y有多相似？</li>
<li>如果我对题目Z感兴趣，应该先看哪些文档？</li>
</ul>
<p>在下一节中，我们将看到一个使用基于Spark MLlib的LDA算法来回答前面问题的TM示例。</p>


            

            
        
    






    
        <title>Topic modeling with Spark MLlib and Stanford NLP</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">用Spark MLlib和Stanford NLP进行主题建模</h1>
                
            
            
                
<p>在这一小节中，我们将介绍一种使用Spark的半自动TM技术。使用其他选项作为默认，我们在<a href="https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test">https://GitHub . com/minghui/Twitter-LDA/tree/master/data/data 4 model/test</a>从GitHub下载的数据集上训练LDA。然而，在本章后面的模型重用和部署阶段，我们将使用更多众所周知的文本数据集。</p>


            

            
        
    






    
        <title>Implementation</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">履行</h1>
                
            
            
                
<p>以下步骤显示了从数据读取到打印主题的TM，以及它们的术语权重。下面是TM管道的简短工作流程:</p>
<pre><strong>object</strong> topicmodelingwithLDA {<br/><strong>    def</strong> main(args: Array[String]): Unit = {<br/><strong>        val</strong> lda = <br/><strong>        new</strong> LDAforTM() <br/>// actual computations are done here<br/><strong>        val</strong> defaultParams = Params().copy(input = "data/docs/") //Loading parameters for training<br/>        lda.run(defaultParams) <br/>// Training the LDA model with the default parameters.<br/>      }<br/>}</pre>
<p>我们还需要导入一些相关的包和库:</p>
<pre><strong>import</strong> edu.stanford.nlp.process.Morphology<br/><strong>import</strong> edu.stanford.nlp.simple.Document<br/><strong>import</strong> org.apache.log4j.{Level, Logger}<br/><strong>import</strong> scala.collection.JavaConversions._<br/><strong>import</strong> org.apache.spark.{SparkConf, SparkContext}<br/><strong>import</strong> org.apache.spark.ml.Pipeline<br/><strong>import</strong> org.apache.spark.ml.feature._<br/><strong>import</strong> org.apache.spark.ml.linalg.{Vector =&gt; MLVector}<br/><strong>import</strong> org.apache.spark.mllib.clustering.{DistributedLDAModel, EMLDAOptimizer, LDA, OnlineLDAOptimizer, LDAModel}<br/><strong>import</strong> org.apache.spark.mllib.linalg.{ Vector, Vectors }<br/><strong>import</strong> org.apache.spark.rdd.RDD<br/><strong>import</strong> org.apache.spark.sql.{Row, SparkSession}</pre>
<p>对TM的实际计算是在<kbd>LDAforTM</kbd>类中完成的。<kbd>Params</kbd>是一个case类，用于加载参数来训练LDA模型。最后，我们使用通过<kbd>Params</kbd>类设置的参数来训练LDA模型。现在，我们将通过分步源代码来广泛解释每个步骤:</p>


            

            
        
    






    
        <title>Step 1 - Creating a Spark session</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤1 -创建Spark会话</h1>
                
            
            
                
<p>让我们通过定义计算核心的数量、SQL仓库和应用程序名称来创建一个Spark会话，如下所示:</p>
<pre><strong>val</strong> spark = SparkSession<br/>    .builder<br/>    .master("local[*]")<br/>    .config("spark.sql.warehouse.dir", "C:/data/")<br/>    .appName(s"LDA")<br/>    .getOrCreate()</pre>


            

            
        
    






    
        <title>Step 2 - Creating vocabulary and tokens count to train the LDA after text pre-processing</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤2 -在文本预处理之后，创建词汇和标记计数来训练LDA</h1>
                
            
            
                
<p><kbd>run()</kbd>方法采用<kbd>params</kbd>,比如输入文本、预定义的词汇大小和停用词文件:</p>
<pre><strong>def</strong> run(params: Params)</pre>
<p>然后，它开始对LDA模型进行文本预处理，如下所示(即在<kbd>run</kbd>方法内):</p>
<pre>// Load documents, and prepare them for LDA.<br/><strong>val</strong> preprocessStart = System.nanoTime()<br/><strong>val</strong> (corpus, vocabArray, actualNumTokens) = preprocess(params.input, params.vocabSize, params.stopwordFile)  </pre>
<p><kbd>Params</kbd> case类用于定义训练LDA模型的参数。这是这样的:</p>
<pre>//Setting the parameters before training the LDA model<br/><strong>case </strong><strong>class</strong> Params(<strong>var</strong> input: String = "", <strong>var</strong> ldaModel: LDAModel = <strong>null</strong>,<br/>    k: Int = 5,<br/>    maxIterations: Int = 100,<br/>    docConcentration: Double = 5,<br/>    topicConcentration: Double = 5,<br/>    vocabSize: Int = 2900000,<br/>    stopwordFile: String = "data/docs/stopWords.txt",<br/>    algorithm: String = "em",<br/>    checkpointDir: Option[String] = None,<br/>    checkpointInterval: Int = 100)</pre>
<p class="mce-root">为了获得更好的结果，可以在试错的基础上设置这些参数。或者，您应该使用交叉验证来获得更好的性能。现在，如果要对当前参数进行检查点检查，会使用以下代码行:</p>
<pre><strong>if</strong> (params.checkpointDir.nonEmpty) {<br/>    spark.sparkContext.setCheckpointDir(params.checkpointDir.get)<br/>     }</pre>
<p><kbd>preprocess</kbd>方法用于处理原始文本。首先，让我们使用<kbd>wholeTextFiles()</kbd>方法阅读全文，如下所示:</p>
<pre><strong>val</strong> initialrdd = spark.sparkContext.wholeTextFiles(paths).map(_._2) <br/>initialrdd.cache()  </pre>
<p>在前面的代码中，<kbd>paths</kbd>是文本文件的路径。然后，我们需要根据<kbd>lemma</kbd>文本从原始文本中准备形态RDD，如下所示:</p>
<pre><strong>val</strong> rdd = initialrdd.mapPartitions { partition =&gt;<br/><strong>    val</strong> morphology = <strong>new</strong> Morphology()<br/>    partition.map { value =&gt; helperForLDA.getLemmaText(value, morphology) }<br/>}.map(helperForLDA.filterSpecialCharacters)</pre>
<p>这里，<kbd>helperForLDA</kbd>类中的<kbd>getLemmaText()</kbd>方法使用<kbd>filterSpaecialChatacters()</kbd>方法过滤特殊字符后提供<kbd>lemma</kbd>文本，如(<kbd>"""[! @ # $ % ^ &amp; * ( ) _ + - − , " ' ; : . ` ? --]</kbd>)作为正则表达式。该方法如下进行:</p>
<pre><strong>def</strong> getLemmaText(document: String, morphology: Morphology) = {<br/><strong>    val</strong> string = <br/><strong>    new</strong> StringBuilder()<br/><strong>    val</strong> value = <br/><strong>    new</strong> Document(document).sentences().toList.flatMap { <br/>        a =&gt;<br/><strong>        val</strong> words = a.words().toList<br/><strong>        val</strong> tags = a.posTags().toList<br/>        (words zip tags).toMap.map { <br/>        a =&gt;<br/><strong>        val</strong> newWord = morphology.lemma(a._1, a._2)<br/><strong>        val</strong> addedWoed = <br/><strong>    if</strong> (newWord.length &gt; 3) {<br/>        newWord<br/>            }<br/><strong>    else</strong> { "" }<br/>        string.append(addedWoed + " ")<br/>        }<br/>        }<br/>    string.toString()<br/>} </pre>
<p>需要注意的是，<kbd>Morphology()</kbd>类通过只去除词形变化(不是派生词形)来计算英语单词的基本形式。也就是说，它只处理名词复数、代词格和动词词尾，而不处理比较级形容词或派生名词。<kbd>getLemmaText()</kbd>方法获取文档和相应的词法，并最终返回已词条化的文本。</p>
<p>这来自斯坦福大学NLP小组。要使用它，您应该在主类文件中有以下导入:<kbd>edu.stanford.nlp.process.Morphology</kbd>。在<kbd>pom.xml</kbd>文件中，您必须包含以下条目作为依赖项:</p>
<pre>&lt;dependency&gt;<br/>    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;<br/>    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;<br/>    &lt;version&gt;3.6.0&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;<br/>    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;<br/>    &lt;version&gt;3.6.0&lt;/version&gt;<br/>    &lt;classifier&gt;models&lt;/classifier&gt;<br/>&lt;/dependency&gt;</pre>
<p><kbd>filterSpecialCharacters()</kbd>如下所示:</p>
<pre><strong>def</strong> filterSpecialCharacters(document: String) = document.replaceAll("""[! @ # $ % ^ &amp; * ( ) _ + - − , " ' ; : . ` ? --]""", " ")</pre>
<p>移除特殊字符的RDD后，我们可以创建一个数据框架来构建文本分析管道:</p>
<pre>rdd.cache()<br/>initialrdd.unpersist()<br/><strong>val</strong> df = rdd.toDF("docs")<br/>df.show() </pre>
<p>DataFrame只包含文档标签。数据帧的快照如下:</p>
<div><img height="286" width="131" class="alignnone size-full wp-image-533 image-border" src="img/aa7d27a0-30ec-409b-8753-ee85f6410800.png"/></div>
<div><br/>
Figure 5: Raw texts from the input dataset</div>
<p>现在，如果你仔细观察前面的数据帧，你会发现我们仍然需要对它们进行标记。此外，数据帧中还有停用词，比如this、with等等，所以我们也需要删除它们。首先，让我们使用<kbd>RegexTokenizer</kbd> API来标记它们，如下所示:</p>
<pre><strong>val</strong> tokenizer = <strong>new</strong> RegexTokenizer()<br/>                .setInputCol("docs")<br/>                .setOutputCol("rawTokens")</pre>
<p>现在让我们删除所有停用词，如下所示:</p>
<pre><strong>val</strong> stopWordsRemover = <strong>new</strong> StopWordsRemover()<br/>                        .setInputCol("rawTokens")<br/>                        .setOutputCol("tokens")<br/>stopWordsRemover.setStopWords(stopWordsRemover.getStopWords ++ customizedStopWords)</pre>
<p>此外，我们还需要应用计数向量来从标记中只找到重要的特征。这将有助于使管道链成为管道阶段。让我们这样做:</p>
<pre><strong>val</strong> countVectorizer = <strong>new</strong> CountVectorizer()<br/>                    .setVocabSize(vocabSize)<br/>                    .setInputCol("tokens")<br/>                    .setOutputCol("features")</pre>
<p>当先验字典不可用时，<kbd>CountVectorizer</kbd>可以用作估计器来提取词汇并生成<kbd>CountVectorizerModel</kbd>。换句话说，<kbd>CountVectorizer</kbd>用于将一组文本文档转换成令牌(即术语)计数的向量。<kbd>CountVectorizerModel</kbd>为词汇表中的文档生成稀疏表示，然后可以将它提供给LDA。从技术上讲，当调用<kbd>fit()</kbd>方法进行拟合时，<kbd>CountVectorizer</kbd>将选择语料库中按词频排序的前<kbd>vocabSize</kbd>个单词。</p>
<p>现在，通过链接转换器(tokenizer、<kbd>stopWordsRemover</kbd>和<kbd>countVectorizer</kbd>)来创建管道，如下所示:</p>
<pre>val pipeline = new Pipeline().setStages(Array(tokenizer, stopWordsRemover, countVectorizer)) </pre>
<p>现在，让我们针对词汇和令牌数量来调整和转换管道:</p>
<pre class="mce-root"><strong>val</strong> model = pipeline.fit(df)<br/><strong>val</strong> documents = model.transform(df).select("features").rdd.map {<br/><strong>    case</strong> Row(features: MLVector) =&gt; Vectors.fromML(features)<br/>    }.zipWithIndex().map(_.swap) </pre>
<p class="mce-root">最后，返回词汇表和令牌计数对，如下所示:</p>
<pre class="mce-root">(documents, model.stages(2).asInstanceOf[CountVectorizerModel].vocabulary, documents.map(_._2.numActives).sum().toLong) Now let's see the statistics of the training data: <br/><br/>println() println("Training corpus summary:") <br/>println("-------------------------------")<br/>println("Training set size: " + actualCorpusSize + " documents")<br/>println("Vocabulary size: " + actualVocabSize + " terms")<br/>println("Number of tockens: " + actualNumTokens + " tokens")<br/>println("Preprocessing time: " + preprocessElapsed + " sec")<br/>println("-------------------------------")<br/>println()<br/>&gt;&gt;&gt;<br/>Training corpus summary:<br/>-------------------------------<br/>Training set size: 19 documents<br/>Vocabulary size: 21611 terms<br/>Number of tockens: 75784 tokens<br/>Preprocessing time: 46.684682086 sec<strong><br/></strong></pre>


            

            
        
    






    
        <title>Step 3 - Instantiate the LDA model before training</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤3 -在训练之前实例化LDA模型</h1>
                
            
            
                
<p>在开始使用以下代码训练LDA模型之前，让我们先实例化它:</p>
<pre>val lda = new LDA() </pre>


            

            
        
    






    
        <title>Step 4 - Set the NLP optimizer</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤4 -设置NLP优化器</h1>
                
            
            
                
<p>为了从LDA模型获得更好和优化的结果，我们需要设置包含LDA算法的优化器，并执行存储内部数据结构(例如，图或矩阵)和算法的其他参数的实际计算。</p>
<p>这里我们使用了<kbd>EMLDAOPtimizer</kbd>优化器。您也可以使用<kbd>OnlineLDAOptimizer()</kbd>优化器。<kbd>EMLDAOPtimizer</kbd>存储一个<em>数据+参数</em>图，加上算法参数。底层实现使用EM。</p>
<p>首先，让我们通过将<kbd>(1.0 / actualCorpusSize)</kbd>和一个非常低的学习率(即0.05)添加到<kbd>MiniBatchFraction</kbd>来实例化<kbd>EMLDAOptimizer</kbd>，以在一个像我们这样的小数据集上收敛训练，如下所示:</p>
<pre><strong>val</strong> optimizer = params.algorithm.toLowerCase <br/><strong>    match</strong> {<br/><strong>        case</strong> "em" =&gt; <br/><strong>            new</strong> EMLDAOptimizer<br/>// add (1.0 / actualCorpusSize) to MiniBatchFraction be more robust on tiny datasets.<br/><strong>        case</strong> "online" =&gt; <br/><strong>            new</strong> OnlineLDAOptimizer().setMiniBatchFraction(0.05 + 1.0 / actualCorpusSize)<br/><strong>        case</strong> _ =&gt; <br/><strong>            throw</strong><strong>new</strong> IllegalArgumentException("Only em, online are supported but got <br/>            ${params.algorithm}.")<br/>    } </pre>
<p>现在，使用LDA API中的<kbd>setOptimizer()</kbd>方法设置优化器，如下所示:</p>
<pre>lda.setOptimizer(optimizer)<br/>    .setK(params.k)<br/>    .setMaxIterations(params.maxIterations)<br/>    .setDocConcentration(params.docConcentration)<br/>    .setTopicConcentration(params.topicConcentration)<br/>    .setCheckpointInterval(params.checkpointInterval)</pre>


            

            
        
    






    
        <title>Step 5 - Training the LDA model</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤5 -训练LDA模型</h1>
                
            
            
                
<p>让我们使用训练语料库开始训练LDA模型，并如下跟踪训练时间:</p>
<pre class="mce-root"><strong>val</strong> startTime = System.nanoTime()<br/>ldaModel = lda.run(corpus)<br/><br/><strong>val</strong> elapsed = (System.nanoTime() - startTime) / 1e9<br/>println("Finished training LDA model. Summary:")<br/>println("Training time: " + elapsed + " sec")</pre>
<p class="mce-root">此外，现在我们可以保存训练好的模型，以备将来重复使用，如下所示:</p>
<pre class="mce-root">//Saving the model for future use<br/>params.ldaModel.save(spark.sparkContext, "model/LDATrainedModel")</pre>
<p>请注意，一旦您完成了训练并获得了最佳的训练，在部署模型之前取消对前面一行的注释。否则，它会在模型重用阶段抛出一个异常而停止。</p>
<p>对于我们现有的文本，LDA模型花了6.309715286秒来训练。注意这些时间代码是可选的。在此，我们提供这些数据仅供参考，以便了解培训时间:</p>


            

            
        
    






    
        <title>Step 6 - Prepare the topics of interest</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤6 -准备感兴趣的话题</h1>
                
            
            
                
<p>准备前5个主题，每个主题有10个术语。包括术语及其相应的权重:</p>
<pre><strong>val</strong> topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 10)<br/>println(topicIndices.length)<br/><strong>val</strong> topics = topicIndices.map {<br/><strong>    case</strong> (terms, termWeights) =&gt; terms.zip(termWeights).map {<br/><strong>    case</strong> (term, weight) =&gt; (vocabArray(term.toInt), weight) <br/>   }<br/>}</pre>


            

            
        
    






    
        <title>Step 7 - Topic modelling </title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤7 -主题建模</h1>
                
            
            
                
<p>打印前10个主题，显示每个主题的权重最高的术语。此外，包括每个主题的总重量，如下所示:</p>
<pre class="mce-root"><strong>var</strong> sum = 0.0<br/>println(s"${params.k} topics:")<br/>topics.zipWithIndex.foreach {<br/><strong>    case</strong> (topic, i) =&gt;<br/>        println(s"TOPIC $i")<br/>        println("------------------------------")<br/>        topic.foreach {<br/><strong>    case</strong> (term, weight) =&gt;<br/>        term.replaceAll("\s", "")<br/>        println(s"$termt$weight")<br/>        sum = sum + weight<br/>    }<br/>println("----------------------------")<br/>println("weight: " + sum)<br/>println()</pre>
<p class="mce-root">现在让我们来看看我们的LDA模型对主题建模的输出:</p>
<pre class="mce-root"> 5 topics:<br/> TOPIC 0<br/> ------------------------------<br/> come 0.0070183359426213635<br/> make 0.006893251344696077<br/> look 0.006629265338364568<br/> know 0.006592594912464674<br/> take 0.006074234442310174<br/> little 0.005876330712306203<br/> think 0.005153843469004155<br/> time 0.0050685675513282525<br/> hand 0.004524837827665401<br/> well 0.004224698942533204<br/> ----------------------------<br/> weight: 0.05805596048329406<br/> TOPIC 1<br/> ------------------------------<br/> thus 0.008447268016707914<br/> ring 0.00750959344769264<br/> fate 0.006802070476284118<br/> trojan 0.006310545607626158<br/> bear 0.006244268350438889<br/> heav 0.005479939900136969<br/> thro 0.005185211621694439<br/> shore 0.004618008184651363<br/> fight 0.004161178536600401<br/> turnus 0.003899151842042464<br/> ----------------------------<br/> weight: 0.11671319646716942<br/> TOPIC 2<br/> ------------------------------<br/> aladdin 7.077183389325728E-4<br/> sultan 6.774311890861097E-4<br/> magician 6.127791175835228E-4<br/> genie 6.06094509479989E-4<br/> vizier 6.051618911188781E-4<br/> princess 5.654756758514474E-4<br/> fatima 4.050749957608771E-4<br/> flatland 3.47788388834721E-4<br/> want 3.4263963705536023E-4<br/> spaceland 3.371784715458026E-4<br/> ----------------------------<br/> weight: 0.1219205386824187<br/> TOPIC 3<br/> ------------------------------<br/> aladdin 7.325869707607238E-4<br/> sultan 7.012354862373387E-4<br/> magician 6.343184784726607E-4<br/> genie 6.273921840260785E-4<br/> vizier 6.264266945018852E-4<br/> princess 5.849046214967484E-4<br/> fatima 4.193089052802858E-4<br/> flatland 3.601371993827707E-4<br/> want 3.5398019331108816E-4<br/> spaceland 3.491505202713831E-4<br/> ----------------------------<br/> weight: 0.12730997993615964<br/> TOPIC 4<br/> ------------------------------<br/> captain 0.02931475169407467<br/> fogg 0.02743105575940755<br/> nautilus 0.022748371008515483<br/> passepartout 0.01802140608022664<br/> nemo 0.016678258146358142<br/> conseil 0.012129894049747918<br/> phileas 0.010441664411654412<br/> canadian 0.006217638883315841<br/> vessel 0.00618937301246955<br/> land 0.00615311666365297<br/> ----------------------------<br/> weight: 0.28263550964558276</pre>
<p>从前面的输出中，我们可以看到输入文档的主题5在<kbd>0.28263550964558276</kbd>处最重要。本主题讨论诸如<kbd>captain</kbd>、<kbd>fogg</kbd>、<kbd>nemo</kbd>、<kbd>vessel</kbd>和<kbd>land</kbd>等术语。</p>


            

            
        
    






    
        <title>Step 8 - Measuring the likelihood of two documents</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤8 -测量两个文档的可能性</h1>
                
            
            
                
<p>现在要获得更多的统计数据，比如文档的最大似然或对数似然，我们可以使用下面的代码:</p>
<pre><strong>if</strong> (ldaModel.isInstanceOf[DistributedLDAModel]) {<br/><strong>    val</strong> distLDAModel = ldaModel.asInstanceOf[DistributedLDAModel]<br/><strong>    val</strong> avgLogLikelihood = distLDAModel.logLikelihood / actualCorpusSize.toDouble<br/>    println("The average log likelihood of the training data: " +
avgLogLikelihood)<br/>    println()<br/>}</pre>
<p>前面的代码计算LDA模型的平均对数似然，作为LDA模型的分布式版本的实例:</p>
<pre>The average log likelihood of the training data: -209692.79314860413</pre>
<p>有关可能性测量的更多信息，感兴趣的读者应该参考<a href="https://en.wikipedia.org/wiki/Likelihood_function">https://en.wikipedia.org/wiki/Likelihood_function</a>。</p>
<p>现在假设我们已经为文档X和y计算了前面的度量，那么我们可以回答下面的问题:</p>
<ul>
<li>文档X和Y有多相似？</li>
</ul>
<p>问题是，我们应该尝试从所有的训练文档中获取最低的可能性，并将其作为之前比较的阈值。最后，回答第三个也是最后一个问题:</p>
<ul>
<li>如果我对题目Z感兴趣，应该先看哪些文档？</li>
</ul>
<p>一个简单的答案:仔细看看主题分布和相对术语权重，我们可以决定我们应该首先阅读哪个文档。</p>


            

            
        
    






    
        <title>Other topic models versus the scalability of LDA</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">其他主题模型与LDA的可伸缩性</h1>
                
            
            
                
<p>在这个端到端项目中，我们使用了LDA，它是用于文本挖掘的最流行的TM算法之一。我们可以使用更健壮的TM算法，例如<strong>概率潜在情感分析</strong> ( <strong> pLSA </strong>)、<strong>弹球分配模型</strong> ( <strong> PAM </strong>)和<strong>分级Drichilet过程</strong> ( <strong> HDP </strong>)算法。</p>
<p>然而，pLSA存在过拟合问题。另一方面，HDP和PAM都是用于复杂文本挖掘的更复杂的TM算法，例如从高维文本数据或非结构化文本的文档中挖掘主题。最后，非负矩阵分解是在文档集合中查找主题的另一种方法。不管采用哪种方法，所有TM算法的输出都是带有相关单词簇的主题列表。</p>
<p>前面的示例显示了如何使用LDA算法作为独立的应用程序来执行TM。LDA的并行化并不简单，已经有许多研究论文提出了不同的策略。这方面的主要障碍是所有方法都涉及大量的通信。</p>
<p>根据Databricks网站上的博客(<a href="https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html" target="_blank">https://data bricks . com/blog/2015/03/25/topic-modeling-with-LDA-ml lib-meets-graphx . html</a>)，以下是实验期间使用的数据集和相关训练和测试集的统计数据:</p>
<ul>
<li><strong>训练集规模</strong>:460万文档</li>
<li><strong>词汇量</strong>:110万个词汇</li>
<li><strong>训练集规模</strong>:11亿令牌(~239字/文档)</li>
<li>100个话题</li>
<li>16-worker EC2集群，例如M4 .大型或M3 .中型，具体取决于预算和要求</li>
</ul>
<p>对于前面的设置，10次迭代的计时结果平均为176秒/迭代。从这些统计数据中，很明显LDA对于大量的语料库也是相当可扩展的。</p>


            

            
        
    






    
        <title>Deploying the trained LDA model</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">部署训练好的LDA模型</h1>
                
            
            
                
<p>对于这个迷你部署，让我们使用一个真实的数据集:PubMed。包含PubMed术语的样本数据集可以从:<a href="https://nlp.stanford.edu/software/tmt/tmt-0.4/examples/pubmed-oa-subset.csv">https://NLP . Stanford . edu/software/TMT/TMT-0.4/examples/PubMed-OA-subset . CSV</a>下载。这个链接实际上包含一个CSV格式的数据集，但是有一个奇怪的名字，<kbd>4UK1UkTX.csv</kbd>。</p>
<p>更具体地说，数据集包含一些生物学文章的摘要、它们的出版年份和序列号。下图给出了大概情况:</p>
<div><img height="105" width="448" class="alignnone size-full wp-image-534 image-border" src="img/a19af830-4cd6-496c-84df-93014ea2a1fc.png"/></div>
<p>图6:样本数据集的快照</p>
<p>在下面的代码中，我们已经保存了训练好的LDA模型以供将来使用，如下所示:</p>
<pre>params.ldaModel.save(spark.sparkContext, "model/LDATrainedModel")</pre>
<p>训练好的模型将被保存到前面提到的位置。该目录将包括有关模型和培训本身的数据和元数据，如下图所示:</p>
<div><img height="231" width="404" class="alignnone size-full wp-image-535 image-border" src="img/a4dd1974-8b96-405d-9fcb-37e24e57d6e9.png"/></div>
<p>图7:训练和保存的LDA模型的目录结构</p>
<p>正如预期的那样，data文件夹中有一些parquet文件，其中包含全局主题、它们的计数、令牌和它们的计数，以及主题和它们各自的计数。现在，下一个任务将是恢复相同的模型，如下所示:</p>
<pre class="mce-root">//Restoring the model for reuse<br/><strong>val</strong> savedLDAModel = DistributedLDAModel.load(spark.sparkContext, "model/LDATrainedModel/")<br/><br/>//Then we execute the following workflow:<br/><strong>val</strong> lda = <strong>new</strong> LDAforTM() <br/>// actual computations are done here <br/><br/> // Loading the parameters to train the LDA model <br/><strong>val</strong> defaultParams = Params().copy(input = "data/4UK1UkTX.csv", savedLDAModel)<br/>lda.run(defaultParams) <br/>// Training the LDA model with the default parameters.<br/>spark.stop()</pre>
<pre class="mce-root">&gt;&gt;&gt;<br/> Training corpus summary:<br/> -------------------------------<br/> Training set size: 1 documents<br/> Vocabulary size: 14670 terms<br/> Number of tockens: 14670 tokens<br/> Preprocessing time: 12.921435786 sec<br/> -------------------------------<br/> Finished training LDA model.<br/> Summary:<br/> Training time: 23.243336895 sec<br/> The average log likelihood of the training data: -1008739.37857908<br/> 5 topics:<br/> TOPIC 0<br/> ------------------------------<br/> rrb 0.015234818404037585<br/> lrb 0.015154125349208018<br/> sequence 0.008924621534990771<br/> gene 0.007391453509409655<br/> cell 0.007020265462594214<br/> protein 0.006479622004524878<br/> study 0.004954523307983932<br/> show 0.0040023453035193685<br/> site 0.0038006126784248945<br/> result 0.0036634344941610534<br/> ----------------------------<br/> weight: 0.07662582204885438<br/> TOPIC 1<br/> ------------------------------<br/> rrb 1.745030693927338E-4<br/> lrb 1.7450110447001028E-4<br/> sequence 1.7424254444446083E-4<br/> gene 1.7411236867642102E-4<br/> cell 1.7407234230511066E-4<br/> protein 1.7400587965300172E-4<br/> study 1.737407317498879E-4<br/> show 1.7347354627656383E-4<br/> site 1.7339989737227756E-4<br/> result 1.7334522348574853E-4<br/> ---------------------------<br/> weight: 0.07836521875668061<br/> TOPIC 2<br/> ------------------------------<br/> rrb 1.745030693927338E-4<br/> lrb 1.7450110447001028E-4<br/> sequence 1.7424254444446083E-4<br/> gene 1.7411236867642102E-4<br/> cell 1.7407234230511066E-4<br/> protein 1.7400587965300172E-4<br/> study 1.737407317498879E-4<br/> show 1.7347354627656383E-4<br/> site 1.7339989737227756E-4<br/> result 1.7334522348574853E-4<br/> ----------------------------<br/> weight: 0.08010461546450684<br/> TOPIC 3<br/> ------------------------------<br/> rrb 1.745030693927338E-4<br/> lrb 1.7450110447001028E-4<br/> sequence 1.7424254444446083E-4<br/> gene 1.7411236867642102E-4<br/> cell 1.7407234230511066E-4<br/> protein 1.7400587965300172E-4<br/> study 1.737407317498879E-4<br/> show 1.7347354627656383E-4<br/> site 1.7339989737227756E-4<br/> result 1.7334522348574853E-4<br/> ----------------------------<br/> weight: 0.08184401217233307<br/> TOPIC 4<br/> ------------------------------<br/> rrb 1.745030693927338E-4<br/> lrb 1.7450110447001028E-4<br/> sequence 1.7424254444446083E-4<br/> gene 1.7411236867642102E-4<br/> cell 1.7407234230511066E-4<br/> protein 1.7400587965300172E-4<br/> study 1.737407317498879E-4<br/> show 1.7347354627656383E-4<br/> site 1.7339989737227756E-4<br/> result 1.7334522348574853E-4<br/> ----------------------------<br/> weight: 0.0835834088801593</pre>
<p>干得好！我们成功地重复使用了这个模型，并做了同样的预测。但是，可能由于数据的随机性，我们观察到了一个略有不同的预测。让我们看看完整的代码，以便获得更清晰的视图:</p>
<pre><strong>package</strong> com.packt.ScalaML.Topicmodeling<br/><strong>import</strong> org.apache.spark.sql.SparkSession<br/><strong>import</strong> org.apache.spark.mllib.clustering.{DistributedLDAModel, LDA}<br/><br/><strong>object</strong> LDAModelReuse {<br/><strong>    def</strong> main(args: Array[String]): Unit = {<br/><strong>        val</strong> spark = SparkSession<br/>                    .builder<br/>                    .master("local[*]")<br/>                    .config("spark.sql.warehouse.dir", "data/")<br/>                    .appName(s"LDA_TopicModelling")<br/>                    .getOrCreate()<br/><br/>//Restoring the model for reuse<br/><strong>    val</strong> savedLDAModel = DistributedLDAModel.load(spark.sparkContext, "model/LDATrainedModel/")<br/><strong>    val</strong> lda = <strong>new</strong> LDAforTM() <br/>// actual computations are done here<br/><strong>    val</strong> defaultParams = Params().copy(input = "data/4UK1UkTX.csv", savedLDAModel) <br/>//Loading params <br/>    lda.run(defaultParams) <br/>// Training the LDA model with the default parameters.<br/>    spark.stop()<br/>        }<br/>    }</pre>


            

            
        
    






    
        <title>Summary</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p>在这一章中，我们已经看到了如何有效地使用和结合LDA算法和NLP库，如斯坦福大学的NLP，从大规模文本中寻找有用的模式。我们已经看到了TM算法和LDA的可伸缩能力之间的比较分析。</p>
<p>最后，对于现实生活中的例子和用例，感兴趣的读者可以参考位于<a href="https://blog.codecentric.de/en/2017/01/topic-modeling-codecentric-blog-articles/">https://blog . code centric . de/en/2017/01/topic-modeling-code centric-blog-articles/</a>的博客文章。</p>
<p><strong>网飞</strong>是一家美国娱乐公司，由雷德·哈斯汀斯和马克·伦道夫于1997年8月29日在加利福尼亚州的斯科茨谷创立。它专门从事，提供，流媒体和视频点播，在线和DVD邮寄。2013年，网飞进军电影和电视制作以及在线发行领域。网飞使用基于模型的协同过滤方法为其用户提供实时电影推荐。</p>
<p>在下一章中，我们将看到两个端到端的项目:一个基于项目的<strong>协同过滤</strong>用于电影相似性测量，一个基于模型的电影推荐引擎使用Spark向新用户推荐电影。我们将看到如何在这两个可伸缩的电影推荐引擎的<strong> ALS </strong>和<strong>矩阵分解</strong>之间进行互操作。</p>
<p class="mce-root"/>


            

            
        
    


</body></html>