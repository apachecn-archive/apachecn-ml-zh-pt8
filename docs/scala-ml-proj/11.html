<html><head/><body>


    
        <title>Image Classification using Convolutional Neural Networks</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">基于卷积神经网络的图像分类</h1>
                
            
            
                
<p>到目前为止，我们还没有开发出任何针对图像处理任务的<strong>机器学习</strong> ( <strong> ML </strong>)项目。线性ML模型和其他常规的<strong>深度神经网络</strong> ( <strong> DNN </strong>)模型，如<strong>多层感知器</strong> ( <strong> MLPs </strong>)或<strong>深度信念网络</strong> ( <strong> DBNs </strong>)无法从图像中学习或建模非线性特征。</p>
<p>另一方面，<strong>卷积神经网络</strong> ( <strong> CNN </strong>)是一种前馈神经网络，其中其神经元之间的连接模式受到动物视觉皮层的启发。在过去的几年里，CNN在复杂的视觉任务中表现出了超人的性能，如图像搜索服务、自动驾驶汽车、自动视频分类、语音识别和<strong>自然语言处理</strong> ( <strong> NLP </strong>)。</p>
<p>在这一章中，我们将看到如何使用基于Scala和<strong>deep learning 4j</strong>(<strong>DL4j</strong>)框架的CNN和真实的<strong> Yelp </strong>图像数据集开发一个端到端的项目来处理多标签(即每个实体可以属于多个类别)图像分类问题。我们还将讨论CNN的一些理论方面，以及在开始之前如何调整超参数以获得更好的分类结果。</p>
<p>简而言之，在这个端到端项目中，我们将学习以下主题:</p>
<ul>
<li>常规dnn的缺点</li>
<li>CNN架构:卷积运算和池层</li>
<li>基于细胞神经网络的图像分类</li>
<li>调谐CNN超参数</li>
</ul>


            

            
        
    






    
        <title>Image classification and drawbacks of DNNs</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">图像分类及DNNs的缺陷</h1>
                
            
            
                
<p>在我们开始开发使用CNN进行图像分类的端到端项目之前，我们需要一些背景研究，例如常规DNNs的缺点、CNN相对于DNNs用于图像分类的适用性、CNN的构造、CNN的不同操作等等。虽然常规的dnn对于小图像(例如MNIST、CIFAR-10)工作良好，但是对于较大的图像就不行了，因为它需要大量的参数。例如，一个100 x 100的图像有10000个像素，如果第一层只有1000个神经元(这已经严重限制了传输到下一层的信息量)，这意味着总共有1000万个连接。这只是第一层。</p>
<p>CNN使用部分连接的层来解决这个问题。因为连续层仅部分连接，并且因为它大量重用其权重，所以CNN具有比完全连接的DNN少得多的参数，这使得训练更快，降低了过度拟合的风险，并且需要更少的训练数据。此外，当CNN已经学习了可以检测特定特征的核时，它可以在图像的任何地方检测该特征。相比之下，当DNN在一个位置学习一个特征时，它只能在那个特定的位置检测到它。由于图像通常具有非常重复的特征，对于图像处理任务，例如分类，使用较少的训练样本，CNN能够比DNNs更好地概括。</p>
<p>重要的是，DNN事先不知道像素是如何组织的；它不知道附近的像素是接近的。CNN的架构嵌入了这种先验知识。较低层通常识别图像的小区域中的特征，而较高层将较低层的特征组合成较大的特征。这对于大多数自然图像都很有效，与DNNs相比，这给了CNN决定性的优势:</p>
<div><img src="img/045f23a6-f77a-48ab-8620-85fcdc44fe31.png"/></div>
<p>图1:普通DNN与CNN</p>
<p>比如在<em>图1 </em>中，左边可以看到一个规则的三层神经网络。在右边，一个ConvNet在三维空间(宽度、高度和深度)排列它的神经元，就像在一个层中可视化的那样。ConvNet的每一层都将3D输入体积转换为神经元激活的3D输出体积。红色输入层保存图像，因此它的宽度和高度是图像的尺寸，深度是三个(红色、绿色和蓝色通道)。</p>
<p>因此，我们研究的所有多层神经网络都有由一长串神经元组成的层，我们必须在将输入图像或数据馈送到神经网络之前，将它们展平到1D。然而，一旦你试图直接向他们灌输2D形象，会发生什么呢？答案是，在CNN中，每一层都用2D表示，这样更容易将神经元与其相应的输入匹配起来。我们将在接下来的章节中看到它的例子。</p>
<p>另一个重要的事实是，特征图中的所有神经元共享相同的参数，因此它极大地减少了模型中的参数数量，但更重要的是，这意味着一旦CNN学会了在一个位置识别模式，它就可以在任何其他位置识别它。相比之下，一旦一只普通的DNN学会了在一个位置识别模式，它就只能在那个特定的位置识别它。</p>


            

            
        
    






    
        <title>CNN architecture</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">CNN架构</h1>
                
            
            
                
<p>在多层网络中，例如MLP或DBN，输入层的所有神经元的输出都连接到隐藏层中的每个神经元，因此输出将再次充当全连接层的输入。在CNN网络中，定义卷积层的连接方案明显不同。卷积层是CNN的主要层类型，其中每个神经元都连接到输入区的某个区域，称为<strong>感受野</strong>。</p>
<p>在典型的CNN架构中，几个卷积层以级联方式连接，其中每一层后面是一个<strong>整流线性单元</strong> ( <strong> ReLU </strong>)层，然后是一个汇集层，然后是几个卷积层(+ReLU)，然后是另一个汇集层，依此类推。</p>
<p>每个卷积层的输出是由单个内核过滤器生成的一组称为<strong>特征图</strong>的对象。然后，可以使用特征地图来定义下一层的新输入。CNN网络中的每个神经元产生一个输出，后跟一个激活阈值，该阈值与输入成比例且不受限制:</p>
<div><img height="133" width="512" class="alignnone size-full wp-image-571 image-border" src="img/86c20f01-65a5-4544-a6ee-7fc6355e20e8.png"/></div>
<p>图CNN的概念架构</p>
<p>正如您在<em>图2 </em>中看到的，池层通常位于卷积层之后。卷积区域然后被汇集层分成子区域。然后，使用最大池或平均池技术选择单个代表值，以减少后续层的计算时间。</p>
<p>这样，该特征相对于其空间位置的鲁棒性也得到提高。更具体地说，当作为特征地图的图像属性穿过图像时，它们随着穿过网络而变得越来越小，但是它们也通常变得越来越深，因为将添加更多的特征地图。在堆栈的顶部，添加了一个常规前馈神经网络，就像MLP一样，它可能由几个完全连接的层(+relu)组成，最后一层输出预测，例如，softmax层输出多类分类的估计类概率。</p>


            

            
        
    






    
        <title>Convolutional operations</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">卷积运算</h1>
                
            
            
                
<p>卷积是一种数学运算，它将一个函数滑过另一个函数，并测量它们逐点相乘的积分。它与傅立叶变换和拉普拉斯变换有很深的联系，在信号处理中被大量使用。卷积层实际上使用互相关，这与卷积非常相似。</p>
<p>因此，CNN最重要的组成部分是卷积层:第一个卷积层中的神经元并不连接到输入图像中的每个像素(如前几章所述)，而是只连接到其感受野中的像素——参见<em>图3 </em>。反过来，第二卷积层中的每个神经元仅连接到位于第一层中的小矩形内的神经元:</p>
<div><img height="196" width="334" class="alignnone size-full wp-image-572 image-border" src="img/b2d6b3bc-542d-42ce-9287-6c21eff983ed.png"/></div>
<p>图3:具有矩形局部感受野的CNN层</p>
<p>这种架构允许网络集中于第一个隐藏层中的低级特征，然后将它们组装成下一个隐藏层中的更高级特征，以此类推。这种层次结构在现实世界的图像中很常见，这也是CNN在图像识别方面如此出色的原因之一。</p>


            

            
        
    






    
        <title>Pooling layer and padding operations</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">池层和填充操作</h1>
                
            
            
                
<p>一旦理解了卷积层的工作原理，池层就很容易理解了。池层通常独立作用于每个输入通道，因此输出深度与输入深度相同。您也可以选择在深度维度上进行合并，我们将在下面看到，在这种情况下，图像的空间维度(高度和宽度)保持不变，但通道的数量减少了。让我们来看一个来自著名TensorFlow网站的池化图层的正式定义:</p>
<p>“汇集运算在输入张量上扫描矩形窗口，计算每个窗口的归约运算(平均值、最大值或具有argmax的最大值)。每个池操作使用大小为ksize的矩形窗口，由偏移步幅分隔。例如，如果步幅都是1，则使用每个窗口，如果步幅都是2，则在每个维度中使用每隔一个窗口，以此类推。”</p>
<p>因此，就像在卷积层中一样，汇集层中的每个神经元都连接到位于一个小矩形感受野内的前一层中有限数量的神经元的输出。您必须像前面一样定义它的大小、步幅和填充类型。然而，汇集的神经元没有权重；它所做的只是使用聚合函数(如最大值或平均值)来聚合输入。</p>
<p>使用池的目的是对输入图像进行二次采样，以减少计算量、内存使用量和参数数量。这有助于避免在训练阶段过度适应。减小输入图像尺寸也使得神经网络能够容忍一点点图像偏移。在下面的例子中，我们使用一个2 x 2的池化内核，步幅为2，没有填充。只有每个内核中的最大输入值进入下一层，因为其他输入都被丢弃:</p>
<div><img height="272" width="696" class="alignnone size-full wp-image-573 image-border" src="img/5c2fcbec-0188-4c45-8a7c-54f19f36d0d9.png"/></div>
<p>图4:使用最大池的例子，即二次抽样</p>
<p>通常大多数基于CNN的网络开发推荐使用<em>(stride _ length)* x+filter _ size&lt;= input _ layer _ size</em>。</p>


            

            
        
    






    
        <title>Subsampling operations</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">子采样操作</h1>
                
            
            
                
<p>如前所述，位于给定层的神经元连接到前一层神经元的输出。现在，为了使一个图层与前一个图层具有相同的高度和宽度，通常要在输入周围添加零，如图所示。这叫做<strong>同</strong>或<strong>补零</strong>。</p>
<p>术语“相同”表示输出要素地图与输入要素地图具有相同的空间维度。引入零填充是为了根据需要使形状匹配，在输入贴图的每一侧都相等。另一方面，VALID意味着没有填充，只删除最右边的列(或最下面的行):</p>
<div><img height="337" width="626" class="alignnone size-full wp-image-574 image-border" src="img/dfdd4a04-457d-444c-9299-ae5649690df6.png"/></div>
<p>图5:使用CNN的相同和有效填充</p>
<p>现在我们已经有了关于CNN及其架构的最基本的理论知识，是时候做一些实际工作，并使用Deeplearning4j(又名。DL4j)，这是第一个为Java和Scala编写的商业级分布式开源深度学习库之一。它还提供了对Hadoop和Spark的集成支持。DL4j设计用于分布式GPU和CPU上的业务环境。</p>


            

            
        
    






    
        <title>Convolutional and subsampling operations in DL4j</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">DL4j中的卷积和子采样操作</h1>
                
            
            
                
<p>在开始之前，设置我们的编程环境是一个先决条件。所以让我们先这样做。</p>


            

            
        
    






    
        <title>Configuring DL4j, ND4s, and ND4j</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">配置DL4j、ND4s和ND4j</h1>
                
            
            
                
<p>以下库可以与DL4j集成。无论您是用Java还是Scala开发ML应用程序，它们都将使您的JVM体验更加轻松:</p>
<ul>
<li><strong> DL4j </strong>:神经网络平台</li>
<li><strong>ND4J</strong>:JVM的NumPy</li>
<li><strong>data vec</strong>:ML ETL操作的工具</li>
<li><strong>JavaCPP</strong>:Java和原生C++之间的桥梁</li>
<li><strong>仲裁器</strong>:ML算法的评估工具</li>
<li><strong>RL4J</strong>:JVM的深度强化学习</li>
</ul>
<p>ND4j就像JVM的NumPy一样。它附带了线性代数的一些基本操作，如矩阵创建、加法和乘法。另一方面，ND4S是一个用于线性代数和矩阵操作的科学计算库。基本上，它支持基于JVM的语言的n维数组。</p>
<p>如果您在Eclipse(或任何其他编辑器——即IntelliJ IDEA)上使用Maven，请在<kbd>pom.xml</kbd>文件中(在<kbd>&lt;dependencies&gt;</kbd>标记内)使用以下依赖项来解析DL4j、ND4s和ND4j的依赖项:</p>
<pre>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>    &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;<br/>    &lt;version&gt;0.4-rc3.9&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;artifactId&gt;canova-api&lt;/artifactId&gt;<br/>    &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>    &lt;version&gt;0.4-rc3.9&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>    &lt;artifactId&gt;nd4j-native&lt;/artifactId&gt;<br/>    &lt;version&gt;0.4-rc3.9&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>    &lt;artifactId&gt;canova-api&lt;/artifactId&gt;<br/>    &lt;version&gt;0.0.0.17&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<p>我使用旧版本，因为我面临一些兼容性问题，它仍在积极开发中。但请放心采用最新的升级。相信读者可以做到事半功倍。</p>
<p class="mce-root">此外，如果您的计算机上没有配置本机系统BLAS，ND4j的性能将会降低。一旦执行用Scala编写的简单代码，您将会遇到一个警告:</p>
<pre>****************************************************************<br/>WARNING: COULD NOT LOAD NATIVE SYSTEM BLAS<br/>ND4J performance WILL be reduced<br/>****************************************************************</pre>
<p>但是，安装配置OpenBLAS或IntelMKL等BLAS并没有那么难；你可以投入一些时间去做。详情请参考以下网址:<a href="http://nd4j.org/getstarted.html#open" target="_blank">http://nd4j.org/getstarted.html#open</a>。还应注意，使用DL4j时，以下是先决条件:</p>
<ul>
<li>Java(开发人员版本)1.8+(仅支持64位版本)</li>
<li>用于自动构建和依赖管理器的Apache Maven</li>
<li>IntelliJ IDEA或Eclipse</li>
<li>饭桶</li>
</ul>
<p>干得好！我们的编程环境已经为简单的深度学习应用开发做好了准备。现在是时候使用一些示例代码了。让我们看看如何使用CIFAR-10数据集构建和训练一个简单的CNN。CIFAR-10是最受欢迎的基准数据集之一，拥有成千上万张带标签的图像。</p>


            

            
        
    






    
        <title>Convolutional and subsampling operations in DL4j</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">DL4j中的卷积和子采样操作</h1>
                
            
            
                
<p>在这一小节中，我们将看到一个如何为MNIST数据分类构建CNN的例子。该网络将具有两个卷积层、两个子采样层、一个密集层以及作为全连接层的输出层。第一层是卷积层，后面是二次采样层，再后面是另一个卷积层。然后，二次采样层之后是密集层，密集层之后是输出层。</p>
<p>让我们看看使用DL4j时这些层会是什么样子。以ReLU为激活函数的第一卷积层:</p>
<pre><strong>val</strong> layer_0 = <strong>new</strong> ConvolutionLayer.Builder(5, 5)<br/>    .nIn(nChannels)<br/>    .stride(1, 1)<br/>    .nOut(20)<br/>    .activation("relu")<br/>    .build()</pre>
<p>DL4j目前支持以下激活功能:</p>
<ul>
<li>热卢</li>
<li>泄漏ReLU</li>
<li>双曲正切</li>
<li>乙状结肠的</li>
<li>硬鞣</li>
<li>Softmax</li>
<li>身份</li>
<li><strong> ELU </strong> ( <strong>指数线性单位</strong>)</li>
<li>软设计</li>
<li>Softplus</li>
</ul>
<p>第二层(即第一子采样层)是具有池类型<kbd>MAX</kbd>的子采样层，具有2×2的内核大小和2×2的步幅大小，但是没有激活函数:</p>
<pre><strong>val</strong> layer_1 = <strong>new</strong> SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>    .kernelSize(2, 2)<br/>    .stride(2, 2)<br/>    .build()</pre>
<p>第三层(第二卷积层)是以ReLU为激活函数的卷积层，1*1步距:</p>
<pre><br/><strong>val</strong> layer_2 = <strong>new</strong> ConvolutionLayer.Builder(5, 5)<br/>    .nIn(nChannels)<br/>    .stride(1, 1)<br/>    .nOut(50)<br/>    .activation("relu")<br/>    .build()</pre>
<p>第四层(即第二个子采样层)是具有池类型<kbd>MAX</kbd>的子采样层，具有2×2的内核大小和2×2的步长大小，但是没有激活函数:</p>
<pre><strong>val</strong> layer_3 = <strong>new</strong> SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>    .kernelSize(2, 2)<br/>    .stride(2, 2)<br/>    .build()</pre>
<p>第五层是具有ReLU作为激活函数的密集层:</p>
<pre><strong>val</strong> layer_4 = <strong>new</strong> DenseLayer.Builder()<br/>    .activation("relu")<br/>    .nOut(500)<br/>    .build()</pre>
<p>第六层(即，最终且完全连接的层)将Softmax作为激活函数，其具有要预测的类的数量(即，10):</p>
<pre><strong>val</strong> layer_5 = <strong>new</strong> OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)<br/>    .nOut(outputNum)<br/>    .activation("softmax")<br/>    .build()</pre>
<p>一旦构建了这些层，下一个任务就是通过链接所有层来构建CNN。使用DL4j，过程如下:</p>
<pre><strong>val</strong> builder: MultiLayerConfiguration.Builder = <strong>new</strong> NeuralNetConfiguration.Builder()<br/>    .seed(seed)<br/>    .iterations(iterations)<br/>    .regularization(<strong>true</strong>).l2(0.0005)<br/>    .learningRate(0.01)<br/>    .weightInit(WeightInit.XAVIER)<br/>   .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)<br/>    .updater(Updater.NESTEROVS).momentum(0.9)<br/>    .list()<br/>        .layer(0, layer_0)<br/>        .layer(1, layer_1)<br/>        .layer(2, layer_2)<br/>        .layer(3, layer_3)<br/>        .layer(4, layer_4)<br/>        .layer(5, layer_5)<br/>    .backprop(<strong>true</strong>).pretrain(<strong>false</strong>) // feedforward and supervised so no pretraining</pre>
<p>最后，我们设置所有卷积层，并按如下方式初始化网络:</p>
<pre><strong>new</strong> ConvolutionLayerSetup(builder, 28, 28, 1) //image size is 28*28<br/><strong>val</strong> conf: MultiLayerConfiguration = builder.build()<br/><strong>val</strong> model: MultiLayerNetwork = <strong>new</strong> MultiLayerNetwork(conf)<br/>model.init()</pre>
<p>传统上，为了训练CNN，所有的图像需要具有相同的形状和大小。因此，为了简单起见，我在前面几行中将尺寸设为28 x 28。现在，你可能在想，我们如何训练这样一个网络？好了，现在我们将看到这一点，但在此之前，我们需要准备MNIST数据集，使用<kbd>MnistDataSetIterator ()</kbd>方法，如下所示:</p>
<pre><strong>val</strong> nChannels = 1 // for grayscale image<br/><strong>val</strong> outputNum = 10 // number of class<br/><strong>val</strong> nEpochs = 10 // number of epoch<br/><strong>val</strong> iterations = 1 // number of iteration<br/><strong>val</strong> seed = 12345 // Random seed for reproducibility<br/><strong>val</strong> batchSize = 64 // number of batches to be sent<br/>log.info("Load data....")<br/><strong>val</strong> mnistTrain: DataSetIterator = <strong>new</strong> MnistDataSetIterator(batchSize, <strong>true</strong>, 12345)<br/><strong>val</strong> mnistTest: DataSetIterator = <strong>new</strong> MnistDataSetIterator(batchSize, <strong>false</strong>, 12345)</pre>
<p>现在让我们开始训练CNN，使用训练集并对每个时期进行迭代:</p>
<pre>log.info("Model training started...")<br/>model.setListeners(<strong>new</strong> ScoreIterationListener(1))<br/><strong>var</strong> i = 0<br/><strong>while</strong> (i &lt;= nEpochs) {<br/>    model.fit(mnistTrain);<br/>    log.info("*** Completed epoch {} ***", i)<br/>    i = i + 1<br/>    }<br/><strong>var</strong> ds: DataSet = <strong>null<br/></strong><strong>var</strong> output: INDArray = <strong>null</strong></pre>
<p>一旦我们训练了CNN，下一个任务是在测试集上评估模型，如下所示:</p>
<pre>log.info("Model evaluation....")<br/><strong>val</strong> eval: Evaluation = <strong>new</strong> Evaluation(outputNum)<br/><strong>while</strong> (mnistTest.hasNext()) {<br/>    ds = mnistTest.next()<br/>    output = model.output(ds.getFeatureMatrix(), <strong>false</strong>)<br/>    }<br/>eval.eval(ds.getLabels(), output)</pre>
<p>最后，我们计算一些性能矩阵，如<kbd>Accuracy</kbd>、<kbd>Precision</kbd>、<kbd>Recall</kbd>和<kbd>F1 measure</kbd>，如下所示:</p>
<pre>println("Accuracy: " + eval.accuracy())<br/>println("F1 measure: " + eval.f1())<br/>println("Precision: " + eval.precision())<br/>println("Recall: " + eval.recall())<br/>println("Confusion matrix: " + "n" + eval.confusionToString())<br/>log.info(eval.stats())<br/>mnistTest.reset()<br/>&gt;&gt;&gt;<br/>==========================Scores=======================================<br/> Accuracy: 1<br/> Precision: 1<br/> Recall: 1<br/> F1 Score: 1<br/>=======================================================================</pre>
<p>为了方便起见，我在这里提供了这个简单图像分类器的完整源代码:</p>
<pre><strong>package</strong> com.example.CIFAR<br/><br/><strong>import</strong> org.canova.api.records.reader.RecordReader<br/><strong>import</strong> org.canova.api.split.FileSplit<br/><strong>import</strong> org.canova.image.loader.BaseImageLoader<br/><strong>import</strong> org.canova.image.loader.NativeImageLoader<br/><strong>import</strong> org.canova.image.recordreader.ImageRecordReader<br/><strong>import</strong> org.deeplearning4j.datasets.iterator.DataSetIterator<br/><strong>import</strong> org.canova.image.recordreader.ImageRecordReader<br/><strong>import</strong> org.deeplearning4j.datasets.canova.RecordReaderDataSetIterator<br/><strong>import</strong> org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator<br/><strong>import</strong> org.deeplearning4j.eval.Evaluation<br/><strong>import</strong> org.deeplearning4j.nn.api.OptimizationAlgorithm<br/><strong>import</strong> org.deeplearning4j.nn.conf.MultiLayerConfiguration<br/><strong>import</strong> org.deeplearning4j.nn.conf.NeuralNetConfiguration<br/><strong>import</strong> org.deeplearning4j.nn.conf.Updater<br/><strong>import</strong> org.deeplearning4j.nn.conf.layers.ConvolutionLayer<br/><strong>import</strong> org.deeplearning4j.nn.conf.layers.DenseLayer<br/><strong>import</strong> org.deeplearning4j.nn.conf.layers.OutputLayer<br/><strong>import</strong> org.deeplearning4j.nn.conf.layers.SubsamplingLayer<br/><strong>import</strong> org.deeplearning4j.nn.conf.layers.setup.ConvolutionLayerSetup<br/><strong>import</strong> org.deeplearning4j.nn.multilayer.MultiLayerNetwork<br/><strong>import</strong> org.deeplearning4j.nn.weights.WeightInit<br/><strong>import</strong> org.deeplearning4j.optimize.listeners.ScoreIterationListener<br/><strong>import</strong> org.nd4j.linalg.api.ndarray.INDArray<br/><strong>import</strong> org.nd4j.linalg.api.rng.Random<br/><strong>import</strong> org.nd4j.linalg.dataset.DataSet<br/><strong>import</strong> org.nd4j.linalg.dataset.SplitTestAndTrain<br/><strong>import</strong> org.nd4j.linalg.lossfunctions.LossFunctions<br/><strong>import</strong> org.slf4j.Logger<br/><strong>import</strong> org.slf4j.LoggerFactory<br/><strong>import</strong> java.io.File<br/><strong>import</strong> java.util.ArrayList<br/><strong>import</strong> java.util.List<br/><br/><strong>object</strong> MNIST {<br/><strong>    val</strong> log: Logger = LoggerFactory.getLogger(MNIST.getClass)<br/><strong>    def</strong> main(args: Array[String]): Unit = {<br/><strong>    val</strong> nChannels = 1 // for grayscale image<br/><strong>    val</strong> outputNum = 10 // number of class<br/><strong>    val</strong> nEpochs = 1 // number of epoch<br/><strong>    val</strong> iterations = 1 // number of iteration<br/><strong>    val</strong> seed = 12345 // Random seed for reproducibility<br/><strong>    val</strong> batchSize = 64 // number of batches to be sent<br/><br/>    log.info("Load data....")<br/><strong>    val</strong> mnistTrain: DataSetIterator = <strong>new</strong> MnistDataSetIterator(batchSize, <strong>true</strong>, 12345)<br/><strong>    val</strong> mnistTest: DataSetIterator = <strong>new</strong> MnistDataSetIterator(batchSize, <strong>false</strong>, 12345)<br/><br/>    log.info("Network layer construction started...")<br/>    //First convolution layer with ReLU as activation function<br/><strong>    val</strong> layer_0 = <strong>new</strong> ConvolutionLayer.Builder(5, 5)<br/>        .nIn(nChannels)<br/>        .stride(1, 1)<br/>        .nOut(20)<br/>        .activation("relu")<br/>        .build()<br/><br/>    //First subsampling layer<br/><strong>    val</strong> layer_1 = <strong>new</strong> SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>        .kernelSize(2, 2)<br/>        .stride(2, 2)<br/>        .build()<br/><br/>    //Second convolution layer with ReLU as activation function<br/><strong>    val</strong> layer_2 = <strong>new</strong> ConvolutionLayer.Builder(5, 5)<br/>        .nIn(nChannels)<br/>        .stride(1, 1)<br/>        .nOut(50)<br/>        .activation("relu")<br/>        .build()<br/><br/>    //Second subsampling layer<br/><strong>    val</strong> layer_3 = <strong>new</strong> SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>        .kernelSize(2, 2)<br/>        .stride(2, 2)<br/>        .build()<br/><br/>    //Dense layer<br/><strong>    val</strong> layer_4 = <strong>new</strong> DenseLayer.Builder()<br/>        .activation("relu")<br/>        .nOut(500)<br/>        .build()<br/><br/>    // Final and fully connected layer with Softmax as activation function<br/><strong>    val</strong> layer_5 = <strong>new</strong> OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)<br/>        .nOut(outputNum)<br/>        .activation("softmax")<br/>        .build()<br/><br/>    log.info("Model building started...")<br/><strong>    val</strong> builder: MultiLayerConfiguration.Builder = <strong>new</strong> NeuralNetConfiguration.Builder()<br/>        .seed(seed)<br/>        .iterations(iterations)<br/>        .regularization(<strong>true</strong>).l2(0.0005)<br/>        .learningRate(0.01)<br/>        .weightInit(WeightInit.XAVIER)<br/>        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)<br/>        .updater(Updater.NESTEROVS).momentum(0.9)<br/>        .list()<br/>            .layer(0, layer_0)<br/>            .layer(1, layer_1)<br/>            .layer(2, layer_2)<br/>            .layer(3, layer_3)<br/>            .layer(4, layer_4)<br/>            .layer(5, layer_5)<br/>    .backprop(<strong>true</strong>).pretrain(<strong>false</strong>) // feedforward so no backprop<br/><br/>// Setting up all the convlutional layers and initialize the network<br/><strong>new</strong> ConvolutionLayerSetup(builder, 28, 28, 1) //image size is 28*28<br/><strong>val</strong> conf: MultiLayerConfiguration = builder.build()<br/><strong>val</strong> model: MultiLayerNetwork = <strong>new</strong> MultiLayerNetwork(conf)<br/>model.init()<br/><br/>log.info("Model training started...")<br/>model.setListeners(<strong>new</strong> ScoreIterationListener(1))<br/><strong>    var</strong> i = 0<br/><strong>    while</strong> (i &lt;= nEpochs) {<br/>        model.fit(mnistTrain);<br/>        log.info("*** Completed epoch {} ***", i)<br/>        i = i + 1<br/><strong>        var</strong> ds: DataSet = <strong>null<br/></strong><strong>        var</strong> output: INDArray = <strong>null<br/></strong>        log.info("Model evaluation....")<br/><strong>        val</strong> eval: Evaluation = <strong>new</strong> Evaluation(outputNum)<br/><br/><strong>        while</strong> (mnistTest.hasNext()) {<br/>            ds = mnistTest.next()<br/>            output = model.output(ds.getFeatureMatrix(), <strong>false</strong>)<br/>                }<br/>        eval.eval(ds.getLabels(), output)<br/>        println("Accuracy: " + eval.accuracy())<br/>        println("F1 measure: " + eval.f1())<br/>        println("Precision: " + eval.precision())<br/>        println("Recall: " + eval.recall())<br/>        println("Confusion matrix: " + "n" + eval.confusionToString())<br/>        log.info(eval.stats())<br/>        mnistTest.reset()<br/>                }<br/>    log.info("****************Example finished********************")<br/>            }<br/>    }</pre>


            

            
        
    






    
        <title>Large-scale image classification using CNN</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">基于CNN的大规模图像分类</h1>
                
            
            
                
<p>在本节中，我们将展示一个为图像分类开发真实ML项目的分步示例。然而，我们首先需要知道问题的描述，了解需要做什么样的图像分类。此外，在开始之前，必须了解数据集。</p>


            

            
        
    






    
        <title>Problem description</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">问题描述</h1>
                
            
            
                
<p>如今，食物自拍和以照片为中心的社交故事正在成为社交趋势。美食爱好者愿意将大量与食物和餐厅照片一起拍摄的自拍照上传到社交媒体和各自的网站。当然，他们还会提供一份书面评论，大大提升餐厅的知名度:</p>
<div><img height="304" width="464" class="alignnone size-full wp-image-575 image-border" src="img/454a5472-e1c8-4bfe-823d-5161d6510316.png"/></div>
<p>图6:从Yelp数据集中挖掘一些商业见解</p>
<p>例如，数百万独立访问者访问Yelp，并撰写了超过1.35亿条评论。有很多照片和很多上传照片的用户。企业主可以张贴照片并给他们的顾客发信息。通过这种方式，Yelp通过向当地企业出售广告来赚钱。一个有趣的事实是，这些照片提供了丰富的跨类别的当地商业信息。因此，训练计算机理解这些照片的背景不是一件小事，也不是一件容易的任务(参见<em>图6 </em>以获得洞察力)。</p>
<p>现在，这个项目的想法是一个挑战:我们如何把这些图片变成文字？让我们试一试。更具体地说，给你的照片属于一个企业。现在我们需要建立一个模型，这样它就可以用用户提交的照片的多个标签自动标记餐馆——也就是说，预测业务属性。</p>


            

            
        
    






    
        <title>Description of the image dataset</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">图像数据集的描述</h1>
                
            
            
                
<p>面对这样的挑战，我们需要一个真实的数据集。不要担心，有几个平台公开提供这样的数据集，或者可以根据一些条款和条件下载。一个这样的平台是<strong> Kaggle </strong>，它为数据分析和ML从业者提供了一个尝试ML挑战并赢得奖品的平台。Yelp数据集和描述可以在:<a href="https://www.kaggle.com/c/yelp-restaurant-photo-classification">https://www . ka ggle . com/c/Yelp-restaurant-photo-classification</a>找到。</p>
<p>Yelp用户在提交评论时会手动选择餐厅的标签。数据集中关联的Yelp社区标注了九个不同的标签:</p>
<ul>
<li><kbd>0: good_for_lunch</kbd></li>
<li><kbd>1: good_for_dinner</kbd></li>
<li><kbd>2: takes_reservations</kbd></li>
<li><kbd>3: outdoor_seating</kbd></li>
<li><kbd>4: restaurant_is_expensive</kbd></li>
<li><kbd>5: has_alcohol</kbd></li>
<li><kbd>6: has_table_service</kbd></li>
<li><kbd>7: ambience_is_classy</kbd></li>
<li><kbd>8: good_for_kids</kbd></li>
</ul>
<p>所以我们需要尽可能准确地预测这些标签。需要注意的一点是，由于Yelp是一个社区驱动的网站，由于多种原因，数据集中有重复的图像。例如，用户可能会不小心将同一张照片多次上传到同一家企业，或者连锁企业可能会将同一张照片上传到不同的分店。数据集中有六个文件，如下所示:</p>
<ul>
<li><kbd>train_photos.tgz</kbd>:用作训练集的照片(234545张)</li>
<li><kbd>test_photos.tgz</kbd>:作为测试集的照片(500张)</li>
<li><kbd>train_photo_to_biz_ids.csv</kbd>:提供照片ID到业务ID的映射(234，545行)</li>
<li><kbd>test_photo_to_biz_ids.csv</kbd>:提供照片ID到业务ID的映射(500行)</li>
<li><kbd>train.csv</kbd>:这是主要的训练数据集，包括业务id及其相应的标签(1996行)</li>
<li><kbd>sample_submission.csv</kbd>:样本提交—参考您的预测的正确格式，包括<kbd>business_id</kbd>和相应的预测标签</li>
</ul>


            

            
        
    






    
        <title>Workflow of the overall project</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">整个项目的工作流程</h1>
                
            
            
                
<p>在这个项目中，我们将看到如何在Scala中将图片从<kbd>.jpg</kbd>格式读入矩阵表示。然后，我们将进一步处理和准备CNN提供的图像。在我们对图像应用灰度滤镜之前，我们将看到一些图像操作，例如将所有图像平方并将每个图像调整到相同的尺寸:</p>
<div><img height="217" width="685" class="alignnone size-full wp-image-576 image-border" src="img/1501815a-0ebd-4b08-a671-19a7990e0bc4.png"/></div>
<p>图7:用于图像分类CNN的概念化视图</p>
<p>然后我们用每个班的训练数据训练9个CNN。训练完成后，我们保存训练好的模型、CNN配置和参数，以便以后可以恢复它们，然后我们应用一个简单的聚合函数来为每个餐馆分配类别，其中每个餐馆都有多个相关联的图像，每个图像都有自己的九个类别的概率向量。然后，我们对测试数据进行评分，最后，使用测试图像对模型进行评估。</p>
<p>现在我们来看看每个CNN的结构。每个网络都有两个卷积层、两个子采样层、一个密集层和作为全连接层的输出层。第一层是卷积层，后面是二次采样层，再后面是另一个卷积层，然后是二次采样层，然后是密集层，再后面是输出层。稍后我们将看到每一层的结构。</p>


            

            
        
    






    
        <title>Implementing CNNs for image classification</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">实现用于图像分类的细胞神经网络</h1>
                
            
            
                
<p>包含<kbd>main()</kbd>方法的Scala对象有以下工作流程:</p>
<ol>
<li>我们从<kbd>train.csv</kbd>文件中读取所有的商业标签</li>
<li>我们读取并创建一个从图片ID到业务ID的映射，格式为<kbd>imageID</kbd> → <kbd>busID</kbd></li>
<li>我们从<kbd>photoDir</kbd>目录中获取一个要加载和处理的图像列表，最后，获取10，000个图像的图像id(随意设置范围)</li>
<li>然后，我们读取图像并将其处理成<kbd>photoID</kbd> →矢量图</li>
<li>我们将<em>步骤3 </em>和<em>步骤4 </em>的输出链接起来，以对齐业务特征、图像id和标签id，从而为CNN提取特征</li>
<li>我们构建了九个CNN。</li>
<li>我们训练所有的CNN并指定模型保存位置</li>
<li>然后我们重复<em>步骤2 </em>到<em>步骤6 </em>从测试集中提取特征</li>
<li>最后，我们评估模型并将预测保存在CSV文件中</li>
</ol>
<p>现在，让我们看看前面的步骤在高级图表中会是什么样子:</p>
<div><img height="282" width="444" class="alignnone size-full wp-image-577 image-border" src="img/ffa6d1ab-e72d-4d69-b4b2-a358679a7ed5.png"/></div>
<p>图8:用于图像分类的DL4j图像处理管道</p>
<p>以编程方式，前面的步骤可以表示如下:</p>
<pre><strong>val</strong> labelMap = readBusinessLabels("data/labels/train.csv")<br/><strong>val</strong> businessMap = readBusinessToImageLabels("data/labels/train_photo_to_biz_ids.csv")<br/><strong>val</strong> imgs = getImageIds("data/images/train/", businessMap, businessMap.map(_._2).toSet.toList).slice(0,100) // 20000 images<br/><br/>println("Image ID retreival done!")<br/><strong>val</strong> dataMap = processImages(imgs, resizeImgDim = 128)<br/>println("Image processing done!")<br/><strong>val</strong> alignedData = <strong>new</strong> featureAndDataAligner(dataMap, businessMap, Option(labelMap))()<br/><br/>println("Feature extraction done!")<br/><strong>val</strong> cnn0 = trainModelEpochs(alignedData, businessClass = 0, saveNN = "models/model0")<br/><strong>val</strong> cnn1 = trainModelEpochs(alignedData, businessClass = 1, saveNN = "models/model1")<br/><strong>val</strong> cnn2 = trainModelEpochs(alignedData, businessClass = 2, saveNN = "models/model2")<br/><strong>val</strong> cnn3 = trainModelEpochs(alignedData, businessClass = 3, saveNN = "models/model3")<br/><strong>val</strong> cnn4 = trainModelEpochs(alignedData, businessClass = 4, saveNN = "models/model4")<br/><strong>val</strong> cnn5 = trainModelEpochs(alignedData, businessClass = 5, saveNN = "models/model5")<br/><strong>val</strong> cnn6 = trainModelEpochs(alignedData, businessClass = 6, saveNN = "models/model6")<br/><strong>val</strong> cnn7 = trainModelEpochs(alignedData, businessClass = 7, saveNN = "models/model7")<br/><strong>val</strong> cnn8 = trainModelEpochs(alignedData, businessClass = 8, saveNN = "models/model8")<br/><br/><strong>val</strong> businessMapTE = readBusinessToImageLabels("data/labels/test_photo_to_biz.csv")<br/><br/><strong>val</strong> imgsTE = getImageIds("data/images/test//", businessMapTE, businessMapTE.map(_._2).toSet.toList)<br/><br/><strong>val</strong> dataMapTE = processImages(imgsTE, resizeImgDim = 128) // make them 128*128<br/><br/><strong>val</strong> alignedDataTE = <strong>new</strong> featureAndDataAligner(dataMapTE, businessMapTE, None)()<br/><strong>val</strong> Results = SubmitObj(alignedDataTE, "results/ModelsV0/")<br/><strong>val</strong> SubmitResults = writeSubmissionFile("kaggleSubmitFile.csv", Results, thresh = 0.9)</pre>
<p>太多了吗？不要担心，我们现在将详细了解每个步骤。如果你仔细观察前面的步骤，你会看到<em>步骤1 </em>到<em>步骤5 </em>基本上都是图像处理和特征构造。</p>


            

            
        
    






    
        <title>Image processing</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">图像处理</h1>
                
            
            
                
<p class="mce-root">当我试图开发这个应用程序时，我发现照片的大小和形状都不一样:有些图像很高，有些很宽，有些在外面，有些在里面，大多数都是食物的图片。然而，有些也是其他随机的事情。另一个重要的方面是，虽然训练图像在纵向/横向和像素数量方面有所不同，但大多数图像大致都是方形的，其中许多图像的大小正好是500 x 375:</p>
<div><img height="170" width="292" class="alignnone size-full wp-image-578 image-border" src="img/40a41e7c-baad-4e63-b049-12851134ce52.png"/></div>
<p>图9:调整后的图(左边是原来的高图，右边是方形图)</p>
<p class="mce-root">正如我们已经看到的，CNN不能处理不同大小和形状的图像。有许多鲁棒且有效的图像处理技术来仅提取感兴趣的<strong>区域</strong> ( <strong> ROI </strong>)。但是，老实说，我不是图像处理专家，所以我决定保持这个调整大小的步骤更简单。</p>
<p>CNN有严重的局限性，因为它不能处理方位和相对空间关系。因此，这些组件对于CNN来说并不重要。简而言之，CNN不太适合具有不同形状和方向的图像。为什么，人们现在在谈论胶囊网络。在<a href="https://arxiv.org/pdf/1710.09829v1.pdf">https://arxiv.org/pdf/1710.09829v1.pdf</a>和<a href="https://openreview.net/pdf?id=HJWLfGWRb">https://openreview.net/pdf?id=HJWLfGWRb</a>查看更多原文。</p>
<p class="mce-root">天真地，我把所有的图像都做成方形，但我仍然试图保持质量。在大多数情况下，感兴趣区域位于中心，因此只捕捉每幅图像最中心的正方形并不那么简单。然而，我们还需要将每张图像转换成灰度图像。让我们把不规则形状的图像做成正方形。看看下图，左边是原图，右边是方形图(见<em>图9) </em>。</p>
<p>现在我们已经生成了一个正方形，我们是如何实现的呢？嗯，我首先检查了高度和宽度是否相同，如果是这样，就不会调整大小。在另外两个例子中，我裁剪了中心区域。下面的方法实现了这个技巧(但是可以随意执行<kbd>SquaringImage.scala</kbd>脚本来查看输出):</p>
<pre><strong>def</strong> makeSquare(img: java.awt.image.BufferedImage): java.awt.image.BufferedImage = {<br/><strong>    val</strong> w = img.getWidth<br/><strong>    val</strong> h = img.getHeight<br/><strong>    val</strong> dim = List(w, h).min<br/>    img <strong>match</strong> {<br/><strong>        case</strong> x <br/><strong>            if</strong> w == h =&gt; img // do nothing and returns the original one<br/><strong>        case</strong> x <br/><strong>            if</strong> w &gt; h =&gt; Scalr.crop(img, (w - h) / 2, 0, dim, dim)<br/><strong>        case</strong> x <br/><strong>            if</strong> w &lt; h =&gt; Scalr.crop(img, 0, (h - w) / 2, dim, dim)<br/>        }<br/>    }</pre>
<p>干得好！既然我们所有的训练图像都是正方形的，下一个导入预处理任务就是调整它们的大小。我决定让所有的图像大小为128 x 128。让我们看看调整大小后前一个(原始)是什么样子:</p>
<div><img height="145" width="219" class="alignnone size-full wp-image-579 image-border" src="img/a529084c-1479-4008-ac6b-37c1de1bebb7.png"/></div>
<p>图10:调整图像大小(分别为256 x 256、128 x 128、64 x 64和32 x 32)</p>
<p>下面的方法成功了(但是可以随意执行<kbd>ImageResize.scala</kbd>脚本来观看演示):</p>
<pre><strong>def</strong> resizeImg(img: java.awt.image.BufferedImage, width: Int, height: Int) = {<br/>    Scalr.resize(img, Scalr.Method.BALANCED, width, height) <br/>}</pre>
<p>顺便说一下，对于图像大小调整和平方，我使用了一些内置的包来读取图像，使用了一些第三方的包来处理:</p>
<pre><strong>import</strong> org.imgscalr._<br/><strong>import</strong> java.io.File<br/><strong>import</strong> javax.imageio.ImageIO</pre>
<p>要使用前面的包，请在一个Maven友好的<kbd>pom.xml</kbd>文件中添加以下依赖项:</p>
<pre>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.imgscalr&lt;/groupId&gt;<br/>    &lt;artifactId&gt;imgscalr-lib&lt;/artifactId&gt;<br/>    &lt;version&gt;4.2&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.datavec&lt;/groupId&gt;<br/>    &lt;artifactId&gt;datavec-data-image&lt;/artifactId&gt;<br/>    &lt;version&gt;0.9.1&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;com.sksamuel.scrimage&lt;/groupId&gt;<br/>    &lt;artifactId&gt;scrimage-core_2.10&lt;/artifactId&gt;<br/>    &lt;version&gt;2.1.0&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<p>虽然基于DL4j的CNN可以处理彩色图像，但最好用灰度图像来简化计算。虽然彩色图像更令人兴奋和有效，但这样我们可以使整体表现更简单，空间更有效。</p>
<p>让我们举一个上一步的例子。我们将每幅图像的大小调整为256 x 256像素的图像，由16，384个特征表示，而不是16，384 x 3的具有三个RGB通道的彩色图像(执行<kbd>GrayscaleConverter.scala</kbd>查看演示)。让我们看看转换后的图像会是什么样子:</p>
<div><img height="268" width="481" class="alignnone size-full wp-image-580 image-border" src="img/5c109324-5f47-4bf4-9aec-c9007d184095.png"/></div>
<p>图11:左为原始图像，右为灰度RGB平均值</p>
<p>前面的转换使用两种方法完成，称为<kbd>pixels2Gray()</kbd>和<kbd>makeGray()</kbd>:</p>
<pre><strong>def</strong> pixels2Gray(R: Int, G: Int, B: Int): Int = (R + G + B) / 3<br/><strong>def</strong> makeGray(testImage: java.awt.image.BufferedImage): java.awt.image.BufferedImage = {<br/><strong>    val</strong> w = testImage.getWidth<br/><strong>    val</strong> h = testImage.getHeight<br/><strong>        for</strong> { <br/>        w1 &lt;- (0 until w).toVector<br/>        h1 &lt;- (0 until h).toVector<br/>        } <br/><strong>    yield</strong> <br/>    {<br/><strong>    val</strong> col = testImage.getRGB(w1, h1)<br/><strong>    val</strong> R = (col &amp; 0xff0000) / 65536<br/><strong>    val</strong> G = (col &amp; 0xff00) / 256<br/><strong>    val</strong> B = (col &amp; 0xff)<br/><strong>    val</strong> graycol = pixels2Gray(R, G, B)<br/>testImage.setRGB(w1, h1, <strong>new</strong> Color(graycol, graycol, graycol).getRGB)<br/>    }<br/>testImage<br/>}</pre>
<p>引擎盖下发生了什么？我们将前面的三个步骤串联起来:将所有的图像变成正方形，然后将它们全部转换为25 x 256，最后将调整大小后的图像转换为灰度图像:</p>
<pre><strong>val</strong> demoImage = ImageIO.read(<strong>new</strong> File(x))<br/>    .makeSquare<br/>    .resizeImg(resizeImgDim, resizeImgDim) // (128, 128)<br/>    .image2gray</pre>
<p>因此，总的来说，我们现在有所有的图像在平方和调整大小后都是灰色的。下图给出了转换步骤的一些概念:</p>
<div><img height="192" width="481" class="alignnone size-full wp-image-581 image-border" src="img/4f8c2de5-bfa6-4582-871b-690811daae45.png"/></div>
<p>图12:调整后的图形(左边是原来的高的，右边是方形的)</p>
<p>下面的链接也需要一些额外的努力。现在，我们将这三个步骤放在代码中，最终可以准备好所有的图像:</p>
<pre><strong>import</strong> scala.Vector<br/><strong>import</strong> org.imgscalr._<br/><br/><strong>object</strong> imageUtils {<br/><strong>    implicit</strong><strong>class</strong> imageProcessingPipeline(img: java.awt.image.BufferedImage) {<br/>    // image 2 vector processing<br/><strong>    def</strong> pixels2gray(R: Int, G:Int, B: Int): Int = (R + G + B) / 3<br/><strong>    def</strong> pixels2color(R: Int, G:Int, B: Int): Vector[Int] = Vector(R, G, B)<br/><strong>    private </strong><strong>def</strong> image2vec[A](f: (Int, Int, Int) =&gt; A ): Vector[A] = {<br/><strong>        val</strong> w = img.getWidth<br/><strong>        val</strong> h = img.getHeight<br/><strong>        for</strong> {<br/>            w1 &lt;- (0 until w).toVector<br/>            h1 &lt;- (0 until h).toVector<br/>            } <br/><strong>        yield</strong> {<br/><strong>            val</strong> col = img.getRGB(w1, h1)<br/><strong>            val</strong> R = (col &amp; 0xff0000) / 65536<br/><strong>            val</strong> G = (col &amp; 0xff00) / 256<br/><strong>            val</strong> B = (col &amp; 0xff)<br/>        f(R, G, B)<br/>                }<br/>            }<br/><br/><strong>    def</strong> image2gray: Vector[Int] = image2vec(pixels2gray)<br/><strong>    def</strong> image2color: Vector[Int] = image2vec(pixels2color).flatten<br/><br/>    // make image square<br/><strong>    def</strong> makeSquare = {<br/><strong>        val</strong> w = img.getWidth<br/><strong>        val</strong> h = img.getHeight<br/><strong>        val</strong> dim = List(w, h).min<br/>        img <strong>match</strong> {<br/><strong>            case</strong> x     <br/><strong>                if</strong> w == h =&gt; img<br/><strong>            case</strong> x <br/><strong>                if</strong> w &gt; h =&gt; Scalr.crop(img, (w-h)/2, 0, dim, dim)<br/><strong>            case</strong> x <br/><strong>                if</strong> w &lt; h =&gt; Scalr.crop(img, 0, (h-w)/2, dim, dim)<br/>              }<br/>            }<br/><br/>    // resize pixels<br/><strong>    def</strong> resizeImg(width: Int, height: Int) = {<br/>        Scalr.resize(img, Scalr.Method.BALANCED, width, height)<br/>            }<br/>        }<br/>    }</pre>


            

            
        
    






    
        <title>Extracting image metadata</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">提取图像元数据</h1>
                
            
            
                
<p>到目前为止，我们已经加载并预处理了原始图像，但我们不知道需要让CNN学习的图像元数据。因此，是时候加载包含每个图像元数据的CSV文件了。</p>
<p>我编写了一个方法来读取CSV格式的元数据，这个方法叫做<kbd>readMetadata()</kbd>，稍后会被另外两个叫做<kbd>readBusinessLabels</kbd>和<kbd>readBusinessToImageLabels</kbd>的方法使用。这三种方法在<kbd>CSVImageMetadataReader.scala</kbd>脚本中定义。下面是<kbd>readMetadata()</kbd>方法的签名:</p>
<pre><strong>def</strong> readMetadata(csv: String, rows: List[Int]=List(-1)): List[List[String]] = {<br/><strong>    val</strong> src = Source.fromFile(csv)<br/><br/><strong>    def</strong> reading(csv: String): List[List[String]]= {<br/>        src.getLines.map(x =&gt; x.split(",").toList)<br/>            .toList<br/>            }<br/><strong>        try</strong> {<br/><strong>            if</strong>(rows==List(-1)) reading(csv)<br/><strong>            else</strong> rows.map(reading(csv))<br/>            } <br/><strong>        finally</strong> {<br/>            src.close<br/>            }<br/>        }</pre>
<p><kbd>readBusinessLabels()</kbd>方法从业务ID映射到形式为<kbd>businessID</kbd> → Set (labels)的标签:</p>
<pre><strong>def</strong> readBusinessLabels(csv: String, rows: List[Int]=List(-1)): Map[String, Set[Int]] = {<br/><strong>    val</strong> reader = readMetadata(csv)<br/>    reader.drop(1)<br/>        .map(x =&gt; x <strong>match</strong> {<br/><strong>        case</strong> x :: Nil =&gt; (x(0).toString, Set[Int]())<br/><strong>        case</strong> _ =&gt; (x(0).toString, x(1).split(" ").map(y =&gt; y.toInt).toSet)<br/>        }).toMap<br/>}</pre>
<p><kbd>readBusinessToImageLabels ()</kbd>方法以<kbd>imageID</kbd> → <kbd>businessID</kbd>的形式从图像ID映射到业务ID:</p>
<pre><strong>def</strong> readBusinessToImageLabels(csv: String, rows: List[Int] = List(-1)): Map[Int, String] = {<br/><strong>    val</strong> reader = readMetadata(csv)<br/>    reader.drop(1)<br/>        .map(x =&gt; x <strong>match</strong> {<br/><strong>        case</strong> x :: Nil =&gt; (x(0).toInt, "-1")<br/><strong>        case</strong> _ =&gt; (x(0).toInt, x(1).split(" ").head)<br/>        }).toMap<br/>}</pre>


            

            
        
    






    
        <title>Image feature extraction</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">图像特征提取</h1>
                
            
            
                
<p>到目前为止，我们已经看到了如何预处理图像，以便从这些图像中提取特征并输入到CNN中。此外，我们还看到了如何提取和映射元数据，并将其与原始图像链接起来。现在是时候从那些预处理过的图像中提取特征了。</p>
<p>我们还需要记住每张图片的元数据的出处。正如您所猜测的，我们需要三个地图操作来提取特征。基本上，我们有三张地图。详情参见<kbd>imageFeatureExtractor.scala</kbd>脚本:</p>
<ol>
<li>格式为<kbd>imageID</kbd> → <kbd>businessID</kbd>的业务映射</li>
<li>表格数据图<kbd>imageID</kbd> →图像数据</li>
<li>表单的标签图<kbd>businessID</kbd> →标签</li>
</ol>
<p>我们首先定义一个正则表达式模式，从CSV <kbd>ImageMetadataReader</kbd>类中提取<kbd>.jpg</kbd>名称，用于匹配训练标签:</p>
<pre><strong>val</strong> patt_get_jpg_name = <strong>new</strong> Regex("[0-9]")</pre>
<p>然后，我们提取与其各自的业务ID相关联的所有图像ID:</p>
<pre><strong>def</strong> getImgIdsFromBusinessId(bizMap: Map[Int, String], businessIds: List[String]): List[Int] = {<br/>    bizMap.filter(x =&gt; businessIds.exists(y =&gt; y == x._2)).map(_._1).toList <br/>    }</pre>
<p>现在，我们需要加载和处理所有已经预处理的图像，通过映射从业务id中提取的id来提取图像id，如前面所示:</p>
<pre><strong>def</strong> getImageIds(photoDir: String, businessMap: Map[Int, String] = Map(-1 -&gt; "-1"), businessIds:         <br/>    List[String] = List("-1")): List[String] = {<br/><strong>    val</strong> d = <strong>new</strong> File(photoDir)<br/><strong>    val</strong> imgsPath = d.listFiles().map(x =&gt; x.toString).toList<br/><strong>    if</strong> (businessMap == Map(-1 -&gt; "-1") || businessIds == List(-1)) {<br/>        imgsPath<br/>    } <br/><strong>    else</strong> {<br/><strong>        val</strong> imgsMap = imgsPath.map(x =&gt; patt_get_jpg_name.findAllIn(x).mkString.toInt -&gt; x).toMap<br/><strong>        val</strong> imgsPathSub = getImgIdsFromBusinessId(businessMap, businessIds)<br/>        imgsPathSub.filter(x =&gt; imgsMap.contains(x)).map(x =&gt; imgsMap(x))<br/>        } <br/>    }</pre>
<p>到目前为止，我们已经能够提取与至少一个企业有某种关联的所有图像id。下一步是读取这些图像并将其处理成<kbd>imageID</kbd> →矢量图:</p>
<pre><strong>def</strong> processImages(imgs: List[String], resizeImgDim: Int = 128, nPixels: Int = -1): Map[Int,Vector[Int]]= {<br/>    imgs.map(x =&gt; patt_get_jpg_name.findAllIn(x).mkString.toInt -&gt; {<br/><strong>        val</strong> img0 = ImageIO.read(<strong>new</strong> File(x))<br/>        .makeSquare<br/>        .resizeImg(resizeImgDim, resizeImgDim) // (128, 128)<br/>        .image2gray<br/><strong>   if</strong>(nPixels != -1) img0.slice(0, nPixels)<br/><strong>   else</strong> img0<br/>        }<br/>    ).filter( x =&gt; x._2 != ())<br/>    .toMap<br/>    }</pre>
<p>干得好！我们只差一步就能提取出训练我们CNN所需的信息。特征提取的最后一步是提取像素数据:</p>
<div><img height="338" width="1013" class="alignnone size-full wp-image-582 image-border" src="img/7bee7abb-8627-4ec1-84b4-66d95a1b5bb4.png"/></div>
<p>图13:图像数据表示</p>
<p>总之，我们需要跟踪每幅图像的四个对象部分，即<kbd>imageID</kbd>、<kbd>businessID</kbd>、标签和像素数据。因此，如上图所示，主数据结构由四种数据类型(四元组)构成— <kbd>imgID</kbd>、<kbd>businessID</kbd>、像素数据向量和标签:</p>
<pre>List[(Int, String, Vector[Int], Set[Int])]</pre>
<p>因此，我们应该有一个包含所有这些对象的类。别担心，我们需要的一切都在<kbd>featureAndDataAligner.scala</kbd>脚本中定义了。一旦我们使用<kbd>Main.scala</kbd>脚本中的以下代码行实例化了<kbd>featureAndDataAligner</kbd>的实例(在<kbd>main</kbd>方法下)，就会提供<kbd>businessMap</kbd>、<kbd>dataMap</kbd>和<kbd>labMap</kbd>:</p>
<pre><strong>val</strong> alignedData = <strong>new</strong> featureAndDataAligner(dataMap, businessMap, Option(labelMap))()</pre>
<p>这里使用了<kbd>labMap</kbd>的选项类型，因为我们在对测试数据评分时没有这个信息——也就是说，<kbd>None</kbd>用于那个调用:</p>
<pre><strong>class</strong> featureAndDataAligner(dataMap: Map[Int, Vector[Int]], bizMap: Map[Int, String], labMap: Option[Map[String, Set[Int]]])(rowindices: List[Int] = dataMap.keySet.toList) {<br/><strong>    def </strong><strong>this</strong>(dataMap: Map[Int, Vector[Int]], bizMap: Map[Int, String])(rowindices: List[Int]) =         <strong>this</strong>(dataMap, bizMap, None)(rowindices)<br/><br/><strong>    def</strong> alignBusinessImgageIds(dataMap: Map[Int, Vector[Int]], bizMap: Map[Int, String])<br/>        (rowindices: List[Int] = dataMap.keySet.toList): List[(Int, String, Vector[Int])] = {<br/><strong>        for</strong> { <br/>            pid &lt;- rowindices<br/><strong>            val</strong> imgHasBiz = bizMap.get(pid) <br/>            // returns None if img doe not have a bizID<br/><strong>            val</strong> bid = <strong>if</strong>(imgHasBiz != None) imgHasBiz.get <br/><strong>            else</strong> "-1"<br/><strong>            if</strong> (dataMap.keys.toSet.contains(pid) &amp;&amp; imgHasBiz != None)<br/>            } <br/><strong>        yield</strong> {<br/>        (pid, bid, dataMap(pid))<br/>           }<br/>        }<br/><strong>def</strong> alignLabels(dataMap: Map[Int, Vector[Int]], bizMap: Map[Int, String], labMap: Option[Map[String,     Set[Int]]])(rowindices: List[Int] = dataMap.keySet.toList): List[(Int, String, Vector[Int], Set[Int])] = {<br/><strong>    def</strong> flatten1[A, B, C, D](t: ((A, B, C), D)): (A, B, C, D) = (t._1._1, t._1._2, t._1._3, t._2)<br/><strong>        val</strong> al = alignBusinessImgageIds(dataMap, bizMap)(rowindices)<br/><strong>        for</strong> { p &lt;- al<br/>        } <br/><strong>        yield</strong> {<br/><strong>            val</strong> bid = p._2<br/><strong>            val</strong> labs = labMap <strong>match</strong> {<br/><strong>            case</strong> None =&gt; Set[Int]()<br/><strong>            case</strong> x =&gt; (<strong>if</strong>(x.get.keySet.contains(bid)) x.get(bid) <br/>        <strong>else</strong> Set[Int]())<br/>            }<br/>            flatten1(p, labs)<br/>        }<br/>    }<br/><strong>    lazy </strong><strong>val</strong> data = alignLabels(dataMap, bizMap, labMap)(rowindices)<br/>   // getter functions<br/><strong>    def</strong> getImgIds = data.map(_._1)<br/><strong>    def</strong> getBusinessIds = data.map(_._2)<br/><strong>    def</strong> getImgVectors = data.map(_._3)<br/><strong>    def</strong> getBusinessLabels = data.map(_._4)<br/><strong>    def</strong> getImgCntsPerBusiness = getBusinessIds.groupBy(identity).mapValues(x =&gt; x.size) <br/>}</pre>
<p>太棒了。到目前为止，我们已经成功地提取了特征来训练我们的CNN。然而，当前形式的特征仍然不适合馈入CNN，因为我们只有没有标签的特征向量。因此，我们需要一个中间转换。</p>


            

            
        
    






    
        <title>Preparing the ND4j dataset</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">准备ND4j数据集</h1>
                
            
            
                
<p>正如我所说的，我们需要一个中间转换和预处理来使训练集包含特征向量和标签。转换非常简单:我们需要特征向量和业务标签。</p>
<p>为此，我们有了<kbd>makeND4jDataSets</kbd>类(详见<kbd>makeND4jDataSets.scala</kbd>)。该类从来自<kbd>alignLables</kbd>函数的数据结构中以<kbd>List[(imgID, bizID, labels, pixelVector)]</kbd>的形式创建一个ND4j数据集对象。首先，我们使用<kbd>makeDataSet()</kbd>方法准备数据集:</p>
<pre><strong>def</strong> makeDataSet(alignedData: featureAndDataAligner, bizClass: Int): DataSet = {<br/><strong>    val</strong> alignedXData = alignedData.getImgVectors.toNDArray<br/><strong>    val</strong> alignedLabs = alignedData.getBusinessLabels.map(x =&gt; <br/><strong>    if</strong> (x.contains(bizClass)) Vector(1, 0) <br/>    <strong>else</strong> Vector(0, 1)).toNDArray<br/><strong>    new</strong> DataSet(alignedXData, alignedLabs)<br/>    }</pre>
<p>然后我们需要将前面的数据结构进一步转换成<kbd>INDArray</kbd>，然后CNN可以使用它:</p>
<pre><strong>def</strong> makeDataSetTE(alignedData: featureAndDataAligner): INDArray = {<br/>    alignedData.getImgVectors.toNDArray<br/>    }</pre>


            

            
        
    






    
        <title>Training the CNNs and saving the trained models</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">训练CNN并保存训练好的模型</h1>
                
            
            
                
<p>到目前为止，我们已经看到了如何准备训练集；现在我们面临着一个挑战。我们要训练234545张图片。虽然测试阶段只有500张图像，但最好使用DL4j的<kbd>MultipleEpochsIterator</kbd>以批处理模式训练每个CNN。以下是重要超参数及其详细信息的列表:</p>
<ul>
<li><strong>层</strong>:正如我们已经观察到的简单的5层MNIST，我们获得了出色的分类精度，这是非常有希望的。在这里，我将尝试构建一个类似的网络。</li>
<li><strong>样本数量</strong>:如果你在训练所有的图像，这将花费很长时间。如果你使用CPU而不是GPU进行训练，将需要几天时间。当我尝试处理50，000张图像时，一台配备i7处理器和32 GB内存的机器需要一整天的时间。现在，您可以想象整个数据集需要多长时间。此外，即使您在批处理模式下进行培训，也需要至少256 GB的RAM。</li>
<li><strong>历元数</strong>:这是所有训练记录的迭代次数。</li>
<li><strong>输出特征地图的数量(即nOut) </strong>:这是特征地图的数量。仔细看看DL4j GitHub库中的其他例子。</li>
<li><strong>学习率</strong>:从类似TensorFlow的框架中，我得到了一些感悟。在我看来，把学习率定在0.01，0.001就太好了。</li>
<li><strong>批次号</strong>:每批记录的数量——32、64、128等。我用了128。</li>
</ul>
<p>现在，有了前面的超参数，我们可以开始训练我们的CNN。下面的代码完成了这个任务。首先，我们准备训练集，然后我们定义所需的超参数，然后我们归一化数据集，以便对ND4j数据帧进行编码，任何被认为是真的标签都是1，其余的是0。然后，我们洗牌的行和编码数据集的标签。</p>
<p>现在我们需要分别使用<kbd>ListDataSetIterator</kbd>和<kbd>MultipleEpochsIterator</kbd>为数据集迭代器创建历元。一旦数据集被转换成批量模型，我们就准备好训练构建的CNN:</p>
<pre><strong>def</strong> trainModelEpochs(alignedData: featureAndDataAligner, businessClass: Int = 1, saveNN: String = "") = {<br/><strong>    val</strong> ds = makeDataSet(alignedData, businessClass)<br/><strong>    val</strong> nfeatures = ds.getFeatures.getRow(0).length // Hyperparameter<br/><strong>    val</strong> numRows = Math.sqrt(nfeatures).toInt //numRows*numColumns == data*channels<br/><strong>    val</strong> numColumns = Math.sqrt(nfeatures).toInt //numRows*numColumns == data*channels<br/><strong>    val</strong> nChannels = 1 // would be 3 if color image w R,G,B<br/><strong>    val</strong> outputNum = 9 // # of classes (# of columns in output)<br/><strong>    val</strong> iterations = 1<br/><strong>    val</strong> splitTrainNum = math.ceil(ds.numExamples * 0.8).toInt // 80/20 training/test split<br/><strong>    val</strong> seed = 12345<br/><strong>    val</strong> listenerFreq = 1<br/><strong>    val</strong> nepochs = 20<br/><strong>    val</strong> nbatch = 128 // recommended between 16 and 128<br/><br/>    ds.normalizeZeroMeanZeroUnitVariance()<br/>    Nd4j.shuffle(ds.getFeatureMatrix, <strong>new</strong> Random(seed), 1) // shuffles rows in the ds.<br/>    Nd4j.shuffle(ds.getLabels, <strong>new</strong> Random(seed), 1) // shuffles labels accordingly<br/><br/><strong>    val</strong> trainTest: SplitTestAndTrain = ds.splitTestAndTrain(splitTrainNum, <strong>new</strong> Random(seed))<br/><br/>    // creating epoch dataset iterator<br/><strong>    val</strong> dsiterTr = <strong>new</strong> ListDataSetIterator(trainTest.getTrain.asList(), nbatch)<br/><strong>    val</strong> dsiterTe = <strong>new</strong> ListDataSetIterator(trainTest.getTest.asList(), nbatch)<br/><strong>    val</strong> epochitTr: MultipleEpochsIterator = <strong>new</strong> MultipleEpochsIterator(nepochs, dsiterTr)<br/><br/><strong>    val</strong> epochitTe: MultipleEpochsIterator = <strong>new</strong> MultipleEpochsIterator(nepochs, dsiterTe)<br/>    //First convolution layer with ReLU as activation function<br/><strong>    val</strong> layer_0 = <strong>new</strong> ConvolutionLayer.Builder(6, 6)<br/>        .nIn(nChannels)<br/>        .stride(2, 2) // default stride(2,2)<br/>        .nOut(20) // # of feature maps<br/>        .dropOut(0.5)<br/>        .activation("relu") // rectified linear units<br/>        .weightInit(WeightInit.RELU)<br/>        .build()<br/><br/>    //First subsampling layer<br/><strong>    val</strong> layer_1 = <strong>new</strong> SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>        .kernelSize(2, 2)<br/>        .stride(2, 2)<br/>        .build()<br/><br/>    //Second convolution layer with ReLU as activation function<br/><strong>    val</strong> layer_2 = <strong>new</strong> ConvolutionLayer.Builder(6, 6)<br/>        .nIn(nChannels)<br/>        .stride(2, 2)<br/>        .nOut(50)<br/>        .activation("relu")<br/>        .build()<br/><br/>    //Second subsampling layer<br/><strong>    val</strong> layer_3 = <strong>new</strong> SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>        .kernelSize(2, 2)<br/>        .stride(2, 2)<br/>        .build()<br/><br/>    //Dense layer<br/><strong>    val</strong> layer_4 = <strong>new</strong> DenseLayer.Builder()<br/>        .activation("relu")<br/>        .nOut(500)<br/>        .build()<br/><br/>    // Final and fully connected layer with Softmax as activation function<br/><strong>    val</strong> layer_5 = <strong>new</strong> OutputLayer.Builder(LossFunctions.LossFunction.MCXENT)<br/>        .nOut(outputNum)<br/>        .weightInit(WeightInit.XAVIER)<br/>        .activation("softmax")<br/>        .build()<br/><strong><br/>    val</strong> builder: MultiLayerConfiguration.Builder = <strong>new</strong> NeuralNetConfiguration.Builder()<br/>        .seed(seed)<br/>        .iterations(iterations)<br/>        .miniBatch(<strong>true</strong>)<br/>        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)<br/>        .regularization(<strong>true</strong>).l2(0.0005)<br/>        .learningRate(0.01)<br/>        .list(6)<br/>            .layer(0, layer_0)<br/>            .layer(1, layer_1)<br/>            .layer(2, layer_2)<br/>            .layer(3, layer_3)<br/>            .layer(4, layer_4)<br/>            .layer(5, layer_5)<br/>    .backprop(<strong>true</strong>).pretrain(<strong>false</strong>)<br/><br/><strong>    new</strong> ConvolutionLayerSetup(builder, numRows, numColumns, nChannels)<br/><strong>    val</strong> conf: MultiLayerConfiguration = builder.build()<br/><strong>    val</strong> model: MultiLayerNetwork = <strong>new</strong> MultiLayerNetwork(conf)<br/><br/>    model.init()<br/>    model.setListeners(Seq[IterationListener](<strong>new</strong> ScoreIterationListener(listenerFreq)).asJava)<br/>    model.fit(epochitTr)<br/><br/><strong>    val</strong> eval = <strong>new</strong> Evaluation(outputNum)<br/><strong>    while</strong> (epochitTe.hasNext) {<br/><strong>        val</strong> testDS = epochitTe.next(nbatch)<br/><strong>        val</strong> output: INDArray = model.output(testDS.getFeatureMatrix)<br/>        eval.eval(testDS.getLabels(), output)<br/>        }<br/><strong>    if</strong> (!saveNN.isEmpty) {<br/>        // model config<br/>        FileUtils.write(<strong>new</strong> File(saveNN + ".json"), model.getLayerWiseConfigurations().toJson())<br/>        // model parameters<br/><strong>        val</strong> dos: DataOutputStream = <strong>new</strong> DataOutputStream(Files.newOutputStream(Paths.get(saveNN + ".bin")))<br/>        Nd4j.write(model.params(), dos)<br/>        }<br/>    }</pre>
<p class="mce-root">在前面的代码中，我们还保存了一个包含所有网络配置的<kbd>.json</kbd>文件和一个保存所有CNN的所有权重和参数的<kbd>.bin</kbd>文件。这是通过两种方法完成的；即<kbd>NeuralNetwok.scala</kbd>脚本中定义的<kbd>saveNN()</kbd>和<kbd>loadNN()</kbd>。首先，让我们看看<kbd>saveNN()</kbd>方法的签名，如下所示:</p>
<pre><strong>def</strong> saveNN(model: MultiLayerNetwork, NNconfig: String, NNparams: String) = {<br/>    // save neural network config<br/>    FileUtils.write(<strong>new</strong> File(NNconfig), model.getLayerWiseConfigurations().toJson())<br/>    // save neural network parms<br/><strong>    val</strong> dos: DataOutputStream = <strong>new</strong> DataOutputStream(Files.newOutputStream(Paths.get(NNparams)))<br/>    Nd4j.write(model.params(), dos)<br/>}</pre>
<p>这个想法既有远见又很重要，因为正如我所说，你不会第二次训练你的整个网络来评估一个新的测试集——也就是说，假设你只想测试一个图像。我们还有另一个名为<kbd>loadNN()</kbd>的方法，它将我们之前创建的<kbd>.json</kbd>和<kbd>.bin</kbd>文件读回到一个<kbd>MultiLayerNetwork</kbd>中，并用于对新的测试数据进行评分。该方法如下进行:</p>
<pre><strong>def</strong> loadNN(NNconfig: String, NNparams: String) = {<br/>    // get neural network config<br/><strong>    val</strong> confFromJson: MultiLayerConfiguration =                     <br/>    MultiLayerConfiguration.fromJson(FileUtils.readFileToString(<strong>new</strong> File(NNconfig)))<br/><br/>    // get neural network parameters<br/><strong>    val</strong> dis: DataInputStream = <strong>new</strong> DataInputStream(<strong>new</strong> FileInputStream(NNparams))<br/><strong>    val</strong> newParams = Nd4j.read(dis)<br/><br/>    // creating network object<br/><strong>    val</strong> savedNetwork: MultiLayerNetwork = <strong>new</strong> MultiLayerNetwork(confFromJson)<br/>    savedNetwork.init()<br/>    savedNetwork.setParameters(newParams)<br/>    savedNetwork <br/>    }</pre>


            

            
        
    






    
        <title>Evaluating the model</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">评估模型</h1>
                
            
            
                
<p>我们将要使用的评分方法非常简单。它通过平均图像级预测来分配业务级标签。我知道我做得很幼稚，但你可以尝试更好的方法。我所做的是，如果一个企业的所有图像中属于类别<kbd>0</kbd>的概率平均值大于0.5，则为该企业分配标签<kbd>0</kbd>:</p>
<pre><strong>def</strong> scoreModel(model: MultiLayerNetwork, ds: INDArray) = {<br/>    model.output(ds)<br/>}</pre>
<p>然后我们收集来自<kbd>scoreModel()</kbd>方法的模型预测，并与<kbd>alignedData</kbd>合并:</p>
<pre><br/><strong>def</strong> aggImgScores2Business(scores: INDArray, alignedData: featureAndDataAligner ) = {<br/>    assert(scores.size(0) == alignedData.data.length, "alignedData and scores length are different. They     must be equal")<br/><br/><strong>def</strong> getRowIndices4Business(mylist: List[String], mybiz: String): List[Int] = mylist.zipWithIndex.filter(x     =&gt; x._1 == mybiz).map(_._2)<br/><br/><strong>def</strong> mean(xs: List[Double]) = xs.sum / xs.size<br/>    alignedData.getBusinessIds.distinct.map(x =&gt; (x, {<br/><strong>    val</strong> irows = getRowIndices4Business(alignedData.getBusinessIds, x)<br/><strong>    val</strong> ret = <br/><strong>    for</strong>(row &lt;- irows) <br/><strong>        yield</strong> scores.getRow(row).getColumn(1).toString.toDouble<br/>        mean(ret)<br/>        }))<br/>    }</pre>
<p>最后，我们可以恢复训练好的和保存好的模型，恢复回来，为Kaggle生成提交文件。问题是，我们需要将每个模型的形象预测汇总到业务得分中。</p>


            

            
        
    






    
        <title>Wrapping up by executing the main() method</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">通过执行main()方法结束</h1>
                
            
            
                
<p>让我们通过观看模型的性能来总结整个讨论。下面的代码是一个大概:</p>
<pre><strong>package</strong> Yelp.Classifier<br/><strong>import</strong> Yelp.Preprocessor.CSVImageMetadataReader._<br/><strong>import</strong> Yelp.Preprocessor.featureAndDataAligner<br/><strong>import</strong> Yelp.Preprocessor.imageFeatureExtractor._<br/><strong>import</strong> Yelp.Evaluator.ResultFileGenerator._<br/><strong>import</strong> Yelp.Preprocessor.makeND4jDataSets._<br/><strong>import</strong> Yelp.Evaluator.ModelEvaluation._<br/><strong>import</strong> Yelp.Trainer.CNNEpochs._<br/><strong>import</strong> Yelp.Trainer.NeuralNetwork._<br/><br/><strong>object</strong> YelpImageClassifier {<br/><strong>    def</strong> main(args: Array[String]): Unit = {<br/>        // image processing on training data<br/><strong>        val</strong> labelMap = readBusinessLabels("data/labels/train.csv")<br/><strong>        val</strong> businessMap = readBusinessToImageLabels("data/labels/train_photo_to_biz_ids.csv")<br/><strong>        val</strong> imgs = getImageIds("data/images/train/", businessMap, <br/>        businessMap.map(_._2).toSet.toList).slice(0,20000) // 20000 images<br/>    <br/>        println("Image ID retreival done!")<br/><strong>        val</strong> dataMap = processImages(imgs, resizeImgDim = 256)<br/>        println("Image processing done!")<br/><br/><strong>        val</strong> alignedData = <br/><strong>            new</strong> featureAndDataAligner(dataMap, businessMap, Option(labelMap))()<br/>        println("Feature extraction done!")<br/><br/>        // training one model for one class at a time. Many hyperparamters hardcoded within<br/><strong>        val</strong> cnn0 = trainModelEpochs(alignedData, businessClass = 0, saveNN = "models/model0")<br/><strong>        val</strong> cnn1 = trainModelEpochs(alignedData, businessClass = 1, saveNN = "models/model1")<br/><strong>        val</strong> cnn2 = trainModelEpochs(alignedData, businessClass = 2, saveNN = "models/model2")<br/><strong>        val</strong> cnn3 = trainModelEpochs(alignedData, businessClass = 3, saveNN = "models/model3")<br/><strong>        val</strong> cnn4 = trainModelEpochs(alignedData, businessClass = 4, saveNN = "models/model4")<br/><strong>        val</strong> cnn5 = trainModelEpochs(alignedData, businessClass = 5, saveNN = "models/model5")<br/><strong>        val</strong> cnn6 = trainModelEpochs(alignedData, businessClass = 6, saveNN = "models/model6")<br/><strong>        val</strong> cnn7 = trainModelEpochs(alignedData, businessClass = 7, saveNN = "models/model7")<br/><strong>        val</strong> cnn8 = trainModelEpochs(alignedData, businessClass = 8, saveNN = "models/model8")<br/><br/>    // processing test data for scoring<br/><strong>        val</strong> businessMapTE = readBusinessToImageLabels("data/labels/test_photo_to_biz.csv")<br/><strong>        val</strong> imgsTE = getImageIds("data/images/test//", businessMapTE,     <br/>        businessMapTE.map(_._2).toSet.toList)<br/><br/><strong>        val</strong> dataMapTE = processImages(imgsTE, resizeImgDim = 128) // make them 256x256<br/><strong>        val</strong> alignedDataTE = <strong>new</strong> featureAndDataAligner(dataMapTE, businessMapTE, None)()<br/><br/>        // creating csv file to submit to kaggle (scores all models)<br/><strong>        val</strong> Results = SubmitObj(alignedDataTE, "results/ModelsV0/")<br/><strong>        val</strong> SubmitResults = writeSubmissionFile("kaggleSubmitFile.csv", Results, thresh = 0.9)<br/>        }<br/>    }<br/>&gt;&gt;&gt;<br/>==========================Scores======================================<br/> Accuracy: 0.6833<br/> Precision: 0.53<br/> Recall: 0.5222<br/> F1 Score: 0.5261<br/>======================================================================</pre>
<p>那么，你的印象是什么？的确，我们还没有获得突出的分类准确性。但是我们仍然可以用调整过的超参数来尝试。下一节提供了一些见解。</p>


            

            
        
    






    
        <title>Tuning and optimizing CNN hyperparameters</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">调整和优化CNN超参数</h1>
                
            
            
                
<p>以下超参数非常重要，必须进行调整才能获得最佳结果。</p>
<ul>
<li><strong>遗漏</strong>:用于随机遗漏特征检测器，防止过拟合</li>
<li><strong>稀疏度</strong>:用于强制激活稀疏/稀有输入</li>
<li><strong>阿达格勒</strong>:用于特定特征学习率优化</li>
<li><strong>正规化</strong> : L1和L2正规化</li>
<li><strong>权重变换</strong>:适用于深度自动编码器</li>
<li><strong>概率分布操作</strong>:用于初始权重生成</li>
<li>渐变标准化和裁剪</li>
</ul>
<p>另一个重要的问题是:什么时候要添加一个max pooling层而不是一个相同步幅的卷积层？最大池层没有任何参数，而卷积层有很多参数。有时，添加一个局部响应标准化层，使激活最强烈的神经元抑制同一位置但在相邻特征图中的神经元，鼓励不同的特征图进行专业化，并将它们推开，迫使它们探索更大范围的特征。它通常用于较低的层，以获得较大的底层功能池，较高的层可以在此基础上进行构建。</p>
<p>在大型神经网络的训练过程中观察到的主要优点之一是过拟合，也就是说，对训练数据产生非常好的近似，但是对单点之间的区域发出噪声。在过度拟合的情况下，模型会专门针对训练数据集进行调整，因此不会用于泛化。因此，尽管它在训练集上表现良好，但在测试数据集和后续测试上的性能很差，因为它缺乏泛化属性:</p>
<div><img height="564" width="1499" class="alignnone size-full wp-image-583 image-border" src="img/e7c1d031-2428-4f36-b4d1-a703de85c86e.png"/></div>
<p>图14:辍学与未辍学的对比</p>
<p>这种方法的主要优点是避免了一层中的所有神经元同步优化它们的权重。在随机组中进行的这种自适应避免了所有的神经元都集中在相同的目标上，从而使自适应的权重去相关。在dropout应用中发现的第二个特性是隐藏单元的激活变得稀疏，这也是一个期望的特性。</p>
<p>因为在CNN中，目标函数之一是最小化评估成本，所以我们必须定义一个优化器。DL4j支持以下优化器:</p>
<ul>
<li>新币(仅学习率)</li>
<li>内斯特洛夫动量</li>
<li>阿达格拉德</li>
<li>RMSProp</li>
<li>圣经》和《古兰经》传统中）亚当（人类第一人的名字</li>
<li>阿达德尔塔</li>
</ul>
<p>在大多数情况下，如果性能不令人满意，我们可以采用实现的RMSProp，这是梯度下降的一种高级形式。RMSProp的性能更好，因为它将学习率除以梯度平方的指数衰减平均值。衰减参数的建议设置值为0.9，而学习率的较好默认值为0.001。</p>
<p>从技术上来说，通过使用最常见的优化器，如<strong>随机梯度下降</strong> ( <strong> SGD </strong>)，学习率必须与1/T成比例才能收敛，其中T是迭代次数。RMSProp试图通过调整步长来自动克服这一限制，使步长与梯度的比例相同。因此，如果你正在训练一个神经网络，但计算梯度是强制性的，使用RMSProp将是在小批量设置中更快的学习方法。研究人员还建议在训练深度CNN或DNN时使用动量优化器。</p>
<p>从分层架构的角度来看，CNN不同于CNN它有不同的要求和调整标准。CNN的另一个问题是卷积层需要大量的RAM，尤其是在训练期间，因为反向传播的反向传递需要在正向传递期间计算的所有中间值。在推断过程中(也就是对新实例进行预测时)，一个层占用的RAM可以在下一层计算完成后立即释放，因此您只需要两个连续层所需的RAM。</p>
<p>然而，在训练期间，在正向传递期间计算的所有内容都需要为反向传递保留，因此所需的RAM量(至少)是所有层所需的RAM总量。如果你的GPU在训练CNN时耗尽了内存，你可以尝试以下五种方法来解决这个问题(除了购买一个内存更大的GPU):</p>
<ul>
<li>减少小批量</li>
<li>在一层或多层中使用更大的步幅来减少维度</li>
<li>移除一个或多个层</li>
<li>使用16位浮点而不是32位浮点</li>
<li>在多个设备上分发CNN</li>
</ul>


            

            
        
    






    
        <title>Summary</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p>在本章中，我们已经看到了如何使用CNN来使用和构建现实生活中的应用程序，CNN是一种前馈人工神经网络，其中神经元之间的连接模式受到动物视觉皮层组织的启发。我们使用CNN的图像分类器应用程序可以以可接受的准确度对现实生活中的图像进行分类，尽管我们没有达到更高的准确度。但是，我们鼓励读者调整代码中的超参数，并对另一个数据集尝试相同的方法。</p>
<p>然而，重要的是，由于卷积神经网络的内部数据表示没有考虑简单和复杂对象之间的重要空间层次，CNN对于某些情况有一些严重的缺点和限制。因此，我建议你在<a href="https://github.com/topics/capsule-network">https://github.com/topics/capsule-network</a>看看GitHub上最近围绕胶囊网络的活动。希望你能从那里得到一些有用的东西</p>
<p>这或多或少是我们使用Scala和不同开源框架开发ML项目的小小旅程的结束。在整个章节中，我试图为您提供几个例子来说明如何有效地使用这些美妙的技术来开发ML项目。在写这本书的过程中，我不得不在脑海中保留许多约束，例如，页数、API可用性和我的专业知识。但我试图让这本书或多或少简单一些，我也试图避免理论上的细节，因为你可以在Apache Spark、DL4j和H2O本身的许多书籍、博客和网站上读到这一点。</p>
<p>我也会在我的GitHub repo上保持这本书的代码更新:<a href="https://github.com/PacktPublishing/Scala-Machine-Learning-Projects" target="_blank">https://GitHub . com/packt publishing/Scala-Machine-Learning-Projects</a>。随时打开一个新的问题或任何拉要求改善这本书，并保持关注。</p>
<p>最后，我写这本书不是为了赚钱，而是版税的主要部分将用于我家乡孟加拉国农村地区的儿童教育。我想说谢谢，并对购买和欣赏这本书表示诚挚的感谢！</p>


            

            
        
    


</body></html>