<html><head/><body>


    
        <title>Clients Subscription Assessment for Bank Telemarketing using Deep Neural Networks</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">基于深度神经网络的银行电话营销客户签约评估</h1>
                
            
            
                
<p class="chapter-content">在本章中，我们将看到两个示例，说明如何在银行营销数据集上使用H2O构建非常稳健和准确的预测模型进行预测分析。该数据与一家葡萄牙银行机构的直接营销活动有关。营销活动以电话为基础。这个端到端项目的目标是预测客户将订阅定期存款。</p>
<p class="chapter-content">在整个项目中，本章将涵盖以下主题:</p>
<ul>
<li>客户订阅评估</li>
<li>数据集描述</li>
<li>数据集的探索性分析</li>
<li>使用H2O进行客户订阅评估</li>
<li>调谐超参数</li>
</ul>


            

            
        
    






    
        <title>Client subscription assessment through telemarketing</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">通过电话营销进行客户订购评估</h1>
                
            
            
                
<p class="mce-root">前一段时间，由于全球金融危机，银行在国际市场获得信贷变得更加受限。这将注意力转向内部客户及其存款，以筹集资金。这导致了对有关客户存款行为以及他们对银行定期开展的电话营销活动的反应的信息的需求。通常，为了评估产品(银行定期存款)是否会被(<strong>是</strong>)认购或(<strong>否</strong>)认购，需要与同一客户进行多次联系。</p>
<p class="mce-root">这个项目的目的是实现一个ML模型，预测客户将订阅定期存款(变量<kbd>y</kbd>)。简而言之，这是一个二元分类问题。现在，在我们开始实现我们的应用程序之前，我们需要了解数据集。然后我们将看到数据集的解释性分析。</p>


            

            
        
    






    
        <title>Dataset description</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">数据集描述</h1>
                
            
            
                
<p class="mce-root">我要感谢两个来源。Moro等人于2014年6月在Elsevier出版的《决策支持系统》中发表的一篇研究论文中使用了该数据集。后来，它被捐赠给了UCI机器学习知识库，可以从https://archive.ics.uci.edu/ml/datasets/bank+marketing<a href="https://archive.ics.uci.edu/ml/datasets/bank+marketing">下载。根据数据集描述，有四个数据集:</a></p>
<ul>
<li><kbd>bank-additional-full.csv</kbd>:这包括所有示例(41，188)和20个输入，按日期排序(从2008年5月到2010年11月)，非常接近Moro等人2014年分析的数据</li>
<li><kbd>bank-additional.csv</kbd>:这包括10%的例子(4，119)，从1到20个输入中随机选择</li>
<li><kbd>bank-full.csv</kbd>:这包括所有的例子和17个输入，按日期排序(这个数据集的旧版本，输入较少)</li>
<li><kbd>bank.csv</kbd>:这包括10%的示例和从三个示例中随机选择的17个输入(此数据集的旧版本，输入较少)</li>
</ul>
<p class="mce-root">数据集中有21个属性。独立变量，即特征，可以进一步分类为银行-客户相关数据(属性1至7)、与当前活动的最后联系相关的数据(属性8至11)、其他属性(属性12至15)以及社会和经济背景属性(属性16至20)。因变量由最后一个属性(21)指定<kbd>y</kbd>:</p>
<table class="table">
<tbody>
<tr>
<td><strong> ID </strong></td>
<td><strong>属性</strong></td>
<td><strong>解释</strong></td>
</tr>
<tr>
<td>一</td>
<td><kbd>age</kbd></td>
<td>数字中的年龄。</td>
</tr>
<tr>
<td>2</td>
<td><kbd>job</kbd></td>
<td>这是一种分类格式的作业类型，可能的值有:<kbd>admin</kbd>、<kbd>blue-collar</kbd>、<kbd>entrepreneur</kbd>、<kbd>housemaid</kbd>、<kbd>management</kbd>、<kbd>retired</kbd>、<kbd>self-employed</kbd>、<kbd>services</kbd>、<kbd>student</kbd>、<kbd>technician</kbd>、<kbd>unemployed</kbd>和<kbd>unknown</kbd>。</td>
</tr>
<tr>
<td>3</td>
<td><kbd>marital</kbd></td>
<td>这是分类格式的婚姻状况，可能的值有:<kbd>divorced</kbd>、<kbd>married</kbd>、<kbd>single</kbd>和<kbd>unknown</kbd>。在这里，<kbd>divorced</kbd>是离婚或丧偶的意思。</td>
</tr>
<tr>
<td>四</td>
<td><kbd>education</kbd></td>
<td>这是分类格式的教育背景，可能的值如下:<kbd>basic.4y</kbd>、<kbd>basic.6y</kbd>、<kbd>basic.9y</kbd>、<kbd>high.school</kbd>、<kbd>illiterate</kbd>、<kbd>professional.course</kbd>、<kbd>university.degree</kbd>和<kbd>unknown</kbd>。</td>
</tr>
<tr>
<td>5</td>
<td><kbd>default</kbd></td>
<td>这是一种分类格式，在默认情况下，贷方的可能值为<kbd>no</kbd>、<kbd>yes</kbd>和<kbd>unknown</kbd>。</td>
</tr>
<tr>
<td>6</td>
<td><kbd>housing</kbd></td>
<td>客户有住房贷款吗？</td>
</tr>
<tr>
<td>七</td>
<td><kbd>loan</kbd></td>
<td>分类格式的个人贷款，可能值为<kbd>no</kbd>、<kbd>yes</kbd>和<kbd>unknown</kbd>。</td>
</tr>
<tr>
<td>8</td>
<td><kbd>contact</kbd></td>
<td>这是分类格式的联系信息类型。可能的值是<kbd>cellular</kbd>和<kbd>telephone</kbd>。</td>
</tr>
<tr>
<td>9</td>
<td><kbd>month</kbd></td>
<td>这是一年中最后一个联系月份，以分类格式表示，可能值为<kbd>jan</kbd>、<kbd>feb</kbd>、<kbd>mar</kbd>，...、<kbd>nov</kbd>和<kbd>dec</kbd>。</td>
</tr>
<tr>
<td>10</td>
<td><kbd>day_of_week</kbd></td>
<td>这是一周的最后一个联系日，以分类格式表示，可能值为<kbd>mon</kbd>、<kbd>tue</kbd>、<kbd>wed</kbd>、<kbd>thu</kbd>和<kbd>fri</kbd>。</td>
</tr>
<tr>
<td>11</td>
<td><kbd>duration</kbd></td>
<td>这是最后一次联系持续时间，以秒为单位(数值)。该属性高度影响输出目标(例如，如果<kbd>duration=0</kbd>，则<kbd>y=no</kbd>)。然而，在执行呼叫之前，持续时间是未知的。还有，通话结束后，<kbd>y</kbd>显然是知道了。因此，该输入应仅用于基准测试目的，如果目的是获得现实的预测模型，则应丢弃。</td>
</tr>
<tr>
<td>12</td>
<td><kbd>campaign</kbd></td>
<td>这是在此活动期间为此客户执行的联系次数。</td>
</tr>
<tr>
<td>13</td>
<td><kbd>pdays</kbd></td>
<td>这是上一个活动最后一次联系客户端后经过的天数(数字；999表示之前没有联系过客户)。</td>
</tr>
<tr>
<td>14</td>
<td><kbd>previous</kbd></td>
<td>这是在此活动之前为此客户执行的联系次数(数字)。</td>
</tr>
<tr>
<td>15</td>
<td><kbd>poutcome</kbd></td>
<td>之前营销活动的结果(分类:<kbd>failure</kbd>、<kbd>nonexistent</kbd>和<kbd>success</kbd>)。</td>
</tr>
<tr>
<td>16</td>
<td><kbd>emp.var.rate</kbd></td>
<td>就业变化率—季度指标(数字)。</td>
</tr>
<tr>
<td>17</td>
<td><kbd>cons.price.idx</kbd></td>
<td>消费者价格指数—月度指标(数字)。</td>
</tr>
<tr>
<td>18</td>
<td><kbd>cons.conf.idx</kbd></td>
<td>消费者信心指数—月度指标(数字)。</td>
</tr>
<tr>
<td>19</td>
<td><kbd>euribor3m</kbd></td>
<td>Euribor 3个月利率—每日指标(数字)。</td>
</tr>
<tr>
<td>20</td>
<td><kbd>nr.employed</kbd></td>
<td>员工数量—季度指标(数字)。</td>
</tr>
<tr>
<td>21</td>
<td><kbd>y</kbd></td>
<td>表示客户是否已认购定期存款。它有可能的二进制值(<kbd>yes</kbd>和<kbd>no</kbd>)。</td>
</tr>
</tbody>
</table>
<p>表1:银行营销数据集的描述</p>
<p class="mce-root">对于数据集的探索性分析，我们将使用Apache Zeppelin和Spark。我们将从可视化分类特征的分布开始，然后是数字特征。最后，我们将计算一些描述数字特征的统计数据。但在此之前，我们先配置一下Zeppelin。</p>


            

            
        
    






    
        <title>Installing and getting started with Apache Zeppelin</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">Apache Zeppelin的安装和入门</h1>
                
            
            
                
<p class="mce-root">Apache Zeppelin是一个基于网络的笔记本，使您能够以互动的方式进行数据分析。使用Zeppelin，您可以用SQL、Scala等创建漂亮的、数据驱动的、交互式的协作文档。Apache Zeppelin解释器概念允许任何语言/数据处理后端插入Zeppelin。目前，Apache Zeppelin支持许多解释器，如Apache Spark、Python、JDBC、Markdown和Shell。</p>
<p class="mce-root">Apache Zeppelin是来自Apache Software Foundation的一项相对较新的技术，它使数据科学家、工程师和从业者能够使用多种编程语言后端(如Python、Scala、Hive、SparkSQL、Shell、Markdown等)进行数据探索、可视化、共享和协作。由于使用其他解释器不是本书的目标，我们将在Zeppelin上使用Spark，所有代码都将使用Scala编写。因此，在本节中，我们将向您展示如何使用只包含Spark解释器的二进制包来配置Zeppelin。Apache Zeppelin正式支持以下环境，并在这些环境中进行了测试:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td><strong>要求</strong></td>
<td><strong>值/版本</strong></td>
</tr>
<tr>
<td>甲骨文JDK公司</td>
<td>1.7+ <br/>(设置<kbd>JAVA_HOME</kbd>)</td>
</tr>
<tr>
<td>操作系统（Operating System）</td>
<td>Mac OS X <br/> Ubuntu 14。X+ <br/>厘斯6。x+<br/>SP1视窗7专业版</td>
</tr>
</tbody>
</table>
<p class="mce-root">如上表所示，在Zeppelin上执行Spark代码需要Java。因此，如果没有设置，请在前面提到的任何平台上安装和设置java。阿帕奇飞艇的最新版本可以从https://zeppelin.apache.org/download.html的<a href="https://zeppelin.apache.org/download.html">下载。每个版本都有三个选项:</a></p>
<ul>
<li><strong>包含所有解释器的二进制包</strong>:包含对许多解释器的所有支持。例如，Spark、JDBC、Pig、Beam、Scio、BigQuery、Python、Livy、HDFS、Alluxio、Hbase、burning、Elasticsearch、Angular、Markdown、Shell、Flink、Hive、Tajo、Cassandra、Geode、Ignite、Kylin、Lens、Phoenix和PostgreSQL目前在Zeppelin中得到支持。</li>
<li><strong>带Spark解释器的二进制包</strong>:通常，这个包只包含Spark解释器。它还包含一个解释器网络安装脚本。</li>
<li>来源:你也可以用GitHub库的所有最新变化来构建Zeppelin(稍后会详细介绍)。为了向您展示如何安装和配置Zeppelin，我们从这个站点的镜像下载了二进制包。一旦你下载了它，解压到你机器的某个地方。假设你解压文件的路径是<kbd>/home/Zeppelin/</kbd>。</li>
</ul>


            

            
        
    






    
        <title>Building from the source</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">从源头开始构建</h1>
                
            
            
                
<p class="mce-root">您还可以使用GitHub资源库中的所有最新更改来构建Zeppelin。如果要从源代码构建，必须首先安装以下依赖项:</p>
<ul>
<li><strong> Git </strong>:任何版本</li>
<li><strong> Maven </strong> : 3.1.x以上</li>
<li><strong> JDK </strong> : 1.7或更高</li>
</ul>
<p class="mce-root">如果您还没有安装Git和Maven，请在<a href="http://zeppelin.apache.org/docs/latest/install/build.html#build-requirements" target="_blank">http://zeppelin . Apache . org/docs/latest/install/Build . html # Build-requirements</a>查看构建要求。由于篇幅所限，我们没有详细讨论所有步骤。感兴趣的读者应该参考这个网址，更多细节请访问阿帕奇齐柏林飞艇网站，地址是<a href="http://zeppelin.apache.org/" target="_blank">http://zeppelin.apache.org/</a>。</p>


            

            
        
    






    
        <title>Starting and stopping Apache Zeppelin</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">启动和停止Apache Zeppelin</h1>
                
            
            
                
<p class="mce-root">在所有类似UNIX的平台(如Ubuntu、Mac等)上，使用以下命令:</p>
<pre><strong>$ bin/zeppelin-daemon.sh start</strong></pre>
<p class="mce-root">如果上述命令成功执行，您应该在终端上观察到以下日志:</p>
<div><img height="68" width="440" class="alignnone size-full wp-image-276 image-border" src="img/f2e1f12f-b74a-47d1-93bb-e67096f361ad.png"/></div>
<p>图1:从Ubuntu终端启动Zeppelin</p>
<p>如果您在Windows上，请使用以下命令:</p>
<pre><strong>$ binzeppelin.cmd</strong> </pre>
<p class="mce-root">Zeppelin成功启动后，用你的网络浏览器进入<kbd>http://localhost:8080</kbd>，你会看到Zeppelin正在运行。更具体地说，您将在浏览器上看到:</p>
<div><img height="263" width="601" class="alignnone size-full wp-image-277 image-border" src="img/25a4ebe1-b037-46ed-8396-897842d096bf.png"/></div>
<p>图2: Zeppelin运行在http://localhost:8080上</p>
<p class="mce-root">恭喜你！您已经成功安装了阿帕奇齐柏林飞艇！现在让我们在<kbd>http://localhost:8080/</kbd>的浏览器上访问Zeppelin，一旦我们配置好首选解释器，就开始我们的数据分析。现在，要从命令行停止Zeppelin，发出以下命令:</p>
<pre><strong>$ bin/zeppelin-daemon.sh stop</strong></pre>


            

            
        
    






    
        <title>Creating notebooks</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">创建笔记本</h1>
                
            
            
                
<p class="mce-root">一旦你上了<kbd>http://localhost:8080/</kbd>，你可以探索不同的选项和菜单，帮助你了解如何熟悉Zeppelin。更多关于齐柏林飞艇及其用户友好界面的信息，感兴趣的读者可以参考<a href="http://zeppelin.apache.org/docs/latest/" target="_blank">http://zeppelin.apache.org/docs/latest/</a>。现在，让我们首先创建一个示例笔记本，然后开始。如下图所示，您可以通过点击<em>图2 </em>中的新建笔记选项来创建一个新的笔记本:</p>
<div><img height="182" width="375" class="alignnone size-full wp-image-278 image-border" src="img/c900fcff-1938-4c1b-9163-da82e72ff41b.png"/></div>
<p>图3:创建一个样本Zeppelin笔记本</p>
<p class="mce-root">如<em>图3 </em>所示，默认解释器选择为Spark。在下拉列表中，您将只看到Spark，因为您已经为Zeppelin下载了Spark-only二进制包。</p>


            

            
        
    






    
        <title>Exploratory analysis of the dataset</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">数据集的探索性分析</h1>
                
            
            
                
<p class="mce-root">干得好！我们已经能够安装、配置并开始使用Zeppelin。现在我们走吧。我们将看到变量是如何与标签相关联的。我们首先在Apache中加载数据集，如下所示:</p>
<pre class="mce-root"><strong>val</strong> trainDF = spark.read.option("inferSchema", "true")<br/>            .format("com.databricks.spark.csv")<br/>            .option("delimiter", ";")<br/>            .option("header", "true")<br/>            .load("data/bank-additional-full.csv")<br/>trainDF.registerTempTable("trainData")</pre>


            

            
        
    






    
        <title>Label distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">标签分发</h1>
                
            
            
                
<p class="mce-root">我们来看看班级分布。为此，我们将使用SQL解释器。只需在Zeppelin笔记本上执行以下SQL查询:</p>
<pre class="mce-root">%sql select y, count(1) from trainData group by y order by y<br/>&gt;&gt;&gt;</pre>
<div><img height="330" width="596" class="alignnone size-full wp-image-279 image-border" src="img/f96956b3-12cf-4c1e-9024-0f97366e8cf1.png"/></div>


            

            
        
    






    
        <title>Job distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">工作分配</h1>
                
            
            
                
<p class="mce-root">现在让我们看看职位与订阅决策是否有任何关联:</p>
<pre class="mce-root">%sql select job,y, count(1) from trainData group by job, y order by job, y</pre>
<div><img height="905" width="1281" class="alignnone size-full wp-image-280 image-border" src="img/532371f6-b5a0-4848-9ae2-d887205ec555.png"/></div>
<p>从图表中我们可以看到，大多数客户的工作是行政、蓝领或技术人员，而学生和退休客户的比例最大。</p>


            

            
        
    






    
        <title>Marital distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">婚姻分配</h1>
                
            
            
                
<p class="mce-root">婚姻状况与认购决策有关联吗？让我们看看:</p>
<pre class="mce-root">%sql select marital,y, count(1) from trainData group by marital,y order by marital,y<br/>&gt;&gt;&gt;</pre>
<div><img height="300" width="1291" class="alignnone size-full wp-image-281 image-border" src="img/1ff14701-0f9b-44b3-8397-eb053becfa63.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">这种分布表明，无论客户的婚姻状况如何，订阅都与实例的数量成比例。</p>


            

            
        
    






    
        <title>Education distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">教育分布</h1>
                
            
            
                
<p class="mce-root">现在，让我们看看教育状况是否与订阅决定相关:</p>
<pre class="mce-root">%sql select education,y, count(1) from trainData group by education,y order by education,y</pre>
<div><img height="300" width="1291" class="alignnone size-full wp-image-282 image-border" src="img/37719635-692d-47e2-bc3a-2975491b4102.png"/></div>
<p class="mce-root">因此，类似于婚姻状况，教育水平没有给出关于订阅的线索。现在让我们继续探索其他变量。</p>


            

            
        
    






    
        <title>Default distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">默认分布</h1>
                
            
            
                
<p class="mce-root">让我们检查一下违约信用是否与认购决策相关:</p>
<pre class="mce-root">%sql select default,y, count(1) from trainData group by default,y order by default,y</pre>
<div><img height="300" width="1291" class="alignnone size-full wp-image-283 image-border" src="img/fae02864-02b6-43f0-a1a9-3ab41e0559ff.png"/></div>
<p class="mce-root">该图表显示几乎没有违约信贷的客户，没有违约信贷的客户有轻微的认购比率。</p>


            

            
        
    






    
        <title>Housing distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">住房分配</h1>
                
            
            
                
<p class="mce-root">现在让我们看看拥有一栋房子是否与认购决策有着有趣的关联:</p>
<pre class="mce-root">%sql select housing,y, count(1) from trainData group by housing,y order by housing,y</pre>
<div><br/>
<img height="300" width="1291" class="alignnone size-full wp-image-289 image-border" src="img/41a17ae7-7cc5-470e-9c42-04bf60185643.png"/><br/></div>
<p class="mce-root">前面的数字表明，住房也没有提供订阅的线索。</p>


            

            
        
    






    
        <title>Loan distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">贷款分配</h1>
                
            
            
                
<p class="mce-root">现在让我们看看贷款分布:</p>
<pre class="mce-root">%sql select loan,y, count(1) from trainData group by loan,y order by loan,y</pre>
<div><img height="300" width="1291" class="alignnone size-full wp-image-290 image-border" src="img/0952fe25-77cb-45e5-a45a-59606d69a87f.png"/></div>
<p class="mce-root">图表显示，大多数客户没有个人贷款，贷款对认购比例没有影响。</p>


            

            
        
    






    
        <title>Contact distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">接触分布</h1>
                
            
            
                
<p class="mce-root">现在，让我们检查一下联系方式是否与订购决策有重要关联:</p>
<pre class="mce-root">%sql select contact,y, count(1) from trainData group by contact,y order by contact,y</pre>
<div><img height="300" width="1291" class="alignnone size-full wp-image-291 image-border" src="img/91093805-6701-40b3-8197-760d5d51be4d.png"/></div>


            

            
        
    






    
        <title>Month distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">月份分布</h1>
                
            
            
                
<p class="mce-root">这听起来可能很可笑，但是电话销售的月份可能与订阅决定有很大的相关性:</p>
<pre class="mce-root">%sql select month,y, count(1) from trainData group by month,y order by month,y</pre>
<div><img height="300" width="1291" class="alignnone size-full wp-image-288 image-border" src="img/3d762f6c-9761-4bba-931f-1a56c2264df1.png"/></div>
<p class="mce-root">因此，上图显示实例较少的月份(例如，12月、3月、10月和9月)的订阅率最高。</p>


            

            
        
    






    
        <title>Day distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">日分布</h1>
                
            
            
                
<p class="mce-root">那么，星期几及其与订阅决策的相关性如何呢:</p>
<pre class="mce-root">%sql select day_of_week,y, count(1) from trainData group by day_of_week,y order by day_of_week,y</pre>
<div><img src="img/83409632-61b3-45f3-bfca-d347404f5ce9.png"/></div>
<p class="mce-root">日特征具有均匀分布，所以它不是那么重要。</p>


            

            
        
    






    
        <title>Previous outcome distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">先前结果分布</h1>
                
            
            
                
<p class="mce-root">之前的结果及其与订阅决策的相关性如何:</p>
<pre class="mce-root">%sql select poutcome,y, count(1) from trainData group by poutcome,y order by poutcome,y</pre>
<div><img src="img/63c6ed5f-083c-42c5-8f2b-15a6fa28cca1.png"/></div>
<p class="mce-root">该分布显示，在之前的营销活动中取得成功的客户最有可能订阅。同时，这些客户代表了数据集的少数。</p>


            

            
        
    






    
        <title>Age feature</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">年龄特征</h1>
                
            
            
                
<p>让我们看看年龄与订购决策的关系:</p>
<pre class="mce-root">%sql select age,y, count(1) from trainData group by age,y order by age,y</pre>
<div><img height="300" width="1291" class="alignnone size-full wp-image-294 image-border" src="img/131b8d6e-2689-49ce-b137-cfbfc515e96c.png"/></div>
<p class="mce-root">标准化图表显示，大多数客户的年龄在<strong> 25 </strong>到<strong> 60 </strong>之间。</p>
<p class="mce-root">下图显示银行在<em> (25，60) </em>年龄段客户中获得了较高的认购比例。</p>
<div><img height="300" width="1291" class="alignnone size-full wp-image-295 image-border" src="img/edd13366-b56b-4710-8695-f48b18c45f3f.png"/></div>


            

            
        
    






    
        <title>Duration distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">持续时间分布</h1>
                
            
            
                
<p>现在让我们来看看通话时长与套餐的关系:</p>
<pre class="mce-root">%sql select duration,y, count(1) from trainData group by duration,y order by duration,y</pre>
<div><img height="300" width="1291" class="alignnone size-full wp-image-296 image-border" src="img/ac6d0053-9163-4cdc-81da-7eab79b18b99.png"/></div>
<p class="chapter-content">图表显示，大部分通话时间都很短，订阅比例与通话时长成正比。扩展版本提供了更好的洞察力:</p>
<div><img height="300" width="1291" class="alignnone size-full wp-image-297 image-border" src="img/43a919d0-cf55-4db3-9fa9-26f3b90b2d42.png"/></div>


            

            
        
    






    
        <title>Campaign distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">活动分布</h1>
                
            
            
                
<p>现在，我们将了解营销活动分布如何与订阅相关联:</p>
<pre class="mce-root">%sql select campaign, count(1), y from trainData group by campaign,y order by campaign,y</pre>
<div><img height="300" width="1291" class="alignnone size-full wp-image-298 image-border" src="img/57baabd2-03b5-41e0-972c-b4cbd34b151f.png"/></div>
<p class="chapter-content">图表显示，大多数客户被联系的次数少于5次，客户被联系的次数越多，他/她就越不可能订阅。现在，扩展版本提供了更好的洞察力:</p>
<div><img src="img/0a935ae2-27e5-4156-a391-ceb1c3e7bfc0.png"/></div>


            

            
        
    






    
        <title>Pdays distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">Pdays分布</h1>
                
            
            
                
<p>现在让我们来看看<kbd>pdays</kbd>分布如何与订阅相关联:</p>
<pre class="mce-root">%sql select pdays, count(1), y from trainData group by pdays,y order by pdays,y</pre>
<div><img height="300" width="1291" class="alignnone size-full wp-image-309 image-border" src="img/23830f8d-4edf-4be9-a0ce-d255e0d5be8f.png"/></div>
<p class="chapter-content">图表显示，大多数客户之前都没有联系过。</p>


            

            
        
    






    
        <title>Previous distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">以前的分布</h1>
                
            
            
                
<p>在下面的命令中，我们可以看到以前的分发如何影响订阅:</p>
<pre class="mce-root">%sql select previous, count(1), y from trainData group by previous,y order by previous,y</pre>
<div><img height="300" width="1291" class="alignnone size-full wp-image-310 image-border" src="img/c9251b02-5d01-4058-a0ea-4e682fd557d5.png"/></div>
<p class="chapter-content">与上一张图表一样，这张图表证实了之前没有联系过大多数客户来参与此次活动。</p>


            

            
        
    






    
        <title>emp_var_rate distributions</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">员工比率分布</h1>
                
            
            
                
<p>以下命令显示了<kbd>emp_var_rate</kbd>分发如何与订阅相关联:</p>
<pre class="mce-root">%sql select emp_var_rate, count(1), y from trainData group by emp_var_rate,y order by emp_var_rate,y</pre>
<div><img height="300" width="1291" class="alignnone size-full wp-image-311 image-border" src="img/c1bdd55d-b703-423e-8cfc-2f1b003b66c7.png"/></div>
<p class="chapter-content">图表显示，与其他客户相比，具有不太常见的就业变化率的客户更有可能订阅。现在扩展版提供了更好的见解:</p>
<div><img height="300" width="1291" class="alignnone size-full wp-image-312 image-border" src="img/810f87cd-3a6a-4004-a436-784f78bb8091.png"/></div>


            

            
        
    






    
        <title>cons_price_idx features</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">cons_price_idx特性</h1>
                
            
            
                
<p>可以通过以下命令计算<kbd>con_price_idx</kbd>特性和订阅之间的相关性:</p>
<pre class="mce-root">%sql select cons_price_idx, count(1), y from trainData group by cons_price_idx,y order by cons_price_idx,y</pre>
<div><img src="img/6cac30e0-83dc-4271-be8e-f136816db9a9.png"/></div>
<p class="chapter-content">图表显示，与其他客户相比，消费价格指数不常见的客户更有可能订阅。现在，扩展版本提供了更好的洞察力:</p>
<div><img src="img/e1158e07-d282-4269-b3de-bb6fcfbe9133.png"/></div>


            

            
        
    






    
        <title>cons_conf_idx distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">cons_conf_idx分发</h1>
                
            
            
                
<p>通过以下命令可以计算出<kbd>cons_conf_idx</kbd>分发和订阅之间的相关性:</p>
<pre class="mce-root">%sql select cons_conf_idx, count(1), y from trainData group by cons_conf_idx,y order by cons_conf_idx,y</pre>
<div><img src="img/d57b72e9-5e2d-4b6a-97e7-64d876d2f5c1.png"/></div>
<p class="chapter-content">与其他客户相比，消费者信心指数较低的客户更有可能订阅。</p>


            

            
        
    






    
        <title>Euribor3m distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">Euribor3m发行版</h1>
                
            
            
                
<p>让我们看看<kbd>euribor3m</kbd>发行版如何与订阅相关联:</p>
<pre class="mce-root">%sql select euribor3m, count(1), y from trainData group by euribor3m,y order by euribor3m,y</pre>
<div><img src="img/08a74d07-cf57-43a2-8d3c-f289b2137fd7.png"/></div>
<p class="chapter-content">该图表显示，3个月期euribor利率的波动范围很大，大多数客户都集中在该特征的四个或五个值附近。</p>


            

            
        
    






    
        <title>nr_employed distribution</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">nr _就业分布</h1>
                
            
            
                
<p>借助以下命令，可以看到<kbd>nr_employed</kbd>分发和订阅之间的关联:</p>
<pre class="mce-root">%sql select nr_employed, count(1), y from trainData group by nr_employed,y order by nr_employed,y</pre>
<div><img src="img/2345643f-0898-408b-98b3-0b9c37b36312.png"/></div>
<p class="chapter-content">图表显示，订阅率与员工人数成反比。</p>


            

            
        
    






    
        <title>Statistics of numeric features</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">数字特征的统计</h1>
                
            
            
                
<p>我们现在来看看数字特征的统计数据:</p>
<pre class="mce-root"><strong>import</strong> org.apache.spark.sql.types._<br/><br/><strong>val</strong> numericFeatures = trainDF.schema.filter(_.dataType != StringType)<br/><strong>val</strong> description = trainDF.describe(numericFeatures.map(_.name): _*)<br/><br/><strong>val</strong> quantils = numericFeatures<br/>                .map(f=&gt;trainDF.stat.approxQuantile(f.name,                 <br/>                Array(.25,.5,.75),0)).transposeval <br/><br/>rowSeq = Seq(Seq("q1"+:quantils(0): _*),<br/>            Seq("median"+:quantils(1): _*),<br/>            Seq("q3"+:quantils(2): _*))<br/><br/><strong>val</strong> rows = rowSeq.map(s=&gt; s match{ <br/>    <strong>case </strong>Seq(a:String,b:Double,c:Double,d:Double,<br/>             e:Double,f:Double,g:Double,                                              <br/>             h:Double,i:Double,j:Double,k:Double)=&gt; (a,b,c,d,e,f,g,h,i,j,k)})<br/>         <strong>val</strong> allStats = description.unionAll(sc.parallelize(rows).toDF)<br/>         allStats.registerTempTable("allStats")<br/><br/>%sql select * from allStats<br/>&gt;&gt;&gt;</pre>
<table class="MsoTableGrid">
<tbody>
<tr>
<td><kbd>summary</kbd></td>
<td><kbd>age</kbd></td>
<td><kbd>duration</kbd></td>
<td><kbd>campaign</kbd></td>
<td><kbd>pdays</kbd></td>
<td><kbd>previous</kbd></td>
</tr>
<tr>
<td><kbd>count</kbd></td>
<td>41188.00</td>
<td>41188.00</td>
<td>41188.00</td>
<td>41188.00</td>
<td>41188.00</td>
</tr>
<tr>
<td><kbd>mean</kbd></td>
<td>40.02</td>
<td>258.29</td>
<td>2.57</td>
<td>962.48</td>
<td>0.17</td>
</tr>
<tr>
<td><kbd>stddev</kbd></td>
<td>10.42</td>
<td>259.28</td>
<td>2.77</td>
<td>186.91</td>
<td>0.49</td>
</tr>
<tr>
<td><kbd>min</kbd></td>
<td>17.00</td>
<td>0.00</td>
<td>1.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td><kbd>max</kbd></td>
<td>98.00</td>
<td>4918.00</td>
<td>56.00</td>
<td>999.00</td>
<td>7.00</td>
</tr>
<tr>
<td><kbd>q1</kbd></td>
<td>32.00</td>
<td>102.00</td>
<td>1.00</td>
<td>999.00</td>
<td>0.00</td>
</tr>
<tr>
<td><kbd>median</kbd></td>
<td>38.00</td>
<td>180.00</td>
<td>2.00</td>
<td>999.00</td>
<td>0.00</td>
</tr>
<tr>
<td><kbd>q3</kbd></td>
<td>47.00</td>
<td>319.00</td>
<td>3.00</td>
<td>999.00</td>
<td>0.00</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td><kbd>summary</kbd></td>
<td><kbd>emp_var_rate</kbd></td>
<td><kbd>cons_price_idx</kbd></td>
<td><kbd>cons_conf_idx</kbd></td>
<td><kbd>euribor3m</kbd></td>
<td><kbd>nr_employed</kbd></td>
</tr>
<tr>
<td><kbd>count</kbd></td>
<td>41188.00</td>
<td>41188.00</td>
<td>41188.00</td>
<td>41188.00</td>
<td>41188.00</td>
</tr>
<tr>
<td><kbd>mean</kbd></td>
<td>0.08</td>
<td>93.58</td>
<td>-40.50</td>
<td>3.62</td>
<td>5167.04</td>
</tr>
<tr>
<td><kbd>stddev</kbd></td>
<td>1.57</td>
<td>0.58</td>
<td>4.63</td>
<td>1.73</td>
<td>72.25</td>
</tr>
<tr>
<td><kbd>min</kbd></td>
<td>-3.40</td>
<td>92.20</td>
<td>-50.80</td>
<td>0.63</td>
<td>4963.60</td>
</tr>
<tr>
<td><kbd>max</kbd></td>
<td>1.40</td>
<td>94.77</td>
<td>-26.90</td>
<td>5.05</td>
<td>5228.10</td>
</tr>
<tr>
<td><kbd>q1</kbd></td>
<td>-1.80</td>
<td>93.08</td>
<td>-42.70</td>
<td>1.34</td>
<td>5099.10</td>
</tr>
<tr>
<td><kbd>median</kbd></td>
<td>1.10</td>
<td>93.75</td>
<td>-41.80</td>
<td>4.86</td>
<td>5191.00</td>
</tr>
<tr>
<td><kbd>q3</kbd></td>
<td>1.40</td>
<td>93.99</td>
<td>-36.40</td>
<td>4.96</td>
<td>5228.10</td>
</tr>
</tbody>
</table>


            

            
        
    






    
        <title>Implementing a client subscription assessment model</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">实施客户订阅评估模型</h1>
                
            
            
                
<p class="mce-root">为了预测客户订阅评估，我们使用来自H2O的深度学习分类器实现。首先，我们设置并创建一个Spark会话:</p>
<pre class="chapter-content"><strong>val</strong> spark = SparkSession.builder<br/>        .master("local[*]")<br/>        .config("spark.sql.warehouse.dir", "E:/Exp/") // change accordingly<br/>        .appName(s"OneVsRestExample")<br/>        .getOrCreate()</pre>
<p class="mce-root">然后，我们将数据集作为数据框加载:</p>
<pre class="chapter-content">spark.sqlContext.setConf("spark.sql.caseSensitive", "false");<br/><strong>val</strong> trainDF = spark.read.option("inferSchema","true")<br/>            .format("com.databricks.spark.csv")<br/>            .option("delimiter", ";")<br/>            .option("header", "true")<br/>            .load("data/bank-additional-full.csv")</pre>
<p class="chapter-content">尽管该数据集中有分类特征，但由于分类特征的范围很小，因此不需要使用<kbd>StringIndexer</kbd>。通过索引它们，引入了不存在的顺序关系。因此，更好的解决方案是使用一个热编码，事实证明，默认情况下，H2O使用这种编码策略进行枚举。</p>
<p class="chapter-content">在数据集描述中，我已经说明了<kbd>duration</kbd>特性只有在标签已知后才可用。所以不能用来预测。因此，在呼叫客户端之前，我们应该将其删除为不可用:</p>
<pre>val withoutDuration = trainDF.drop("duration")</pre>
<p class="chapter-content">到目前为止，我们已经使用了Sparks内置方法来加载数据集和删除不需要的特性，但是现在我们需要设置<kbd>h2o</kbd>并导入它的隐含:</p>
<pre class="chapter-content"><strong>implicit</strong> val h2oContext = H2OContext.getOrCreate(spark.sparkContext)<br/><strong>import</strong> h2oContext.implicits._implicit <br/><br/><strong>val</strong> sqlContext = SparkSession.builder().getOrCreate().sqlContext<br/><strong>import</strong> sqlContext.implicits._</pre>
<p class="chapter-content">然后，我们混洗训练数据集，并将其转换为H2O帧:</p>
<pre class="chapter-content"><strong>val</strong> H2ODF: H2OFrame = withoutDuration.orderBy(rand())</pre>
<p class="mce-root">然后将字符串特征转换为分类特征(类型“2字节”代表H2O的字符串类型):</p>
<pre class="chapter-content">H2ODF.types.zipWithIndex.foreach(c=&gt; if(c._1.toInt== 2) toCategorical(H2ODF,c._2))</pre>
<p class="chapter-content">在前面的代码行中，<kbd>toCategorical()</kbd>是一个用户定义的函数，用于将字符串特征转换为分类特征。下面是该方法的签名:</p>
<pre class="chapter-content"><strong>def</strong> toCategorical(f: Frame, i: Int): Unit = {f.replace(i,f.vec(i).toCategoricalVec)f.update()}</pre>
<p class="chapter-content">现在是时候将数据集分成60%的训练数据集、20%的验证数据集和20%的测试数据集了:</p>
<pre class="chapter-content"><strong>val</strong> sf = new FrameSplitter(H2ODF, Array(0.6, 0.2), <br/>                            Array("train.hex", "valid.hex", "test.hex")<br/>                            .map(Key.make[Frame](_)), null)<br/><br/>water.H2O.submitTask(sf)<br/><strong>val</strong> splits = sf.getResultval (train, valid, test) = (splits(0), splits(1), splits(2))</pre>
<p class="chapter-content">然后，我们使用训练集训练深度学习模型，并使用验证集验证训练，如下所示:</p>
<pre class="chapter-content"><strong>val</strong> dlModel = buildDLModel(train, valid)</pre>
<p class="chapter-content">在前面的行中，<kbd>buildDLModel()</kbd>是一个用户定义的函数，它设置了一个深度学习模型，并使用训练和验证数据帧来训练它:</p>
<pre class="chapter-content"><strong>def</strong> buildDLModel(train: Frame, valid: Frame,epochs: Int = 10, <br/>                l1: Double = 0.001,l2: Double = 0.0,<br/>                hidden: Array[Int] = Array[Int](256, 256, 256)<br/>               )(implicit h2oContext: H2OContext): <br/>     DeepLearningModel = {import h2oContext.implicits._<br/>                // Build a model<br/>    <strong>val</strong> dlParams = new DeepLearningParameters()<br/>        dlParams._train = traindlParams._valid = valid<br/>        dlParams._response_column = "y"<br/>        dlParams._epochs = epochsdlParams._l1 = l2<br/>        dlParams._hidden = hidden<br/><br/>    <strong>val</strong> dl = new DeepLearning(dlParams, water.Key.make("dlModel.hex"))<br/>    dl.trainModel.get<br/>    }</pre>
<p class="chapter-content">在这段代码中，我们实例化了一个深度学习(即MLP)网络，该网络具有三个隐藏层、L1正则化，并且仅打算迭代训练10次。请注意，这些是超参数，没有进行任何调整。因此，请随意更改这一点，并查看性能以获得一组最佳参数。培训阶段完成后，我们将打印培训指标(即AUC):</p>
<pre class="chapter-content">val auc = dlModel.auc()println("Train AUC: "+auc)<br/>println("Train classification error" + dlModel.classification_error())<br/>&gt;&gt;&gt;<br/>Train AUC: 0.8071186909427446<br/>Train classification error: 0.13293674881631662</pre>
<p class="chapter-content">81%左右的准确率似乎不太好。我们现在在测试集上评估模型。我们预测测试数据集的标签:</p>
<pre class="chapter-content"><strong>val</strong> result = dlModel.score(test)('predict)</pre>
<p class="mce-root">然后我们将原始标签添加到结果中:</p>
<pre class="mce-root">result.add("actual",test.vec("y"))</pre>
<p class="chapter-content">将结果转换成火花数据帧并打印混淆矩阵:</p>
<pre class="chapter-content">val predict_actualDF = h2oContext.asDataFrame(result)predict_actualDF.groupBy("actual","predict").count.show<br/>&gt;&gt;&gt;</pre>
<div><img height="121" width="158" src="img/82558f0e-ad15-4fa7-bdd4-36432e211d52.png"/></div>
<p class="chapter-content">现在，前面的混淆矩阵可以使用Vegas由下面的图来表示:</p>
<pre class="chapter-content CDPAlignLeft CDPAlign">Vegas().withDataFrame(predict_actualDF)<br/>    .mark(Bar)<br/>     .encodeY(field="*", dataType=Quantitative, AggOps.Count, axis=Axis(title="",format=".2f"),hideAxis=true)<br/>    .encodeX("actual", Ord)<br/>    .encodeColor("predict", Nominal, scale=Scale(rangeNominals=List("#FF2800", "#1C39BB")))<br/>    .configMark(stacked=StackOffset.Normalize)<br/>    .show()<br/>&gt;&gt;&gt;</pre>
<div><img height="212" width="338" src="img/c8fe633b-61e1-4663-aaca-6ec8241b7655.png"/></div>
<p>图4:混淆矩阵的图形表示——标准化(左)与非标准化(右)</p>
<p class="chapter-content">现在让我们来看看测试集的总体性能总结，即测试AUC:</p>
<pre class="chapter-content">val trainMetrics = ModelMetricsSupport.modelMetrics[ModelMetricsBinomial](dlModel, test)println(trainMetrics)<br/>&gt;&gt;&gt;</pre>
<div><img height="187" width="446" src="img/f70bf664-ff77-4418-8449-e73fd44992dd.png"/></div>
<p class="chapter-content">所以AUC测试的准确性是76%,这不是很好。但是为什么我们不重复训练更多次(比如1000次)？好吧，我让你决定。但是，我们仍然可以直观地检查精确度-召回曲线，看看评估阶段是如何进行的:</p>
<pre><strong>val</strong> auc = trainMetrics._auc//tp,fp,tn,fn<br/><strong>val</strong> metrics = auc._tps.zip(auc._fps).zipWithIndex.map(x =&gt; x match { <br/>    <strong>case</strong> ((a, b), c) =&gt; (a, b, c) })<br/><br/><strong>val</strong> fullmetrics = metrics.map(_ match { <br/>    <strong>case</strong> (a, b, c) =&gt; (a, b, auc.tn(c), auc.fn(c)) })<br/><br/><strong>val</strong> precisions = fullmetrics.map(_ match {<br/>     <strong>case</strong> (tp, fp, tn, fn) =&gt; tp / (tp + fp) })<br/><br/><strong>val</strong> recalls = fullmetrics.map(_ match { <br/>    <strong>case</strong> (tp, fp, tn, fn) =&gt; tp / (tp + fn) })<br/><br/><strong>val</strong> rows = for (i &lt;- 0 until recalls.length) <br/>    <strong>yield</strong> r(precisions(i), recalls(i))<br/><br/><strong>val</strong> precision_recall = rows.toDF()<br/><br/>//precision vs recall<br/>Vegas("ROC", width = 800, height = 600)<br/>    .withDataFrame(precision_recall).mark(Line)<br/>    .encodeX("re-call", Quantitative)<br/>    .encodeY("precision", Quantitative)<br/>    .show()<br/>&gt;&gt;&gt;</pre>
<div><img height="554" width="740" src="img/f0e403c2-05c7-459c-b50d-34ab5e5340fa.png"/></div>
<p>图5:精确召回曲线</p>
<p>然后，我们计算并绘制灵敏度特异性曲线:</p>
<pre class="chapter-content CDPAlignLeft CDPAlign"><strong>val</strong> sensitivity = fullmetrics.map(_ match { <br/>    <strong>case</strong> (tp, fp, tn, fn) =&gt; tp / (tp + fn) })<br/><br/><strong>val</strong> specificity = fullmetrics.map(_ match {<br/>    <strong>case</strong> (tp, fp, tn, fn) =&gt; tn / (tn + fp) })<br/><strong>val</strong> rows2 = for (i &lt;- 0 until specificity.length) <br/>    <strong>yield</strong> r2(sensitivity(i), specificity(i))<br/><strong>val</strong> sensitivity_specificity = rows2.toDF<br/><br/>Vegas("sensitivity_specificity", width = 800, height = 600)<br/>    .withDataFrame(sensitivity_specificity).mark(Line)<br/>    .encodeX("specificity", Quantitative)<br/>    .encodeY("sensitivity", Quantitative).show()<br/>&gt;&gt;&gt;</pre>
<div><img height="397" width="531" src="img/0c6c8a6a-1879-4928-af84-cd455a2b3eec.png"/></div>
<p>图6:敏感性特异性曲线</p>
<p class="chapter-content CDPAlignLeft CDPAlign">现在，灵敏度特异性曲线告诉我们来自两种标记的正确预测类别之间的关系。例如，如果我们有100%正确预测的欺诈案例，就不会有正确分类的非欺诈案例，反之亦然。最后，通过手动检查不同的预测阈值并计算在两个类别中有多少案例被正确分类，以稍微不同的方式更仔细地查看这一点会很棒。</p>
<p class="chapter-content CDPAlignLeft CDPAlign">更具体地说，我们可以直观地检查超过不同预测阈值的真阳性、假阳性、真阴性和假阴性，比如说<strong> 0.0 </strong>到<strong> 1.0 </strong>:</p>
<pre class="chapter-content CDPAlignLeft CDPAlign"><strong>val</strong> withTh = auc._tps.zip(auc._fps).zipWithIndex.map(x =&gt; x match {<br/>    <strong>case</strong> ((a, b), c) =&gt; (a, b, auc.tn(c), auc.fn(c), auc._ths(c)) })<br/><br/><strong>val</strong> rows3 = for (i &lt;- 0 until withTh.length) <br/>   <strong> yield</strong> r3(withTh(i)._1, withTh(i)._2, withTh(i)._3, withTh(i)._4, withTh(i)._5)</pre>
<p class="mce-root">首先，让我们画出真正积极的一面:</p>
<pre class="chapter-content CDPAlignLeft CDPAlign">Vegas("tp", width = 800, height = 600).withDataFrame(rows3.toDF)<br/>    .mark(Line).encodeX("th", Quantitative)<br/>    .encodeY("tp", Quantitative)<br/>    .show<br/>&gt;&gt;&gt;</pre>
<div><img height="375" width="512" src="img/651bd9cd-c73d-453f-9ee9-3b8a3cff2c1c.png"/></div>
<p>图7:[0.0，1.0]中不同预测阈值的真阳性</p>
<p class="chapter-content CDPAlignLeft CDPAlign">其次，我们来画一个假阳性的:</p>
<pre class="chapter-content CDPAlignLeft CDPAlign">Vegas("fp", width = 800, height = 600)<br/>    .withDataFrame(rows3.toDF).mark(Line)<br/>    .encodeX("th", Quantitative)<br/>    .encodeY("fp", Quantitative)<br/>    .show<br/>&gt;&gt;&gt;</pre>
<div><img height="554" width="740" src="img/92ccd496-3fee-4774-b0ef-bfd227041d9d.png"/></div>
<p>图8:[0.0，1.0]中不同预测阈值的假阳性</p>
<p class="chapter-content CDPAlignLeft CDPAlign">然后，轮到真正消极的人了:</p>
<pre class="chapter-content CDPAlignLeft CDPAlign">Vegas("tn", width = 800, height = 600)<br/>    .withDataFrame(rows3.toDF).mark(Line)<br/>    .encodeX("th", Quantitative)<br/>    .encodeY("tn", Quantitative)<br/>    .show<br/>&gt;&gt;&gt;</pre>
<div><img height="542" width="725" src="img/bc6a1193-63f9-428f-9637-c077bc88c951.png"/></div>
<p>图9:[0.0，1.0]中不同预测阈值的假阳性</p>
<p class="chapter-content CDPAlignLeft CDPAlign">最后，我们把假阴性的画出来如下:</p>
<pre class="chapter-content CDPAlignLeft CDPAlign">Vegas("fn", width = 800, height = 600)<br/>    .withDataFrame(rows3.toDF).mark(Line)<br/>    .encodeX("th", Quantitative)<br/>    .encodeY("fn", Quantitative)<br/>    .show<br/>&gt;&gt;&gt;</pre>
<div><img height="536" width="736" src="img/721dfd31-3f89-4e42-9afa-9001575bec49.png"/></div>
<p>图10:[0.0，1.0]中不同预测阈值的假阳性</p>
<p class="chapter-content CDPAlignLeft CDPAlign">因此，前面的图告诉我们，当我们将预测阈值从默认的<strong> 0.5 </strong>增加到<strong> 0.6 </strong>时，我们可以增加正确分类的非欺诈案例的数量，而不会丢失正确分类的欺诈案例。</p>
<p class="chapter-content CDPAlignLeft CDPAlign">除了这两个辅助方法，我还定义了三个Scala case类用于计算<kbd>precision</kbd>、<kbd>recall</kbd>、<kbd>sensitivity</kbd>、<kbd>specificity</kbd>、真阳性(<kbd>tp</kbd>)、真阴性(<kbd>tn</kbd>)、假阳性(<kbd>fp</kbd>)、假阴性(<kbd>fn</kbd>)等等。签名如下:</p>
<pre class="chapter-content CDPAlignCenter CDPAlign CDPAlignLeft"><strong>case class</strong> r(precision: Double, recall: Double)<br/><strong>case class</strong> r2(sensitivity: Double, specificity: Double)<br/><strong>case class</strong> r3(tp: Double, fp: Double, tn: Double, fn: Double, th: Double)</pre>
<p class="chapter-content CDPAlignCenter CDPAlign CDPAlignLeft">最后，停止火花会议和H2O的背景。<kbd>stop()</kbd>方法调用将分别关闭H2O上下文和Spark集群:</p>
<pre>h2oContext.stop(stopSparkContext = true)<br/>spark.stop()</pre>
<p class="mce-root"/>
<p class="chapter-content CDPAlignCenter CDPAlign CDPAlignLeft">第一个尤其重要；否则，它有时不会停止H2O流，但仍然拥有计算资源。</p>


            

            
        
    






    
        <title>Hyperparameter tuning and feature selection</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">超参数调谐和特征选择</h1>
                
            
            
                
<p class="mce-root">神经网络的灵活性也是其主要缺点之一:有许多超参数需要调整。即使在简单的MLP中，您也可以更改层数、每层神经元的数量、每层中使用的激活函数的类型、历元数、学习速率、权重初始化逻辑、退出保持概率等等。您如何知道什么样的超参数组合最适合您的任务？</p>
<p class="mce-root">当然，你可以使用交叉验证的网格搜索来为线性机器学习模型找到正确的超参数，但对于深度学习模型，有许多超参数需要调整。由于在大型数据集上训练神经网络需要大量时间，因此在合理的时间内，您只能探索超参数空间的一小部分。以下是一些有用的见解。</p>


            

            
        
    






    
        <title>Number of hidden layers</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">隐藏层数</h1>
                
            
            
                
<p class="mce-root">对于许多问题，你可以从一个或两个隐藏层开始，使用两个具有相同神经元总数的隐藏层，在大致相同的训练时间内就可以很好地工作。对于更复杂的问题，您可以逐渐增加隐藏层的数量，直到您开始过度适应训练集。非常复杂的任务，如大型图像分类或语音识别，通常需要几十层的网络，并且需要大量的训练数据。</p>


            

            
        
    






    
        <title>Number of neurons per hidden layer</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">每个隐藏层的神经元数量</h1>
                
            
            
                
<p class="mce-root">显然，输入和输出层中神经元的数量是由任务所需的输入和输出类型决定的。例如，如果您的数据集具有28 x 28的形状，它应该预期具有大小为784的输入神经元，并且输出神经元应该等于要预测的类的数量。</p>
<p class="mce-root">我们已经在这个项目中看到了它在下一个使用MLP的例子中是如何工作的，我们设置了256个神经元，每个隐藏层4个；这只是一个要调整的超参数，而不是每层一个。就像层的数量一样，你可以尝试逐渐增加神经元的数量，直到网络开始过度拟合。</p>


            

            
        
    






    
        <title>Activation functions</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">激活功能</h1>
                
            
            
                
<p class="mce-root">在大多数情况下，您可以在隐藏层中使用ReLU激活功能。与其他激活函数相比，它的计算速度稍快，而且与逻辑函数或双曲正切函数相比，梯度下降不会卡在平台上，逻辑函数或双曲正切函数通常会在一点饱和。</p>
<p class="mce-root">对于输出层，softmax激活函数通常是分类任务的好选择。对于回归任务，您可以简单地使用no activation函数。其他激活功能包括Sigmoid和Tanh。基于H2O的深度学习模型的当前实现支持以下激活功能:</p>
<ul>
<li>实验者</li>
<li>带Dropout的expectiver</li>
<li>最大输出</li>
<li>MaxoutWithDropout</li>
<li>整流器</li>
<li>整流器电压下降</li>
<li>双曲正切</li>
<li>谭威思辍学</li>
</ul>
<p class="mce-root">除了Tanh(H2O默认的)，我没有尝试过这个项目的其他激活功能。但是，你一定要试一试。</p>


            

            
        
    






    
        <title>Weight and bias initialization</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">权重和偏差初始化</h1>
                
            
            
                
<p class="mce-root">初始化隐藏层的权重和偏差是一个需要注意的重要超参数:</p>
<ul>
<li><strong>不要进行全零初始化</strong>:一个听起来合理的想法可能是将所有的初始权重设置为零，但在实践中并不可行，因为如果网络中的每个神经元都计算相同的输出，那么神经元之间就不会存在不对称性，因为它们的权重被初始化为相同。</li>
<li><strong>小随机数</strong>:也可以将神经元的权重初始化为小数值，但不等于零。或者，也可以使用从均匀分布中抽取的小数字。</li>
<li><strong>初始化偏置</strong>:将偏置初始化为零是可能的，也是常见的，因为权重中的小随机数提供了不对称打破。将偏差设置为一个小的常数值，如所有偏差为0.01，可确保所有ReLU单元都能传播梯度。然而，它既没有很好的表现，也没有持续的改进。所以建议坚持归零。</li>
</ul>


            

            
        
    






    
        <title>Regularization</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">正规化</h1>
                
            
            
                
<p class="mce-root">有几种方法可以控制神经网络的训练，以防止训练阶段的过拟合，例如，L2/L1正则化、最大范数约束和丢弃:</p>
<ul>
<li>L2正规化:这可能是最常见的正规化形式。使用梯度下降参数更新，L2正则化表示每个权重将向零线性衰减。</li>
<li><strong> L1正则化</strong>:对于每个权重<em> w </em>，我们将λ∣w∣项添加到目标中。然而，也可以结合L1和L2正则化<em>来实现</em>弹性网正则化。</li>
<li><strong>最大范数约束</strong>:用于对每个隐层神经元的权重向量的大小施加一个绝对上限。然后可以使用投影梯度下降来进一步加强约束。</li>
<li><strong> Dropout </strong>:在处理神经网络时，我们需要另一个用于Dropout的占位符，这是一个要调整的超参数和训练时间，而不是测试时间。它是通过仅保持一个神经元以某种概率活动来实现的，比如说<em> p &lt; 1.0 </em>，否则将其设置为零。这个想法是在测试时使用一个单一的神经网络而不丢失。这个网络的权重是经过训练的权重的缩小版本。如果一个单元在训练过程中保留了<kbd>dropout_keep_prob</kbd> <em> &lt; 1.0 </em>，则该单元的输出权重在测试时乘以<em>p</em>(<em>图17 </em>)。</li>
</ul>
<p>除了这些超参数之外，使用基于H2O的深度学习算法的另一个优势是我们可以获取相对变量/特征重要性。在前面的章节中，我们看到通过在Spark中使用随机森林算法，也可以计算变量重要性。</p>
<p>因此，这个想法是，如果你的模型表现不佳，就应该放弃不太重要的特性，重新进行训练。现在，有可能在监督训练期间发现特征重要性。我观察到了这个特性的重要性:</p>
<div><img height="273" width="419" src="img/39aee4cf-5a16-4e45-80de-72b7c9e6ca4e.png"/></div>
<p>图25:相对变量重要性</p>
<p class="mce-root CDPAlignLeft CDPAlign">现在的问题是:你为什么不扔掉它们，再次尝试训练，看看准确性是否有所提高？好吧，我把它留给读者。</p>


            

            
        
    






    
        <title>Summary</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p class="mce-root">在本章中，我们看到了如何在银行营销数据集上使用H2O开发一个<strong>机器学习</strong> ( <strong> ML </strong>)项目以进行预测分析。我们能够以80%的准确率预测客户将订阅定期存款。此外，我们看到了如何调整典型的神经网络超参数。考虑到这个小规模数据集的事实，最终的改进建议是使用基于Spark的随机森林、决策树或梯度增强树，以获得更好的准确性。</p>
<p class="mce-root">在下一章中，我们将使用超过284，807个信用卡使用实例的数据集，其中只有0.172%的交易是欺诈性的，即高度不平衡的数据。因此，使用自动编码器来预训练分类模型并应用异常检测来预测可能的欺诈交易是有意义的，也就是说，我们希望我们的欺诈案例是整个数据集中的异常。</p>


            

            
        
    


</body></html>