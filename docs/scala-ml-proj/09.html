<html><head/><body>


    
        <title>Fraud Analytics Using Autoencoders and Anomaly Detection</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">使用自动编码器和异常检测进行欺诈分析</h1>
                
            
            
                
<p>检测和防止金融公司(如银行、保险公司和信用合作社)中的欺诈是促进企业发展的一项重要任务。到目前为止，在前一章中，我们已经看到了如何使用经典的监督机器学习模型；现在是时候使用其他无监督的学习算法了，比如自动编码器。</p>
<p>在本章中，我们将使用超过284，807个信用卡使用实例的数据集，并且对于每笔交易，只有0.172%的交易是欺诈性的。所以，这是高度不平衡的数据。因此使用自动编码器来预训练分类模型并应用异常检测技术来预测可能的欺诈交易是有意义的；也就是说，我们期望我们的欺诈案例是整个数据集中的异常。</p>
<p>总之，我们将通过这个端到端项目了解以下主题:</p>
<ul>
<li>异常值和使用异常值的异常检测</li>
<li>在无监督学习中使用自动编码器</li>
<li>开发欺诈分析预测模型</li>
<li>超参数调整，最重要的是特性选择</li>
</ul>


            

            
        
    






    
        <title>Outlier and anomaly detection</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">异常值和异常检测</h1>
                
            
            
                
<p>异常是观察到的世界中不寻常和意想不到的模式。因此，从可见和不可见的数据中分析、识别、理解和预测异常是数据挖掘中最重要的任务之一。因此，检测异常可以从数据中提取关键信息，然后用于多种应用。</p>
<p>虽然异常是一个普遍接受的术语，但其他同义词，如异常值、不一致的观察值、例外、异常、意外、异常或污染，经常用于不同的应用领域。特别是，异常值和异常值经常互换使用。异常检测广泛应用于信用卡、保险或医疗保健的欺诈检测、网络安全的入侵检测、安全关键系统的故障检测以及敌方活动的军事监视。</p>
<p>异常检测的重要性源于这样一个事实，即对于各种应用领域，数据中的异常通常会转化为重要的可操作见解。当我们开始探索一个高度不平衡的数据集时，使用峰度有三种可能的数据集解释。因此，在应用特征工程之前，需要通过数据探索来回答和了解以下问题:</p>
<ul>
<li>对于所有可用字段，存在或没有空值或缺失值的数据占总数据的百分比是多少？然后尝试处理那些丢失的值，并在不丢失数据语义的情况下很好地解释它们。</li>
<li>字段之间的相关性是什么？每个字段与预测变量的相关性如何？它们取什么值(即，分类的或关于分类的，数字的或字母数字的，等等)？</li>
</ul>
<p>然后找出数据分布是否偏斜。您可以通过查看异常值或长尾来确定偏斜度(稍微向右偏斜或正偏斜，稍微向左偏斜或负偏斜，如图1所示)。现在确定异常值是否有助于做出预测。从统计学角度来看，您的数据具有如下3种可能的峰度之一:</p>
<ul>
<li>如果峰度小于但几乎等于3，则为中峰度</li>
<li>如果峰度的度量大于3，则为细峰度</li>
<li>如果峰度小于3，则为平峰度</li>
</ul>
<div><img height="318" width="507" class="alignnone size-full wp-image-541 image-border" src="img/1fa1b01a-9ef4-40c5-9b15-4fc6f24f7ff0.png"/></div>
<p>图1:不平衡数据集中不同种类的偏斜</p>
<p>举个例子吧。假设你对健身散步感兴趣，并且在过去四周(不包括周末)在运动场或乡村散步。你用了以下时间(以分钟为单位完成4公里的步行赛道):15、16、18、17.16、16.5、18.6、19.0、20.4、20.6、25.15、27.27、25.24、21.05、21.65、20.92、22.61、23.71、35、39、50。使用R计算并解释这些值的偏斜度和峰度将产生如下密度图。</p>
<p><em>图2 </em>中对数据分布(锻炼次数)的解释显示，密度图向右倾斜，leptokurtic也是如此。因此，对于我们的用例，数据指向最右边的位置可以被认为是不寻常或可疑的。因此，我们可以潜在地识别或删除它们，以使我们的数据集平衡。然而，这不是这个项目的目的，只有识别才是。</p>
<div><img height="327" width="504" class="alignnone size-full wp-image-542 image-border" src="img/7b9c075a-e64b-4e9d-8ed8-591699664df1.png"/></div>
<p>图2:锻炼时间直方图(右偏)</p>
<p>然而，通过消除长尾效应，我们无法完全消除失衡。还有另一种称为异常值检测的解决方法，删除这些数据点会很有用。</p>
<p>此外，我们还可以查看每个单独特征的箱线图。其中箱线图显示基于五个数字汇总的数据分布:<strong>最小值</strong>、<strong>第一个四分位数</strong>、中值、<strong>第三个四分位数</strong>和<strong>最大值</strong>，如图<em>图3 </em>所示，其中我们可以查找超出三(3) <strong>四分位数间范围</strong> ( <strong> IQR </strong>)的异常值:</p>
<div><img height="289" width="253" class="alignnone size-full wp-image-543 image-border" src="img/e5b1e2a8-6ec6-4f45-af32-39e594a0f7d0.png"/></div>
<p>图3:超出三(3)个四分位数范围的异常值(IQR)</p>
<p>因此，探索移除长尾是否可以为监督或无监督学习提供更好的预测将是有用的。但是对于这种高度不平衡的数据集，没有具体的建议。简而言之，偏态分析在这方面对我们没有帮助。</p>
<p>最后，如果您观察到您的模型不能为您提供完美的分类，但是<strong>均方差</strong> ( <strong> MSE </strong>)可以提供一些发现异常值或异常值的线索。例如，在我们的案例中，即使我们的预测模型无法将您的数据集分类为欺诈和非欺诈案例，但欺诈交易的平均MSE肯定高于常规交易。因此，即使这听起来很天真，我们仍然可以通过应用MSE阈值来识别异常值。例如，我们可以将MSE &gt;为0.02的实例视为异常/异常值。</p>
<p>现在的问题是我们如何做到这一点？嗯，通过这个端到端的项目，我们将看到如何使用自动编码器和异常检测。我们还将看到如何使用自动编码器来预先训练一个分类模型。最后，我们将看到如何测量不平衡数据的模型性能。让我们从了解自动编码器开始。</p>


            

            
        
    






    
        <title>Autoencoders and unsupervised learning</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">自动编码器和无监督学习</h1>
                
            
            
                
<p>自动编码器是人工神经网络，能够在没有任何监督的情况下学习输入数据的有效表示(即，训练集是无标签的)。这种编码通常具有比输入数据低得多的维数，使得自动编码器对于维数减少是有用的。更重要的是，自动编码器充当了强大的特征检测器，它们可以用于深度神经网络的无监督预训练。</p>


            

            
        
    






    
        <title>Working principles of an autoencoder</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">自动编码器的工作原理</h1>
                
            
            
                
<p>自动编码器是具有三层或更多层的网络，其中输入层和输出层具有相同数量的神经元，而中间(隐藏)层具有较少数量的神经元。网络被训练成对于每个输入数据，在输出中简单地再现输入中相同的活动模式。问题的显著方面是，由于隐藏层中神经元的数量较少，如果网络可以从示例中学习，并归纳到可接受的程度，则它执行数据压缩:隐藏神经元的状态为每个示例提供了输入和输出公共状态的压缩版本。</p>
<p>问题的显著方面是，由于隐藏层中神经元的数量较少，如果网络可以从示例中学习，并在可接受的范围内进行归纳，它会执行<em>数据压缩</em>:隐藏神经元的状态为每个示例提供了<em>输入</em>和<em>输出公共状态</em>的<em>压缩版本</em>。自动编码器的有用应用是用于数据可视化的<strong>数据去噪</strong>和<strong>维度</strong> <strong>缩减</strong>。</p>
<p>下面的架构显示了自动编码器通常是如何工作的。它通过两个阶段重构接收到的输入:对应于原始输入<em>、</em>的维度缩减的编码阶段，以及能够从编码(压缩)表示重构原始输入的解码阶段:</p>
<div><img height="260" width="704" class="alignnone size-full wp-image-545 image-border" src="img/844e530e-9cd2-4222-95d4-a3e29a4cd44a.png"/></div>
<p>图4:自动编码器中的编码器和解码器阶段</p>
<p>作为一个无监督的神经网络，自动编码器的主要特点是它的对称结构。自动编码器有两个组件:将输入转换为内部表示的编码器，后面是将内部表示转换回输出的解码器。换句话说，自动编码器可以看作是编码器和解码器的组合，前者将一些输入编码成代码，后者将代码解码/重构回原始输入作为输出。因此，<strong>多层感知器</strong> ( <strong> MLP </strong>)通常具有与自动编码器相同的架构，除了输出层中神经元的数量必须等于输入的数量。</p>
<p>如前所述，训练自动编码器的方法不止一种。第一种是一次性训练整个层，类似于MLP。尽管在计算成本函数时(如在监督学习中)，我们没有使用一些标记的输出，而是使用输入本身。因此，<kbd>cost</kbd>函数显示了实际输入和重构输入之间的差异。</p>
<p>第二种方法是贪婪地一次训练一层。这种训练实现来自监督学习(例如，分类)中的反向传播方法所产生的问题。在具有大量层的网络中，反向传播方法在梯度计算中变得非常慢且不准确。为了解决这个问题，Geoffrey Hinton应用了一些预训练方法来初始化分类权重，并且这种预训练方法一次对两个相邻层进行。</p>


            

            
        
    






    
        <title>Efficient data representation with autoencoders</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">使用自动编码器的高效数据表示</h1>
                
            
            
                
<p>困扰所有监督学习系统的一个大问题是所谓的<strong>维数灾难</strong>:在增加输入空间维数的同时，性能逐渐下降。这是因为获得输入空间的足够采样所需的样本数量随着维数呈指数增长。为了克服这些问题，已经开发了一些优化网络。</p>
<p>第一个是自动编码器网络:这些网络被设计和训练用于转换输入模式本身，以便在存在输入模式的降级或不完整版本的情况下，有可能获得原始模式。训练网络以创建输出数据，例如在入口中呈现的数据，隐藏层存储压缩的数据，即捕获输入数据的基本特征的紧凑表示。</p>
<p>第二种优化网络是<strong>波尔兹曼机器</strong>:这类网络由一个输入/输出可见层和一个隐藏层组成。可见层和隐藏层之间的连接是无方向性的:数据可以双向传输，可见-隐藏和隐藏-可见，不同的神经元单元可以完全连接或部分连接。</p>
<p>让我们看一个例子。决定以下哪个系列你认为更容易记忆:</p>
<ul>
<li>45, 13, 37, 11, 23, 90, 79, 24, 87, 47</li>
<li>50, 25, 76, 38, 19, 58, 29, 88, 44, 22, 11, 34, 17, 52, 26, 13, 40, 20</li>
</ul>
<p>看到前面的两个系列，似乎第一个系列对人类来说更容易，因为它比第二个系列更短，只包含几个数字。然而，如果你仔细观察第二个数列，你会发现偶数正好是后面数字的两倍。而奇数后面是一个数字乘以3加1。这是一个著名的数字序列，叫做<strong>冰雹序列</strong>。</p>
<p>但是，如果你能轻松地记住长序列，你也能轻松快速地识别数据中的模式。在20世纪70年代，研究人员观察到，国际象棋专家只要看棋盘5秒钟，就能记住一局棋中所有棋子的位置。这听起来可能有争议，但象棋专家的记忆力并不比你我强。事实是，他们比非棋手更容易理解国际象棋的模式。自动编码器首先观察输入，将它们转换为更好的内部表示，并可以接收类似于它已经学习到的内容:</p>
<div><img height="269" width="465" class="alignnone size-full wp-image-546 image-border" src="img/6b8afe47-97cc-4bc8-8cd9-12e01f78432d.png"/></div>
<p>图5:象棋游戏视角中的自动编码器</p>
<p>让我们看看一个关于我们刚刚讨论的象棋例子的更现实的图:隐藏层有两个神经元(即编码器本身)，而输出层有三个神经元(换句话说，解码器)。因为内部表示具有比输入数据更低的维度(它是2D而不是3D)，所以自动编码器被认为是不完全的。一个不完整的自动编码器不能简单地将其输入复制到编码中，但它必须找到一种方法来输出其输入的副本。</p>
<p>它被迫学习输入数据中最重要的特征，并丢弃不重要的特征。这样，自动编码器可以与<strong>主成分分析</strong> ( <strong> PCA </strong>)进行比较，后者用于使用比原来更少的维数来表示给定的输入。</p>
<p>至此，我们知道了自动编码器是如何工作的。现在，了解使用异常值识别的异常检测是值得的。</p>


            

            
        
    






    
        <title>Developing a fraud analytics model</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">开发欺诈分析模型</h1>
                
            
            
                
<p>在我们完全开始之前，我们需要做两件事:了解数据集，然后准备我们的编程环境。</p>


            

            
        
    






    
        <title>Description of the dataset and using linear models</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">数据集的描述和线性模型的使用</h1>
                
            
            
                
<p>对于这个项目，我们将使用来自Kaggle的信用卡欺诈检测数据集。数据集可以从<a href="https://www.kaggle.com/dalpozz/creditcardfraud">https://www.kaggle.com/dalpozz/creditcardfraud</a>下载。因为我正在使用数据集，所以引用以下出版物是一个透明的好主意:</p>
<ul>
<li>Andrea Dal Pozzolo、Olivier Caelen、Reid A. Johnson和gian Luca Bontempi<em>利用欠采样校准不平衡分类的概率</em>。在2015年IEEE<strong>计算智能和数据挖掘</strong> ( <strong> CIDM </strong>)研讨会上。</li>
</ul>
<p>这些数据集包含了欧洲持卡人在2013年9月仅两天内的信用卡交易。总共有285，299笔交易，在284，807笔交易中只有492笔欺诈，这意味着数据集高度不平衡，正类(欺诈)占所有交易的0.172%。</p>
<p>它只包含数字输入变量，这些变量是PCA变换的结果。不幸的是，由于保密问题，我们不能提供原始功能和更多的数据背景信息。共有28个特征，分别是<kbd>V1</kbd>、<kbd>V2</kbd>、...、<kbd>V28</kbd>，除了<kbd>Time</kbd>和<kbd>Amount</kbd>之外，都是用PCA得到的主要成分。特征<kbd>Class</kbd>是响应变量，在欺诈的情况下取值1，否则取值0。我们稍后会看到细节。</p>


            

            
        
    






    
        <title>Problem description</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">问题描述</h1>
                
            
            
                
<p>考虑到类别不平衡率，我们建议使用精度-召回曲线 ( <strong> AUPRC </strong>)下的<strong>区域测量精度。混淆矩阵精度对于不平衡分类没有意义。关于这一点，通过应用过采样或欠采样技术，使用线性机器学习模型，如随机森林、逻辑回归或支持向量机。或者，我们可以尝试在数据中发现异常，因为假设只有少数欺诈案例是整个数据集中的异常。</strong></p>
<p>在处理如此严重的响应标签不平衡时，我们还需要在测量模型性能时小心谨慎。因为只有少数几个欺诈的例子，一个预测一切都不是欺诈的模型将已经达到超过99%的准确性。尽管准确率很高，但线性机器学习模型不一定能帮助我们发现欺诈案件。</p>
<p>因此，探索深度学习模型，如自动编码器，将是值得的。此外，我们需要使用异常检测来发现异常。特别是，我们将看到如何使用自动编码器预先训练一个分类模型，并衡量不平衡数据的模型性能。</p>


            

            
        
    






    
        <title>Preparing programming environment</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">准备编程环境</h1>
                
            
            
                
<p>特别是，我将在这个项目中使用几种工具和技术。以下是解释每项技术的列表:</p>
<ul>
<li><strong>H2O/火花水</strong>:针对深度学习平台(详见上一章)</li>
<li><strong> Apache Spark </strong>:用于数据处理环境</li>
<li>维加斯:Matplotlib的替代品，类似于Python，用于绘图。它可以与Spark集成用于绘图目的</li>
<li>Scala:我们项目的编程语言</li>
</ul>
<p>嗯，我将创建一个Maven项目，其中所有的依赖项都将被注入到<kbd>pom.xml</kbd>文件中。<kbd>pom.xml</kbd>的完整内容可以从Packt存储库中下载。所以让我们开始吧:</p>
<pre>&lt;dependencies&gt;<br/>   &lt;dependency&gt;<br/>      &lt;groupId&gt;ai.h2o&lt;/groupId&gt;<br/>      &lt;artifactId&gt;sparkling-water-core_2.11&lt;/artifactId&gt;<br/>      &lt;version&gt;2.2.2&lt;/version&gt;<br/>   &lt;/dependency&gt;<br/>   &lt;dependency&gt;<br/>      &lt;groupId&gt;org.vegas-viz&lt;/groupId&gt;<br/>      &lt;artifactId&gt;vegas_2.11&lt;/artifactId&gt;<br/>      &lt;version&gt;0.3.11&lt;/version&gt;<br/>   &lt;/dependency&gt;<br/>   &lt;dependency&gt;<br/>     &lt;groupId&gt;org.vegas-viz&lt;/groupId&gt;<br/>     &lt;artifactId&gt;vegas-spark_2.11&lt;/artifactId&gt;<br/>     &lt;version&gt;0.3.11&lt;/version&gt;<br/>     &lt;/dependency&gt;<br/>&lt;/dependencies&gt;</pre>
<p>现在，Eclipse或您最喜欢的IDE将提取所有的依赖项。第一个依赖项还将提取与这个H2O版本兼容的所有Spark相关的依赖项。然后，创建一个Scala文件并提供一个合适的名称。然后我们准备出发。</p>


            

            
        
    






    
        <title>Step 1 - Loading required packages and libraries</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤1 -加载所需的包和库</h1>
                
            
            
                
<p>因此，让我们从导入所需的库和包开始:</p>
<pre><strong>package</strong> com.packt.ScalaML.FraudDetection<br/><br/><strong>import</strong> org.apache.spark.sql.SparkSession<br/><strong>import</strong> org.apache.spark.sql.functions._<br/><strong>import</strong> org.apache.spark.sql._<br/><strong>import</strong> org.apache.spark.h2o._<br/><strong>import</strong> _root_.hex.FrameSplitter<br/><strong>import</strong> water.Key<br/><strong>import</strong> water.fvec.Frame<br/><strong>import</strong> _root_.hex.deeplearning.DeepLearning<br/><strong>import</strong> _root_.hex.deeplearning.DeepLearningModel.DeepLearningParameters<br/><strong>import</strong> _root_.hex.deeplearning.DeepLearningModel.DeepLearningParameters.Activation<br/><strong>import</strong> java.io.File<br/><strong>import</strong> water.support.ModelSerializationSupport<br/><strong>import</strong> _root_.hex.{ ModelMetricsBinomial, ModelMetrics }<br/><strong>import</strong> org.apache.spark.h2o._<br/><strong>import</strong> scala.reflect.api.materializeTypeTag<br/><strong>import</strong> water.support.ModelSerializationSupport<br/><strong>import</strong> water.support.ModelMetricsSupport<br/><strong>import</strong> _root_.hex.deeplearning.DeepLearningModel<br/><strong>import</strong> vegas._<br/><strong>import</strong> vegas.sparkExt._<br/><strong>import</strong> org.apache.spark.sql.types._</pre>


            

            
        
    






    
        <title>Step 2 - Creating a Spark session and importing implicits</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤2 -创建Spark会话并导入隐含</h1>
                
            
            
                
<p>然后我们需要创建一个Spark会话作为我们程序的入口:</p>
<pre><strong>val</strong> spark = SparkSession<br/>        .builder<br/>        .master("local[*]")<br/>        .config("spark.sql.warehouse.dir", "tmp/")<br/>        .appName("Fraud Detection")<br/>        .getOrCreate()</pre>
<p class="mce-root">此外，我们需要为spark.sql和h2o导入隐含:</p>
<pre><strong>implicit </strong><strong>val</strong> sqlContext = spark.sqlContext<br/><strong>import</strong> sqlContext.implicits._<br/><strong>val</strong> h2oContext = H2OContext.getOrCreate(spark)<br/><strong>import</strong> h2oContext._<br/><strong>import</strong> h2oContext.implicits._</pre>


            

            
        
    






    
        <title>Step 3 - Loading and parsing input data</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤3 -加载和解析输入数据</h1>
                
            
            
                
<p>我们加载并获取事务。然后我们得到分布:</p>
<pre><strong>val</strong> inputCSV = "data/creditcard.csv"<br/><br/><strong>val</strong> transactions = spark.read.format("com.databricks.spark.csv")<br/>        .option("header", "true")<br/>        .option("inferSchema", <strong>true</strong>)<br/>        .load(inputCSV)</pre>


            

            
        
    






    
        <title>Step 4 - Exploratory analysis of the input data</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤4 -输入数据的探索性分析</h1>
                
            
            
                
<p>如前所述，数据集包含数值输入变量<kbd>V1</kbd>到<kbd>V28</kbd>，它们是原始特征的PCA变换的结果。响应变量<kbd>Class</kbd>告诉我们一个交易是欺诈性的(值= 1)还是非欺诈性的(值= 0)。</p>
<p>还有两个附加功能，<kbd>Time</kbd>和<kbd>Amount</kbd>。<kbd>Time</kbd>列表示当前事务和第一个事务之间的时间，以秒为单位。而<kbd>Amount</kbd>列表示在该交易中转移了多少钱。因此，让我们看一下<em>图6 </em>中的输入数据(虽然只显示了<kbd>V1</kbd>、<kbd>V2</kbd>、<kbd>V26</kbd>和<kbd>V27</kbd>):</p>
<div><img height="263" width="681" class="alignnone size-full wp-image-162 image-border" src="img/29ef9fd6-5183-47f2-8d32-6715d749d7cf.png"/></div>
<p>图6:信用卡欺诈检测数据集的快照</p>
<p>我们已经能够加载事务，但是前面的数据帧没有告诉我们关于类分布的信息。所以，让我们来计算类分布，并考虑绘制它们:</p>
<pre><strong>val</strong> distribution = transactions.groupBy("Class").count.collect<br/>Vegas("Class Distribution").withData(distribution.map(r =&gt; Map("class" -&gt; r(0), "count" -&gt; r(1)))).encodeX("class", Nom).encodeY("count", Quant).mark(Bar).show<br/>&gt;&gt;&gt;</pre>
<div><img height="242" width="124" src="img/df46daf3-9596-4e75-9ade-2d93f781f36e.png"/></div>
<p>图7:信用卡欺诈检测数据集中的类别分布</p>
<p>现在，让我们看看时间对可疑交易是否有任何重要贡献。<kbd>Time</kbd>列告诉我们交易完成的顺序，但不告诉我们交易的实际时间(即一天中的时间)。因此，按天对它们进行规范化，并根据一天中的时间将它们宁滨成四组，以从<kbd>Time</kbd>构建一个<kbd>Day</kbd>列将会很有用。我为此写过一篇UDF:</p>
<pre><strong>val</strong> daysUDf = udf((s: Double) =&gt; <br/><strong>if</strong> (s &gt; 3600 * 24) "day2" <br/><strong>else</strong> "day1")<br/><br/><strong>val</strong> t1 = transactions.withColumn("day", daysUDf(col("Time")))<br/><strong>val</strong> dayDist = t1.groupBy("day").count.collect</pre>
<p class="mce-root">现在让我们画出来:</p>
<pre>Vegas("Day Distribution").withData(dayDist.map(r =&gt; Map("day" -&gt; r(0), "count" -&gt; r(1)))).encodeX("day", Nom).encodeY("count", Quant).mark(Bar).show<br/>&gt;&gt;&gt;</pre>
<div><img height="251" width="121" src="img/e01f7590-6be0-4115-acac-8410a5dbf6bf.png"/></div>
<p>图8:信用卡欺诈检测数据集中的日分布</p>
<p>上图显示，这两天的交易数量相同，但更具体地说，在<kbd>day1</kbd>进行的交易略多。现在让我们构建<kbd>dayTime</kbd>列。我又为它写了一首UDF:</p>
<pre><strong>val</strong> dayTimeUDf = udf((day: String, t: Double) =&gt; <strong>if</strong> (day == "day2") t - 86400 <strong>else</strong> t)<br/><strong>val</strong> t2 = t1.withColumn("dayTime", dayTimeUDf(col("day"), col("Time")))<br/><br/>t2.describe("dayTime").show()<br/>&gt;&gt;&gt;<br/>+-------+------------------+<br/>|summary| dayTime |<br/>+-------+------------------+<br/>| count| 284807|<br/>| mean| 52336.926072744|<br/>| stddev|21049.288810608432|<br/>| min| 0.0|<br/>| max| 86400.0|<br/>+-------+------------------+</pre>
<p>现在我们需要得到分位数(<kbd>q1</kbd>、中位数、<kbd>q2</kbd>)和构建时间仓(<kbd>gr1</kbd>、<kbd>gr2</kbd>、<kbd>gr3</kbd>和<kbd>gr4</kbd>):</p>
<pre><br/><strong>val</strong> d1 = t2.filter($"day" === "day1")<br/><strong>val</strong> d2 = t2.filter($"day" === "day2")<br/><strong>val</strong> quantiles1 = d1.stat.approxQuantile("dayTime", Array(0.25, 0.5, 0.75), 0)<br/><br/><strong>val</strong> quantiles2 = d2.stat.approxQuantile("dayTime", Array(0.25, 0.5, 0.75), 0)<br/><br/><strong>val</strong> bagsUDf = udf((t: Double) =&gt; <br/><strong>    if</strong> (t &lt;= (quantiles1(0) + quantiles2(0)) / 2) "gr1" <br/><strong>    else</strong><strong>if</strong> (t &lt;= (quantiles1(1) + quantiles2(1)) / 2) "gr2" <br/><strong>    else</strong><strong>if</strong> (t &lt;= (quantiles1(2) + quantiles2(2)) / 2) "gr3" <br/><strong>    else</strong> "gr4")<br/><br/><strong>val</strong> t3 = t2.drop(col("Time")).withColumn("Time", bagsUDf(col("dayTime")))</pre>
<p>然后让我们得到<kbd>0</kbd>和<kbd>1</kbd>类的分布:</p>
<pre><strong>val</strong> grDist = t3.groupBy("Time", "class").count.collect<br/><strong>val</strong> grDistByClass = grDist.groupBy(_(1))</pre>
<p>现在让我们绘制<kbd>0</kbd>类的组分布图:</p>
<pre>Vegas("gr Distribution").withData(grDistByClass.get(0).get.map(r =&gt; Map("Time" -&gt; r(0), "count" -&gt; r(2)))).encodeX("Time", Nom).encodeY("count", Quant).mark(Bar).show<br/>&gt;&gt;&gt;</pre>
<div><img height="215" width="135" src="img/29070829-7d9b-40e4-918c-7d4de5fe8913.png"/></div>
<p>图9:信用卡欺诈检测数据集中类别0的分组分布</p>
<p>从上图中可以清楚地看到，大部分都是正常交易。现在让我们看看<kbd>class 1</kbd>的组分布:</p>
<pre>Vegas("gr Distribution").withData(grDistByClass.get(1).get.map(r =&gt; Map("Time" -&gt; r(0), "count" -&gt; r(2)))).encodeX("Time", Nom).encodeY("count", Quant).mark(Bar).show<br/>&gt;&gt;&gt;</pre>
<div><img height="215" width="136" src="img/077d35cb-67f7-4002-82d8-9b53fb78d8ad.png"/></div>
<p>图10:信用卡欺诈检测数据集中类别1的分组分布</p>
<p>因此，交易在四个<strong>时间</strong>箱中的分布显示，大多数欺诈案例发生在组1中。我们当然可以看看被转移资金的分布情况:</p>
<pre><strong>val</strong> c0Amount = t3.filter($"Class" === "0").select("Amount")<br/><strong>val</strong> c1Amount = t3.filter($"Class" === "1").select("Amount")<br/><br/>println(c0Amount.stat.approxQuantile("Amount", Array(0.25, 0.5, 0.75), 0).mkString(","))<br/><br/>Vegas("Amounts for class 0").withDataFrame(c0Amount).mark(Bar).encodeX("Amount", Quantitative, bin = Bin(50.0)).encodeY(field = "*", Quantitative, aggregate = AggOps.Count).show<br/>&gt;&gt;&gt;</pre>
<div><img height="244" width="243" src="img/491ab0c9-6738-411e-a8a2-fc6e00edba83.png"/></div>
<p>图11:为类别0转移的金额分布</p>
<p>现在让我们为<kbd>class 1</kbd>绘制同样的图形:</p>
<pre>Vegas("Amounts for class 1").withDataFrame(c1Amount).mark(Bar).encodeX("Amount", Quantitative, bin = Bin(50.0)).encodeY(field = "*", Quantitative, aggregate = AggOps.Count).show<br/>&gt;&gt;&gt;</pre>
<div><img height="251" width="243" src="img/3dfd4828-7a0d-44e5-902e-4ab6e1c6dc8b.png"/></div>
<p>图12:为类别1转移的金额分布</p>
<p>因此，从前面两张图中可以看出，与正常交易相比，欺诈性信用卡交易的平均转账金额更高，但最大金额却低得多。正如我们在手工构建的<kbd>dayTime</kbd>列中看到的，它并不重要，所以我们可以简单地删除它。让我们开始吧:</p>
<pre><strong>val</strong> t4 = t3.drop("day").drop("dayTime")</pre>


            

            
        
    






    
        <title>Step 5 - Preparing the H2O DataFrame</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤5 -准备H2O数据框架</h1>
                
            
            
                
<p>至此，我们的数据帧(即<kbd>t4</kbd>)在Spark数据帧中。但它不能被H2O模式所消耗。所以，我们必须把它转换成H2O框架。所以让我们开始吧:</p>
<pre><strong>val</strong> creditcard_hf: H2OFrame = h2oContext.asH2OFrame(t4.orderBy(rand()))</pre>
<p>我们使用名为FrameSplitter的H2O内置拆分器将数据集拆分为40%监督训练、40%非监督训练和20%测试:</p>
<pre><strong>val</strong> sf = <strong>new</strong> FrameSplitter(creditcard_hf, Array(.4, .4), <br/>                Array("train_unsupervised", "train_supervised", "test")<br/>                .map(Key.make[Frame](_)), <strong>null</strong>)<br/><br/>water.H2O.submitTask(sf)<br/><strong>val</strong> splits = sf.getResult<br/><strong>val</strong> (train_unsupervised, train_supervised, test) = (splits(0), splits(1), splits(2))</pre>
<p>在上面的代码段中，<kbd>Key.make[Frame](_)</kbd>被用作一个底层任务，根据拆分比率来拆分帧，这也有助于获得分布式的键/值对。</p>
<div><p>密钥在H2O计算中非常重要。H2O支持分布式键/值存储，具有精确的Java内存模型一致性。事情是这样的，键是一种在云中的某个地方找到链接值的方法，本地缓存它，允许全局一致地更新链接值。</p>
</div>
<p>最后，我们需要显式地将<kbd>Time</kbd>列从字符串转换为分类(即<strong>枚举</strong>):</p>
<pre>toCategorical(train_unsupervised, 30)<br/>toCategorical(train_supervised, 30)<br/>toCategorical(test, 30)</pre>


            

            
        
    






    
        <title>Step 6 - Unsupervised pre-training using autoencoder</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤6 -使用autoencoder进行无监督的预训练</h1>
                
            
            
                
<p>如前所述，我们将使用Scala和<kbd>h2o</kbd>编码器。现在是时候开始无监督的自动编码器训练了。由于训练是无监督的，这意味着我们需要从无监督的训练集中排除<kbd>response</kbd>列:</p>
<pre><strong>val</strong> response = "Class"<br/><strong>val</strong> features = train_unsupervised.names.filterNot(_ == response)</pre>
<p>下一个任务是定义超参数，例如具有神经元的隐藏层的数量、可重复性的种子、训练时期的数量和深度学习模型的激活函数。对于无监督的预训练，只需将自动编码器参数设置为<kbd>true</kbd>:</p>
<pre><strong>var</strong> dlParams = <strong>new</strong> DeepLearningParameters()<br/>    dlParams._ignored_columns = Array(response))// since unsupervised, we ignore the label<br/>    dlParams._train = train_unsupervised._key // use the train_unsupervised frame for training<br/>    dlParams._autoencoder = <strong>true </strong>// use H2O built-in autoencoder<strong><br/></strong>    dlParams._reproducible = <strong>true </strong>// ensure reproducibility<strong><br/></strong>    dlParams._seed = 42 // random seed for reproducibility<br/>    dlParams._hidden = Array[Int](10, 2, 10)<br/>    dlParams._epochs = 100 // number of training epochs<br/>    dlParams._activation = Activation.Tanh // Tanh as an activation function<br/>    dlParams._force_load_balance = <strong>false<br/><br/></strong><strong>var</strong> dl = <strong>new</strong> DeepLearning(dlParams)<br/><strong>val</strong> model_nn = dl.trainModel.get</pre>
<p>在前面的代码中，我们应用了一种叫做<strong>瓶颈</strong>训练的技术，其中中间的隐藏层非常小。这意味着我的模型必须减少输入数据的维度(在这种情况下，减少到两个节点/维度)。</p>
<p>然后，自动编码器模型将学习输入数据的模式，而不考虑给定的类别标签。在这里，它将了解哪些信用卡交易是相似的，哪些交易是异常值或异常值。但是，我们需要记住，autoencoder模型对数据中的异常值很敏感，这可能会丢弃其他典型的模式。</p>
<p>一旦预训练完成，我们应该将模型保存在<kbd>.csv</kbd>目录中:</p>
<pre><strong>val</strong> uri = <strong>new</strong> File(<strong>new</strong> File(inputCSV).getParentFile, "model_nn.bin").toURI ModelSerializationSupport.exportH2OModel(model_nn, uri)</pre>
<p>重新加载模型并恢复它以供将来使用:</p>
<pre><strong>val</strong> model: DeepLearningModel = ModelSerializationSupport.loadH2OModel(uri)</pre>
<p>现在，让我们打印模型的指标，看看培训进展如何:</p>
<pre>println(model)<br/>&gt;&gt;&gt;</pre>
<div><img src="img/5dc693c5-f3ed-46b4-8012-b902b7470df6.png"/></div>
<p>图13:自动编码器模型的度量</p>
<p>太棒了。预训练进行得非常顺利，因为我们可以看到RMSE和MSE相当低。我们还可以看到，有些功能相当不重要，比如<kbd>v16</kbd>、<kbd>v1</kbd>、<kbd>v25</kbd>等等。我们稍后将尝试分析它。</p>


            

            
        
    






    
        <title>Step 7 - Dimensionality reduction with hidden layers</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤7 -使用隐藏层进行降维</h1>
                
            
            
                
<p>因为我们使用了一个浅层自动编码器，在中间的隐藏层中有两个节点，所以使用降维来探索我们的特征空间是值得的。我们可以用<kbd>scoreDeepFeatures()</kbd>方法提取这个隐藏的特征，并绘制它来显示输入数据的简化表示。</p>
<p><kbd>scoreDeepFeatures()</kbd>方法即时对自动编码的重建进行评分，并具体化给定层的深层特征。它采用以下参数，帧原始数据(可以包含响应，将被忽略)和要提取特征的隐藏层的层索引。最后，返回包含深度特征的帧。其中列数是隐藏的[层]</p>
<p>现在，对于监督训练，我们需要提取深层特征。让我们从第2层开始:</p>
<pre><strong>var</strong> train_features = model_nn.scoreDeepFeatures(train_unsupervised, 1) <br/>train_features.add("Class", train_unsupervised.vec("Class"))</pre>
<p>最终聚类识别的绘图如下:</p>
<pre>train_features.setNames(train_features.names.map(_.replaceAll("[.]", "-")))<br/>train_features._key = Key.make()<br/>water.DKV.put(train_features)<br/><br/><strong>val</strong> tfDataFrame = asDataFrame(train_features) Vegas("Compressed").withDataFrame(tfDataFrame).mark(Point).encodeX("DF-L2-C1", Quantitative).encodeY("DF-L2-C2", Quantitative).encodeColor(field = "Class", dataType = Nominal).show<br/>&gt;&gt;&gt;</pre>
<div><img height="247" width="310" src="img/7408e0ac-c725-476f-b0d4-52d0a2c442a3.png"/></div>
<p>图14:类0和1的最终聚类</p>
<p>从上图中，我们看不到任何与非欺诈交易明显不同的欺诈交易聚类，因此仅使用我们的autoencoder模型进行降维不足以识别该数据集中的欺诈行为。但是我们可以使用一个隐藏层的降维表示作为模型训练的特征。例如，使用第一个或第三个隐藏层的10个特征。现在，让我们从第3层提取深层特征:</p>
<pre>train_features = model_nn.scoreDeepFeatures(train_unsupervised, 2)<br/>train_features._key = Key.make()<br/>train_features.add("Class", train_unsupervised.vec("Class"))<br/>water.DKV.put(train_features)<br/><br/><strong>val</strong> features_dim = train_features.names.filterNot(_ == response)<br/><strong>val</strong> train_features_H2O = asH2OFrame(train_features)</pre>
<p class="mce-root">现在，让我们再次使用新维度的数据集进行无监督DL:</p>
<pre class="mce-root">dlParams = <strong>new</strong> DeepLearningParameters()<br/>        dlParams._ignored_columns = Array(response)<br/>        dlParams._train = train_features_H2O<br/>        dlParams._autoencoder = <strong>true<br/></strong>        dlParams._reproducible = <strong>true<br/></strong>        dlParams._ignore_const_cols = <strong>false<br/></strong>        dlParams._seed = 42<br/>        dlParams._hidden = Array[Int](10, 2, 10)<br/>        dlParams._epochs = 100<br/>        dlParams._activation = Activation.Tanh<br/>        dlParams._force_load_balance = <strong>false<br/><br/></strong>dl = <strong>new</strong> DeepLearning(dlParams)<br/><strong>val</strong> model_nn_dim = dl.trainModel.get</pre>
<p>然后我们保存模型:</p>
<pre>ModelSerializationSupport.exportH2OModel(model_nn_dim, new File(new File(inputCSV).getParentFile, "model_nn_dim.bin").toURI)</pre>
<p>为了测量测试数据的模型性能，我们需要将测试数据转换为与训练数据相同的缩减维度:</p>
<pre><strong>val</strong> test_dim = model_nn.scoreDeepFeatures(test, 2)<br/><strong>val</strong> test_dim_score = model_nn_dim.scoreAutoEncoder(test_dim, Key.make(), <strong>false</strong>)<br/><br/><strong>val</strong> result = confusionMat(test_dim_score, test, test_dim_score.anyVec.mean)<br/>println(result.deep.mkString("n"))<br/>&gt;&gt;&gt;<br/>Array(38767, 29)<br/>Array(18103, 64)</pre>
<p>现在，就识别欺诈案件而言，这看起来相当不错:93%的欺诈案件都被识别出来了！</p>


            

            
        
    






    
        <title>Step 8 - Anomaly detection</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤8 -异常检测</h1>
                
            
            
                
<p>我们还可以询问哪些实例被认为是测试数据中的异常值或异常。基于之前训练的自动编码器模型，将重构输入数据，并且对于每个实例，计算实际值和重构之间的MSE。我还计算了两个类别标签的平均MSE:</p>
<pre>test_dim_score.add("Class", test.vec("Class"))<br/><strong>val</strong> testDF = asDataFrame(test_dim_score).rdd.zipWithIndex.map(r =&gt; Row.fromSeq(r._1.toSeq :+ r._2))<br/><br/><strong>val</strong> schema = StructType(Array(StructField("Reconstruction-MSE", DoubleType, nullable = <strong>false</strong>), StructField("Class", ByteType, nullable = <strong>false</strong>), StructField("idRow", LongType, nullable = <strong>false</strong>)))<br/><br/><strong>val</strong> dffd = spark.createDataFrame(testDF, schema)<br/>dffd.show()<br/>&gt;&gt;&gt;</pre>
<div><img height="237" width="159" src="img/01c06441-36c9-4976-a9ea-c669408e55b3.png"/></div>
<p>图15:显示MSE、类和行ID的数据帧</p>
<p>看到这个数据框架，很难识别异常值。但是绘制它们会提供更多的见解:</p>
<pre>Vegas("Reduced Test", width = 800, height = 600).withDataFrame(dffd).mark(Point).encodeX("idRow", Quantitative).encodeY("Reconstruction-MSE", Quantitative).encodeColor(field = "Class", dataType = Nominal).show<br/>&gt;&gt;&gt;</pre>
<div><img height="510" width="716" src="img/add43d6c-82c0-472e-b39f-5ebc1dfc40e4.png"/></div>
<p>图16:重构的MSE在不同行id之间的分布</p>
<p>正如我们在图中看到的，欺诈和非欺诈案例没有完美的分类，但欺诈交易的平均MSE肯定高于正常交易。但是最低限度的解释是必要的。</p>
<p>从上图中，我们至少可以看出大多数的<strong> idRows </strong>的MSE为<strong> 5 </strong>。或者，如果我们将MSE阈值向上扩展到<strong> 10 </strong>，那么超过这个阈值的数据点就可以认为是异常值或异常，也就是欺诈交易。</p>


            

            
        
    






    
        <title>Step 9 - Pre-trained supervised model</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤9 -预先训练的监督模型</h1>
                
            
            
                
<p>我们现在可以尝试使用自动编码器模型作为监督模型的预训练输入。这里，我再次使用神经网络。该模型现在将使用来自自动编码器的权重进行模型拟合。然而，为了训练分类，将类从Int转换成Categorical是必要的。否则，H2O训练算法会将其视为回归:</p>
<pre>toCategorical(train_supervised, 29)</pre>
<p>既然训练集(即<kbd>train_supervised</kbd>)已经为监督学习做好了准备，让我们开始吧:</p>
<pre><strong>val</strong> train_supervised_H2O = asH2OFrame(train_supervised)<br/>        dlParams = <strong>new</strong> DeepLearningParameters()<br/>        dlParams._pretrained_autoencoder = model_nn._key<br/>        dlParams._train = train_supervised_H2O<br/>        dlParams._reproducible = <strong>true<br/></strong>        dlParams._ignore_const_cols = <strong>false<br/></strong>        dlParams._seed = 42<br/>        dlParams._hidden = Array[Int](10, 2, 10)<br/>        dlParams._epochs = 100<br/>        dlParams._activation = Activation.Tanh<br/>        dlParams._response_column = "Class"<br/>        dlParams._balance_classes = <strong>true<br/><br/></strong>dl = <strong>new</strong> DeepLearning(dlParams)<br/><strong>val</strong> model_nn_2 = dl.trainModel.get</pre>
<p>干得好！我们现在已经完成了监督培训。现在，要查看预测类和实际类:</p>
<pre><strong>val</strong> predictions = model_nn_2.score(test, "predict")<br/>test.add("predict", predictions.vec("predict"))<br/>asDataFrame(test).groupBy("Class", "predict").count.show //print<br/>&gt;&gt;&gt;<br/>+-----+-------+-----+<br/>|Class|predict|count|<br/>+-----+-------+-----+<br/>| 1| 0| 19|<br/>| 0| 1| 57|<br/>| 0| 0|56804|<br/>| 1| 1| 83|<br/>+-----+-------+-----+</pre>
<p>现在，这个看起来好多了！我们确实遗漏了17%的欺诈案例，但是我们也没有错误分类太多的非欺诈案例。在现实生活中，我们会花更多的时间尝试通过示例来改进模型，执行网格搜索以进行超参数调整，返回到原始功能并尝试不同的工程功能和/或尝试不同的算法。现在，想象一下前面的结果怎么样？让我们使用<kbd>Vegas</kbd>包来完成它:</p>
<pre>Vegas().withDataFrame(asDataFrame(test)).mark(Bar).encodeY(field = "*", dataType = Quantitative, AggOps.Count, axis = Axis(title = "", format = ".2f"), hideAxis = <strong>true</strong>).encodeX("Class", Ord).encodeColor("predict", Nominal, scale = Scale(rangeNominals = List("#EA98D2", "#659CCA"))).configMark(stacked = StackOffset.Normalize).show<br/>&gt;&gt;&gt;</pre>
<div><img height="252" width="180" src="img/f46c6d3a-8edc-489b-9250-cba98b3b7f58.png"/></div>
<p>图17:使用监督训练模型预测的与实际的类别</p>


            

            
        
    






    
        <title>Step 10 - Model evaluation on the highly-imbalanced data</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤10 -高度不平衡数据的模型评估</h1>
                
            
            
                
<p>由于数据集对非欺诈案件高度不平衡，使用模型评估指标，如准确性或曲线下的<strong>面积</strong> ( <strong> AUC </strong>)没有意义。原因在于，基于多数类的高正确分类百分比，这些度量会给出过于乐观的结果。</p>
<p>AUC的一个替代方法是使用精确度-召回曲线，或灵敏度(召回)-特异性曲线。首先，让我们使用来自<kbd>ModelMetricsSupport</kbd>类的<kbd>modelMetrics()</kbd>方法来计算ROC:</p>
<pre><strong>val</strong> trainMetrics = ModelMetricsSupport.modelMetrics[ModelMetricsBinomial](model_nn_2, test)<br/><strong>val</strong> auc = trainMetrics._auc<br/><strong>val</strong> metrics = auc._tps.zip(auc._fps).zipWithIndex.map(x =&gt; x <strong>match</strong> { <strong>case</strong> ((a, b), c) =&gt; (a, b, c) })<br/><br/><strong>val</strong> fullmetrics = metrics.map(_ <strong>match</strong> { <strong>case</strong> (a, b, c) =&gt; (a, b, auc.tn(c), auc.fn(c)) })<br/><strong>val</strong> precisions = fullmetrics.map(_ <strong>match</strong> { <strong>case</strong> (tp, fp, tn, fn) =&gt; tp / (tp + fp) })<br/><br/><strong>val</strong> recalls = fullmetrics.map(_ <strong>match</strong> { <strong>case</strong> (tp, fp, tn, fn) =&gt; tp / (tp + fn) })<br/><strong>val</strong> rows = <strong>for</strong> (i &lt;- 0 until recalls.length) <strong>yield</strong> r(precisions(i), recalls(i))<br/><strong>val</strong> precision_recall = rows.toDF()</pre>
<p>现在我们已经有了<kbd>precision_recall</kbd>数据框架，绘制它将是令人兴奋的。所以让我们开始吧:</p>
<pre>Vegas("ROC", width = 800, height = 600).withDataFrame(precision_recall).mark(Line).encodeX("recall", Quantitative).encodeY("precision", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div><img height="478" width="632" src="img/f66e5dc0-0b53-4b60-9ba2-9186bfa32671.png"/></div>
<p>图18:精确召回曲线</p>
<p>精度是被预测为欺诈的测试用例与真正欺诈的测试用例的比例，也称为<strong>真阳性</strong>预测。另一方面，回忆或敏感度是被识别为欺诈的欺诈案例的比例。特异性是被识别为非欺诈性的非欺诈性案例的比例。</p>
<p>前面的精确回忆曲线告诉我们实际欺诈预测和被预测的欺诈案例比例之间的关系。现在，问题是如何计算灵敏度和特异性。嗯，我们可以使用标准的Scala语法并使用<kbd>Vegas</kbd>包来绘制它:</p>
<pre><strong>val</strong> sensitivity = fullmetrics.map(_ <br/><strong>    match</strong> { <br/><strong>        case</strong> (tp, fp, tn, fn) =&gt; tp / (tp + fn) })<br/><strong>        val</strong> specificity = fullmetrics.map(_ <br/><strong>        match</strong> { <br/><strong>            case</strong> (tp, fp, tn, fn) =&gt; tn / (tn + fp) })<br/><strong>            val</strong> rows2 = <br/><strong>            for</strong> (i &lt;- 0 until specificity.length) <br/><strong>     yield</strong> r2(sensitivity(i), specificity(i))<br/><br/><strong>val</strong> sensitivity_specificity = rows2.toDF<br/>Vegas("sensitivity_specificity", width = 800, height = 600).withDataFrame(sensitivity_specificity).mark(Line).encodeX("specificity", Quantitative).encodeY("sensitivity", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div><img height="364" width="482" src="img/d09bc5bc-db14-4d43-8cd8-69615e21e749.png"/></div>
<p>图19:灵敏度对特异性曲线</p>
<p>现在，前面的灵敏度-特异性曲线告诉我们来自两个标签的正确预测的类别之间的关系-例如，如果我们有100%正确预测的欺诈案件，则没有正确分类的非欺诈案件，反之亦然)。</p>
<p>最后，通过手动检查不同的预测阈值并计算在两个类别中有多少案例被正确分类，以稍微不同的方式更仔细地查看这一点会很棒。更具体地说，我们可以直观地检查不同预测阈值上的真阳性、假阳性、真阴性和假阴性，例如，0.0到1.0:</p>
<pre><strong>val</strong> withTh = auc._tps.zip(auc._fps)<br/>            .zipWithIndex<br/>            .map(x =&gt; x <strong>match</strong> { <strong>case</strong> ((a, b), c) <br/>            =&gt; (a, b, auc.tn(c), auc.fn(c), auc._ths(c)) })<br/><strong>val</strong> rows3 = <strong>for</strong> (i &lt;- 0 until withTh.length) <strong>yield</strong> r3(withTh(i)._1, withTh(i)._2, withTh(i)._3, withTh(i)._4, withTh(i)._5)</pre>
<p>首先，让我们画出真正积极的一面:</p>
<pre>Vegas("tp", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).encodeX("th", Quantitative).encodeY("tp", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div><img height="330" width="438" src="img/8744aaab-cff4-4f47-91a5-c79c408bd47e.png"/></div>
<p>图20:[0.0，1.0]中不同预测阈值的真阳性</p>
<p>其次，我们来画一个假阳性的:</p>
<pre>Vegas("fp", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).encodeX("th", Quantitative).encodeY("fp", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div><img height="485" width="643" src="img/3a0053e8-ba09-438e-ba3a-95f59da4e307.png"/></div>
<p>图21:[0.0，1.0]中不同预测阈值的假阳性</p>
<p>然而，前面的数字不容易解释。所以让我们为<kbd>datum.th</kbd>提供一个0.01的阈值，然后再画一次:</p>
<pre>Vegas("fp", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).filter("datum.th &gt; 0.01").encodeX("th", Quantitative).encodeY("fp", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div><img height="549" width="728" src="img/f4190313-1a84-41f5-8193-164149958df3.png"/></div>
<p>图22:[0.0，1.0]中不同预测阈值的假阳性</p>
<p>然后，就轮到真正的否定了:</p>
<pre>Vegas("tn", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).encodeX("th", Quantitative).encodeY("tn", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div><img height="544" width="722" src="img/4720dbe2-95f4-4b8c-a5ee-df4b4b9d61c9.png"/></div>
<p>图23:[0.0，1.0]中不同预测阈值的假阳性</p>
<p>最后，我们来画假阴性的，如下图:</p>
<pre>Vegas("fn", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).encodeX("th", Quantitative).encodeY("fn", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div><img height="524" width="695" src="img/bab24a7b-dd1a-4857-96dc-c82ae1521854.png"/></div>
<p>图24:[0.0，1.0]中不同预测阈值的假阳性</p>
<p>因此，前面的图告诉我们，当我们将预测阈值从默认的0.5增加到0.6时，我们可以增加正确分类的非欺诈案例的数量，而不会丢失正确分类的欺诈案例。</p>


            

            
        
    






    
        <title>Step 11 - Stopping the Spark session and H2O context</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">步骤11 -停止Spark会话和H2O上下文</h1>
                
            
            
                
<p>最后，停止火花会议和H2O的背景。下面的<kbd>stop()</kbd>方法调用将分别关闭H2O上下文和Spark集群:</p>
<pre>h2oContext.stop(stopSparkContext = <strong>true</strong>)<br/>spark.stop()</pre>
<p>尤其是第一个，更重要，否则它有时不会停止H2O流，但仍然拥有计算资源。</p>


            

            
        
    






    
        <title>Auxiliary classes and methods</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">辅助类和方法</h1>
                
            
            
                
<p>在前面的步骤中，我们已经看到了一些应该在这里描述的类或方法。第一个方法名为<kbd>toCategorical()</kbd>，将框架列从String/Int转换为enum用于将<kbd>dayTime</kbd>箱包(即<kbd>gr1</kbd>、<kbd>gr2</kbd>、<kbd>gr3</kbd>、<kbd>gr4</kbd>)转换为类因子类型。该函数还用于将<kbd>Class</kbd>列转换为因子类型，以便进行分类:</p>
<pre><strong>def</strong> toCategorical(f: Frame, i: Int): Unit = {<br/>    f.replace(i, f.vec(i).toCategoricalVec)<br/>    f.update()<br/>    }</pre>
<p>如果一个实例被认为是异常的(如果它的MSE超过给定的阈值)，这将根据阈值建立用于异常检测的混淆矩阵:</p>
<pre><strong>def</strong> confusionMat(mSEs:water.fvec.Frame,actualFrame:water.fvec.Frame,thresh: Double):Array[Array[Int]] = {<br/><strong>    val</strong> actualColumn = actualFrame.vec("Class");<br/><strong>    val</strong> l2_test = mSEs.anyVec();<br/><strong>    val</strong> result = Array.ofDim[Int](2, 2)<br/><strong>    var</strong> i = 0<br/><strong>    var</strong> ii, jj = 0<br/><br/><strong>    for</strong> (i &lt;- 0 until l2_test.length().toInt) {<br/>        ii = <strong>if</strong> (l2_test.at(i) &gt; thresh) 1 <strong>else</strong> 0;<br/>        jj = actualColumn.at(i).toInt<br/>        result(ii)(jj) = result(ii)(jj) + 1<br/>        }<br/>    result<br/>    }</pre>
<p>除了这两个辅助方法，我还定义了三个Scala case类来计算精度，recall敏感性、特异性；真阳性，真阴性，假阳性和假阴性等等。签名如下:</p>
<pre><strong>case</strong><strong>class</strong> r(precision: Double, recall: Double)<br/><strong>case</strong><strong>class</strong> r2(sensitivity: Double, specificity: Double)<br/><strong>case</strong><strong>class</strong> r3(tp: Double, fp: Double, tn: Double, fn: Double, th: Double)</pre>


            

            
        
    






    
        <title>Hyperparameter tuning and feature selection</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">超参数调谐和特征选择</h1>
                
            
            
                
<p>以下是一些通过调整超参数来提高精度的方法，如隐藏层的数量、每个隐藏层中的神经元、历元的数量和激活函数。基于H2O的深度学习模型的当前实现支持以下激活功能:</p>
<ul>
<li><kbd>ExpRectifier</kbd></li>
<li><kbd>ExpRectifierWithDropout</kbd></li>
<li><kbd>Maxout</kbd></li>
<li><kbd>MaxoutWithDropout</kbd></li>
<li><kbd>Rectifier</kbd></li>
<li><kbd>RectifierWthDropout</kbd></li>
<li><kbd>Tanh</kbd></li>
<li><kbd>TanhWithDropout</kbd></li>
</ul>
<p>除了<kbd>Tanh</kbd>这个，我没有试过这个项目的其他激活功能。但是，你一定要试一试。</p>
<p>使用基于H2O的深度学习算法的最大优势之一是，我们可以采用相对变量/特征重要性。在前面的章节中，我们已经看到，使用Spark中的随机森林算法，也可以计算变量重要性。因此，这个想法是，如果你的模型表现不佳，就应该放弃不太重要的特性，重新进行训练。</p>
<p>我们来看一个例子；在<em>图13 </em>中，我们已经看到了autoencoder中无监督训练的最重要特征。现在，也可以在监督训练期间发现特征重要性。我在这里观察到了特性的重要性:</p>
<div><img height="595" width="693" src="img/e5331bba-dac7-43af-a0b9-428c83482cad.png"/></div>
<p>图25:[0.0，1.0]中不同预测阈值的假阳性</p>
<p>因此，从<em>图25 </em>中可以看出，时间、<kbd>V21</kbd>、<kbd>V17</kbd>、<kbd>V6</kbd>这些特征是次要的。所以你为什么不放弃它们，重新尝试训练，观察准确度是否有所提高？</p>
<p>然而，网格搜索或交叉验证技术仍然可以提供更高的准确性。然而，我将把它留给你。</p>


            

            
        
    






    
        <title>Summary</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p>在本章中，我们使用了包含超过284，807个信用卡使用实例的数据集，并且对于每笔交易，只有0.172%的交易是欺诈性的。我们已经了解了如何使用自动编码器来预训练分类模型，以及如何应用异常检测技术来从高度不平衡的数据中预测可能的欺诈交易，也就是说，我们希望我们的欺诈案例是整个数据集中的异常。</p>
<p>我们的最终模型现在正确识别了83%的欺诈案例和几乎100%的非欺诈案例。然而，我们已经看到了如何使用异常值进行异常检测，一些超参数调整的方法，以及最重要的特征选择。</p>
<p>一个<strong>递归神经网络</strong> ( <strong> RNN </strong>)是一类人工神经网络，其中单元之间的连接形成一个有向循环。rnn利用过去的信息。这样，他们可以在具有高度时间依赖性的数据中进行预测。这创建了网络的内部状态，允许它展示动态的时间行为。</p>
<p>RNN接受许多输入向量来处理它们并输出其他向量。与经典方法相比，使用具有<strong>长短期记忆单元</strong>(<strong>lstm</strong>)的RNN几乎不需要特征工程。数据可以直接输入神经网络，神经网络就像一个黑匣子，正确地模拟问题。这里的方法在预处理多少数据方面相当简单。</p>
<p>在下一章中，我们将看到如何使用智能手机数据集，使用名为<strong> LSTM </strong>的RNN实现来开发一个机器学习项目，用于<strong>人类活动识别</strong> ( <strong> HAR </strong>)。简而言之，我们的机器学习模型将能够从六个类别中对运动类型进行分类:行走、上楼、下楼、坐着、站着和躺着。</p>
<p class="mce-root"/>


            

            
        
    


</body></html>