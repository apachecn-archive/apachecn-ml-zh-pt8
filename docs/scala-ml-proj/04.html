<html><head/><body>


    
        <title>Population-Scale Clustering and Ethnicity Prediction</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">人口规模聚类和种族预测</h1>
                
            
            
                
<p>了解基因组序列的变异有助于我们识别易患常见疾病的人群，治愈罕见疾病，并从更大的人群中找到相应的人群。虽然经典的机器学习技术允许研究人员识别相关变量的组(即聚类)，但对于大型和高维数据集(如整个人类基因组)，这些方法的准确性和有效性会降低。</p>
<p>另一方面，<strong>深度神经网络</strong> ( <strong> DNNs </strong>)形成了<strong>深度学习</strong> ( <strong> DL </strong>)的核心，并提供算法来对数据中复杂的高级抽象进行建模。他们可以更好地利用大规模数据集来构建复杂的模型。</p>
<p>在这一章中，我们将K-means算法应用于来自1000基因组项目分析的大规模基因组数据，该项目旨在在人群规模上聚类基因型变异。最后，我们训练了一个基于H2O的DNN模型和一个基于Spark的随机森林模型来预测地理种族。本章的主题是<em>给我你的基因变异数据，我会告诉你的种族</em>。</p>
<p>然而，我们将配置H2O，以便在接下来的章节中也可以使用相同的设置。简而言之，在这个端到端项目中，我们将学习以下主题:</p>
<ul>
<li>人口规模聚类和地理种族预测</li>
<li>1000个基因组项目，人类基因变异的深度目录</li>
<li>算法和工具</li>
<li>使用K-均值进行人口规模聚类</li>
<li>使用H2O进行种族预测</li>
<li>使用随机森林进行种族预测</li>
</ul>


            

            
        
    






    
        <title>Population scale clustering and geographic ethnicity</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">人口规模聚集和地理种族</h1>
                
            
            
                
<p><strong>下一代基因组测序</strong> ( <strong> NGS </strong>)减少了基因组测序的开销和时间，带来了前所未有的大数据生产。相比之下，分析这种大规模数据的计算成本很高，并日益成为关键的瓶颈。NGS数据在总体样本数量和每个样本的特征方面的增长需要大规模并行数据处理的解决方案，这给机器学习解决方案和生物信息学方法带来了巨大的挑战。在医疗实践中使用基因组信息需要有效的分析方法来处理来自成千上万个个体及其数百万个变体的数据。</p>
<p>最重要的任务之一是分析基因组图谱以将个体归因于特定的种族群体，或者分析疾病易感性的核苷酸单倍型。来自1000个基因组项目的数据是分析全基因组<strong>单核苷酸多态性</strong> ( <strong> SNPs </strong>)的主要来源，用于预测个体的大陆和区域血统。</p>


            

            
        
    






    
        <title>Machine learning for genetic variants</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">遗传变异的机器学习</h1>
                
            
            
                
<p>研究表明，来自亚洲、欧洲、非洲和美洲的人群可以根据他们的基因组数据进行区分。然而，更具挑战性的是准确预测单倍群和起源大陆，即地理、种族和语言。其他研究表明，Y染色体谱系可以在地理上定位，形成了人类基因型的人类等位基因(在地理上)聚类的证据。</p>
<p>因此，个体的聚类与地理起源和祖先相关。由于种族也依赖于祖先，聚类也与更传统的种族概念相关，但这种相关性并不完美，因为遗传变异是根据概率原理发生的。因此，它在不同的种族中并不遵循连续的分布，而是在不同的种群中重叠或溢出。</p>
<p>因此，祖先甚至种族的鉴定可能会被证明对生物医学有用，但任何对疾病相关遗传变异的直接评估最终都会产生更准确和有益的信息。</p>
<p>各种基因组计划提供的数据集，如<strong>癌症基因组图谱</strong> ( <strong> TCGA </strong>)、<strong>国际癌症基因组联盟(ICGC) </strong>、<strong>千人基因组计划</strong>、<strong>个人基因组计划</strong> ( <strong> PGP </strong>)，处理大规模数据。为了快速处理这些数据，已经提出了基于ADAM和Spark的解决方案，并且现在广泛用于基因组数据分析研究。</p>
<p>Spark形成了最有效的数据处理框架，此外，它还为内存集群计算提供了原语，例如，用于重复查询用户数据。这使得Spark成为机器学习算法的绝佳候选，其性能优于基于Hadoop的MapReduce框架。通过使用来自1000个基因组项目的遗传变异数据集，我们将尝试回答以下问题:</p>
<ul>
<li>人类基因变异在不同人群中的地理分布如何？</li>
<li>我们能否利用个体的基因组图谱将他们归属于特定的人群或从他们的核苷酸单体型推导出疾病易感性？</li>
<li>个体的基因组数据是否适合预测地理来源(即个体的人群)？</li>
</ul>
<p>在这个项目中，我们以一种可扩展和更有效的方式解决了前面的问题。特别是，我们研究了如何将Spark和ADAM应用于大规模数据处理，将H2O应用于整个群体的K-means聚类以确定群体间和群体内的群体，以及基于MLP的监督学习，通过调整更多的超参数以根据个体的基因组数据更准确地预测个体的群体。在这一点上不要担心；我们将在后面的小节中提供使用这些技术的技术细节。</p>
<p>然而，在开始之前，让我们简短地浏览一下1000个基因组项目数据集，为您提供一些理由，说明为什么这些技术的互操作非常重要。</p>


            

            
        
    






    
        <title>1000 Genomes Projects dataset description</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">1000个基因组项目数据集描述</h1>
                
            
            
                
<p>来自1000个基因组项目的数据是一个非常大的人类基因变异的目录。该项目旨在确定研究人群中频率高于1%的遗传变异。这些数据已经通过公共数据储存库向全世界的科学家公开提供并免费获取。此外，来自1000个基因组项目的数据被广泛用于筛选在患有遗传疾病的个体的外显子组数据和癌症基因组项目中发现的变体。</p>
<p><strong>变异体调用格式</strong> ( <strong> VCF </strong>)中的基因型数据集提供了人类个体(即样本)及其遗传变异体的数据，此外还有全球等位基因频率以及超级群体的等位基因频率。数据表示在我们的方法中用于预测类别的每个样本的人口区域。特定的染色体数据(VCF格式)可能有额外的信息表明样本的超群体或所用的测序平台。对于多等位基因变异，每个备选的<strong>等位基因频率</strong> ( <strong> AF </strong>)以逗号分隔的列表显示，如下所示:</p>
<pre>1 15211 rs78601809 T G 100 PASS AC=3050;<br/> AF=0.609026;<br/> AN=5008;<br/> NS=2504;<br/> DP=32245;<br/> EAS_AF=0.504;<br/> AMR_AF=0.6772;<br/> AFR_AF=0.5371;<br/> EUR_AF=0.7316;<br/> SAS_AF=0.6401;<br/> AA=t|||;<br/> VT=SNP</pre>
<p>AF计算为<strong>等位基因数</strong> ( <strong> AC </strong>)和<strong>等位基因数</strong> ( <strong> AN </strong>)的商，NS为有数据的样本总数，而<kbd>_AF</kbd>表示特定区域的AF。</p>
<p>1000基因组计划始于2008年；该联盟由400多名生命科学家组成，第三阶段于2014年9月完成，共涵盖来自26个群体(即种族背景)的<kbd>2,504</kbd>个人。总共有超过8800万个变异体(8470万个<strong>单核苷酸多态性</strong> ( <strong> SNPs </strong>)、360万个短插入/缺失(indels)和60000个结构变异体)被鉴定为高质量的单倍型。</p>
<p>简而言之，99.9%的变异由SNPs和短indels组成。不太重要的变异——包括SNPs、indels、缺失、复杂的短替换和其他结构变异类别——已被剔除，以进行质量控制。结果，第三阶段发布留下了8440万个变体。</p>
<p>26个种群中的每一个都有大约60-100个来自欧洲、非洲、美洲(南北)和亚洲(东南)的个体。人群样本根据其优势祖先分为超人群组:东亚人(<strong> CHB </strong>、<strong> JPT </strong>、<strong> CHS </strong>、<strong> CDX </strong>和<strong> KHV </strong>、欧洲人(<strong> CEU </strong>、<strong> TSI </strong>、<strong> FIN </strong>、<strong> GBR </strong>和<strong> IBS </strong>、非洲人(<strong> YRI <strong> ESN </strong>、<strong> ASW </strong>、<strong> ACB </strong>、美国<strong> MXL </strong>、<strong> PUR </strong>、<strong> CLM </strong>、<strong> PEL </strong>、南亚<strong> GIH </strong>、<strong> PJL </strong>、<strong> BEB </strong>、<strong> STU </strong>和<strong> ITU 详见<em>图1 </em>:</strong></strong></p>
<div><img src="img/6c184827-d03e-4b8d-a33b-7a1084dbf66b.png"/></div>
<p>图1:来自1000基因组计划第三版的地理种族群体(来源http://www.internationalgenome.org/)</p>
<p>发布的数据集提供了2，504名健康成年人(18岁及以上，项目第三阶段)的数据；在更先进的解决方案可用之前，仅使用至少70个<strong>碱基对</strong> ( <strong> bp </strong>)的读数。来自所有样本的所有基因组数据被合并，以将所有变异体归属于一个区域。然而，请注意，特定的单倍型可能不会出现在特定区域的基因组中；也就是说，多样本方法允许将变异归因于个体的基因型，即使该变异不包括在该样本的测序读数中。</p>
<p>换句话说，提供了重叠的读数，单个样本基因组不一定被合并。使用这两种方法对所有个体进行测序:</p>
<ul>
<li>全基因组测序(<em>平均深度= 7.4x </em>，其中<em> x </em>是可能在给定参考<em> bp </em>处比对的平均读取数)</li>
<li>靶向外显子组测序(<em>平均深度= 65.7倍</em>)</li>
</ul>
<p>此外，个体和他们的一级亲属，如成年后代，使用高密度SNP微阵列进行基因分型。每个基因型包含所有23条染色体，一个单独的面板文件表示样本和群体信息。<em>表1 </em>概述了1000基因组计划的不同版本:</p>
<p class="packt_figref CDPAlignLeft CDPAlign"><strong>表1——千人基因组计划基因型数据集统计</strong> <strong>(来源:</strong><a href="http://www.internationalgenome.org/data" target="_blank">http://www.internationalgenome.org/data</a><strong>)</strong></p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong> 1000个基因组发布</strong></p>
</td>
<td>
<p><strong>变体</strong></p>
</td>
<td>
<p><strong>个人</strong></p>
</td>
<td>
<p><strong>人口</strong></p>
</td>
<td>
<p><strong>文件格式</strong></p>
</td>
</tr>
<tr>
<td>
<p>第三阶段</p>
</td>
<td>
<p>第三阶段</p>
</td>
<td>
<p>2,504</p>
</td>
<td>
<p>26</p>
</td>
<td>
<p>VCF</p>
</td>
</tr>
<tr>
<td>
<p>第一相</p>
</td>
<td>
<p>3790万</p>
</td>
<td>
<p>1,092</p>
</td>
<td>
<p>14</p>
</td>
<td>
<p>VCF</p>
</td>
</tr>
<tr>
<td>
<p>飞行员</p>
</td>
<td>
<p>1480万</p>
</td>
<td>
<p>179</p>
</td>
<td>
<p>四</p>
</td>
<td>
<p>VCF</p>
</td>
</tr>
</tbody>
</table>
<p>五个超人群群体中的AF，<strong>EAS =东亚人</strong>，<strong>EUR =欧洲人</strong>，<strong>AFR =非洲人</strong>，<strong>AMR =美洲人</strong>，<strong>SAS =南亚人</strong>人群由等位基因数(an，range= [0，1])计算得出。</p>
<p>面板文件详见<a href="ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel">FTP://FTP . 1000 genomes . ebi . AC . uk/vol 1/FTP/release/2013 05 02/integrated _ call _ samples _ v 3.2013 05 02 . all . panel</a>。</p>
<p>算法、工具和技术</p>


            

            
        
    






    
        <title>Algorithms, tools, and techniques</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">来自1000基因组计划第3版的大规模数据贡献了820 GB的数据。因此，ADAM和Spark用于以可扩展的方式为MLP和K-means模型预处理和准备数据(即训练、测试和验证集)。苏打水改变了H2O和火花之间的数据。</h1>
                
            
            
                
<p>然后，K-均值聚类，MLP(使用H2O)被训练。对于聚类和分类分析，需要使用样本ID、变异ID和备选等位基因计数来获得每个样本的基因型信息，其中我们使用的大多数变异是SNPs和indels。</p>
<p>现在，我们应该知道所使用的每个工具的基本信息，如亚当、H2O，以及一些算法的背景信息，如聚类的K-means和MLP，并对人群进行分类。</p>
<p>H2O和苏打水</p>


            

            
        
    






    
        <title>H2O and Sparkling water</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">H2O是一个机器学习的人工智能平台。它提供了一套丰富的机器学习算法和一个基于网络的数据处理UI，既有开源的，也有商业的。使用H2O，可以用多种语言开发机器学习和DL应用程序，比如Java、Scala、Python和R:</h1>
                
            
            
                
<p>图2:https://h20.ai/的H2O计算引擎和可用特性</p>
<div><img height="414" width="728" src="img/96b2598b-a253-48cd-8213-de1386bb9314.png"/></div>
<p>它还能够与Spark、HDFS、SQL和NoSQL数据库接口。简而言之，H2O在Hadoop/Yarn、Spark或laptop上使用R、Python和Scala。另一方面，苏打水结合了H2O的快速、可扩展的ML算法和Spark的能力。它驱动Scala/R/Python的计算，并利用H2O流UI。简而言之，闪闪发光的<em>水= H2O +火花</em>。</p>
<p>在接下来的几章中，我们将探索H2O和波光粼粼的水域的丰富多彩的特征；但是，我认为提供一个包含所有功能领域的图表会很有用:</p>
<p>图3:可用算法和支持的ETL技术的一瞥(来源:https://h20.ai/)</p>
<div><img height="213" width="401" src="img/dbb845b1-18f8-431e-97d1-195b6b8d84ca.png"/></div>
<p>这是一个从H2O网站精选的特性和技术列表。它可用于争论数据、使用数据建模以及对结果模型进行评分:</p>
<p>过程</p>
<ul>
<li>模型</li>
<li>评分工具</li>
<li>数据剖析</li>
<li><strong>广义线性模型</strong> ( <strong> GLM </strong>)</li>
<li>预测</li>
<li>汇总统计数据</li>
<li>决策树</li>
<li>混淆矩阵</li>
<li>聚合、筛选、绑定和派生列</li>
<li><strong>梯度推进机</strong> ( <strong> GBM </strong></li>
<li>罗马纪元</li>
<li>切片、日志转换和匿名化</li>
<li>k均值</li>
<li>命中率</li>
<li>变量创建</li>
<li>异常检测</li>
<li>PCA/PCA得分</li>
<li>分升</li>
<li>多模型评分</li>
<li>培训和验证抽样计划</li>
<li>朴素贝叶斯</li>
<li>网格搜索</li>
<li>下图显示了如何提供一个清晰的方法来描述H2O苏打水是如何被用来扩展Apache Spark的功能的。H2O和Spark都是开源系统。Spark MLlib包含了大量的功能，而H2O用大量的额外功能扩展了它，包括DL。它提供了对数据进行转换、建模和评分的工具，我们可以在Spark ML中找到这些工具。它还提供了一个基于web的用户界面，可以与以下用户进行交互:</li>
</ul>
<p>图4:苏打水延伸H2O，与Spark互通(来源:https://h20.ai/)</p>
<div><img height="282" width="501" src="img/b10c6dbb-b830-491c-9d73-d0b27bdde21c.png"/></div>
<p>下图显示了H2O如何与Spark集成。我们已经知道，Spark有主服务器和工作服务器；工人创建执行者来做实际的工作。运行基于水的起泡应用程序需要执行以下步骤:</p>
<p>Spark的submit命令将汽水罐发送给Spark master</p>
<ul>
<li>Spark master启动工人并分发JAR文件</li>
<li>Spark工作人员启动执行器JVM来执行工作</li>
<li>Spark执行器启动一个H2O实例</li>
<li>H2O实例嵌入在执行器JVM中，因此它与Spark共享JVM堆空间。当所有H2O实例启动后，H2O形成一个集群，然后H2O流web界面可用:</li>
</ul>
<p>图Sparkling water如何融入Spark架构(来源:http://blog . cloud era . com/blog/2015/10/How-to-build-a-machine-learning-app-using-sparking-water-and-Apache-Spark/)</p>
<div><img height="347" width="577" src="img/10b221b6-aa8f-4e8f-ab55-a9da2d90afc3.png"/></div>
<p>上图解释了H2O如何融入Spark架构以及它是如何开始的，但是数据共享呢？现在的问题是:数据是如何在Spark和H2O之间传递的？下图对此进行了解释:</p>
<p>图Spark和H2O之间的数据传递机制</p>
<div><img height="293" width="556" src="img/8b0ab4ad-f630-4cc0-aa39-d8253355e303.png"/></div>
<p>为了更清楚地了解上图，我们为H2O和苏打水创建了一个新的H2O·RDD数据结构。它是基于H2O帧顶部的层，其中的每一列都代表一个数据项，并且被独立压缩以提供最佳的压缩率。</p>
<p>用于大规模基因组数据处理的ADAM</p>


            

            
        
    






    
        <title>ADAM for large-scale genomics data processing</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">分析DNA和RNA测序数据需要大规模的数据处理，以根据其上下文来解释数据。学术实验室已经开发出了优秀的工具和解决方案，但是在可伸缩性和互操作性方面往往存在不足。通过这种方式，ADAM是一个基因组学分析平台，具有使用Apache Avro、Apache Spark和Parquet构建的专用文件格式。</h1>
                
            
            
                
<p>然而，大规模数据处理解决方案，如ADAM-Spark，可以直接应用于测序管道的输出数据，即在质量控制、绘图、读取预处理和使用单个样本数据的变体量化之后。一些例子是用于DNA测序的DNA变体，用于RNA测序的读数，等等。</p>
<p>更多信息请参见<a href="http://bdgenomics.org/">http://bdgenomics.org/</a>和相关出版物:Massie，Matt和Nothaft，Frank等人，ADAM:云计算的基因组格式和处理模式，UCB/EECS，2013-207，加州大学EECS分校。</p>
<p>在我们的研究中，ADAM用于实现支持VCF文件格式的可扩展基因组数据分析平台，以便我们可以将基于基因型的RDD转换为Spark数据框架。</p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">无监督机器学习</p>


            

            
        
    






    
        <title>Unsupervised machine learning</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">无监督学习是一种机器学习算法，用于对相关数据对象进行分组，并通过从无标签数据集(即由无标签输入数据组成的训练集)进行推理来找到隐藏模式。</h1>
                
            
            
                
<p>我们来看一个现实生活中的例子。假设你的硬盘上有一个拥挤的大文件夹，里面有大量非盗版且完全合法的MP3文件。现在，如果您可以建立一个预测模型，帮助您自动将相似的歌曲组合在一起，并将其组织到您最喜欢的类别中，如乡村、说唱和摇滚，会怎么样？</p>
<p>这是将一个项目分配到一个组的行为，以便以无人监管的方式将MP3添加到相应的播放列表中。对于分类，我们假设给你一个正确标记数据的训练数据集。不幸的是，当我们在现实世界中收集数据时，我们并不总是有这种奢侈。</p>
<p>例如，假设我们想将大量的音乐分成有趣的播放列表。如果我们不能直接访问歌曲的元数据，我们怎么可能把它们组合在一起呢？一种可能的方法是混合各种ML技术，但是集群通常是解决方案的核心:</p>
<p>图7:聚类数据样本概览</p>
<div><img height="205" width="639" src="img/dd15ee54-52ae-47a9-8dcf-646723794d3a.png"/></div>
<p>换句话说，无监督学习算法的主要目标是在未标记的输入数据中探索未知/隐藏的模式。然而，无监督学习还包括其他技术，以探索的方式解释数据的关键特征，以找到隐藏的模式。为了克服这一挑战，聚类技术被广泛用于以无监督的方式基于某些相似性度量对未标记的数据点进行分组。</p>
<p>群体基因组学和聚类</p>


            

            
        
    






    
        <title>Population genomics and clustering</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">聚类分析是关于划分数据样本或数据点，并将其放入相应的同质类或簇中。因此，集群的简单定义可以认为是将对象组织成组的过程，这些组的成员在某些方面是相似的，如。</h1>
                
            
            
                
<p>这样，一个集群就是一个对象的集合，这些对象之间具有某种相似性，并且与属于其他集群的对象不相似。如果给定了遗传变异的集合，聚类算法会根据相似性将这些对象归入一个组，即种群组或超种群组。</p>
<p>K-means是如何工作的？</p>


            

            
        
    






    
        <title>How does K-means work?</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">诸如K-means的聚类算法定位数据点组的质心。但是，为了使聚类准确有效，该算法会计算每个点与聚类质心之间的距离。</h1>
                
            
            
                
<p>最终，聚类的目标是确定一组未标记数据中的内在分组。例如，K-means算法试图在预定义的<strong>三个</strong>(即<em> k = 3 </em>)聚类内对相关数据点进行聚类，如图<em>图8 </em>所示:</p>
<p>图8:典型聚类算法的结果和聚类中心的表示</p>
<div><img height="342" width="419" src="img/e9b8ab25-2f24-4635-b875-9710eb439de4.png"/></div>
<p>在我们的案例中，通过使用Spark的组合方法，亚当和H2O能够处理大量不同的数据点。假设，我们有n个数据点(x <sub> i </sub>，i=1，2… n，例如，遗传变异体)需要划分成<em> k </em>个聚类。然后K-means给每个数据点分配一个聚类，目的是找到位置<em>μ<sub>I</sub>T13】，<em> i=1...k个使数据点到聚类的距离最小的聚类。在数学上，K-means试图通过求解一个方程来实现目标，即一个优化问题:</em></em></p>
<p>在上式中，<em>c<sub>I</sub>T19】是分配给聚类<em> i </em>的数据点集，<em> d(x，μ<sub>I</sub>)=∨xμ<sub>I</sub>∨<sub>2</sub><sup>2</sup></em>是要计算的欧氏距离。该算法通过最小化<strong>聚类内平方和</strong>(即<strong> WCSS </strong>)来计算数据点和k个聚类中心之间的距离，其中<em>c<sub>I</sub>T39】是属于聚类<em> i </em>的点集。</em></em></p>
<div><img height="53" width="390" class="fm-editor-equation" src="img/bb521b9d-0f9d-49ed-b2f7-e642e98ee905.png"/></div>
<p>因此，我们可以理解，使用K-means的整体聚类操作不是一个微不足道的问题，而是一个NP-hard优化问题。这也意味着K-means算法不仅试图找到全局最小值，而且经常陷入不同的解决方案。K-means算法通过在两个步骤之间交替进行:</p>
<p><strong>聚类分配步骤</strong>:将每个观察值分配给其平均值产生最小<strong> WCSS </strong>的聚类。平方和是欧几里德距离的平方。</p>
<ul>
<li><strong>质心更新步骤</strong>:计算新的均值作为新聚类中观察值的质心。</li>
<li>简而言之，K-means训练的总体方法可以用下图来描述:</li>
</ul>
<p>图9:K-means算法过程的整体方法</p>
<div><img height="302" width="537" src="img/aab181a6-0476-41d9-822d-bb2c0feeab04.png"/></div>
<p>用于地理种族预测的DNNs</p>


            

            
        
    






    
        <title>DNNs for geographic ethnicity prediction</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title"><strong>多层感知器</strong> ( <strong> MLP </strong>)是前馈神经网络DNN的一个例子；也就是说，不同层的神经元之间只有连接。有一个(直通)输入层，一层或多层<strong>线性阈值单元</strong>(<strong>ltu</strong>)(称为<strong>隐藏层</strong>)，以及一个ltu的最终层(称为<strong>输出层</strong>)。</h1>
                
            
            
                
<p>除了输出层，每一层都包含一个偏向神经元，并与下一层完全连接，形成一个完全连接的二分图。信号专门从输入流向输出，即单向(<strong>前馈</strong>)。</p>
<p>直到最近，使用反向传播训练算法来训练MLP，但现在优化版本(即梯度下降)使用反向模式自动差异；也就是说，使用反向传播作为梯度计算技术，用SGD训练神经网络。在DNN训练中使用了两个抽象层来解决分类问题:</p>
<p><strong>梯度计算</strong>:使用反向传播</p>
<ul>
<li><strong>优化级别</strong>:使用SGD、ADAM、RMSPro和Momentum优化器来计算之前计算的梯度</li>
<li>在每个训练周期中，该算法将数据输入网络，并计算连续层中每个神经元的状态和输出。然后，该方法测量网络上的输出误差，即期望输出和当前输出之间的差距，以及最后一个隐藏层中每个神经元对神经元输出误差的贡献。</li>
</ul>
<p>迭代地，输出误差通过所有隐藏层传播回输入层，并且在反向传播期间跨所有连接权重计算误差梯度:</p>
<p>图10:由输入层、ReLU和softmax组成的现代MLP</p>
<div><img height="192" width="336" src="img/a6998423-d881-4f26-a9e3-c868dc81c503.png"/></div>
<p>对于多类分类任务，与单独的激活函数相比，输出层通常由共享的softmax函数确定(更多信息参见<em>图2 </em>，每个输出神经元提供相应类的估计概率。</p>
<p>此外，我们将使用树集合，如随机森林进行分类。此时，我相信我们可以跳过RF的基本介绍，因为我们已经在<a href="4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml" target="_blank">第一章</a>、<em>分析保险严重索赔</em>、<a href="4e196881-40c8-4eb9-b2b3-e332a49adc1a.xhtml" target="_blank">第二章</a>、<em>分析和预测电信流失</em>、<a href="51e66c26-e12b-4764-bbb7-444986c05870.xhtml" target="_blank">第三章</a>、<em>从历史数据预测高频比特币价格</em>中详细介绍过。好了，该是被注视的时候了。尽管如此，在动手之前准备好编程环境总是好的。</p>
<p>配置编程环境</p>


            

            
        
    






    
        <title>Configuring programming environment</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">在本节中，我们将描述如何配置我们的编程环境，以便我们可以与Spark、H2O和Adam进行互操作。请注意，在笔记本电脑或台式机上使用H2O是非常耗费资源的。因此，请确保您的笔记本电脑至少有16 GB的内存和足够的存储空间。</h1>
                
            
            
                
<p>无论如何，我要让这个项目成为Eclipse上的Maven项目。但是，您也可以尝试在SBT定义相同的依赖项。让我们为一个Maven友好的项目定义一个<kbd>pom.xml</kbd>文件上的属性标签:</p>
<p>然后我们可以最新版的Spark 2.2.1版本(任何2.x版本甚至更高版本应该都可以):</p>
<pre>&lt;properties&gt;<br/>    &lt;spark.version&gt;2.2.1&lt;/spark.version&gt;<br/>    &lt;scala.version&gt;2.11.12&lt;/scala.version&gt;<br/>    &lt;h2o.version&gt;3.16.0.2&lt;/h2o.version&gt;<br/>    &lt;sparklingwater.version&gt;2.2.6&lt;/sparklingwater.version&gt;<br/>    &lt;adam.version&gt;0.23.0&lt;/adam.version&gt;<br/>&lt;/properties&gt;</pre>
<p>然后我们需要声明H2O和苏打水的依赖项，它们与properties标签中指定的版本相匹配。更高版本也可能有效，您可以尝试:</p>
<pre>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;<br/>    &lt;version&gt;${spark.version}&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<p>最后，让我们定义ADAM及其依赖项:</p>
<pre>&lt;dependency&gt;<br/>    &lt;groupId&gt;ai.h2o&lt;/groupId&gt;<br/>    &lt;artifactId&gt;sparkling-water-core_2.11&lt;/artifactId&gt;<br/>    &lt;version&gt;2.2.6&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;ai.h2o&lt;/groupId&gt;<br/>    &lt;artifactId&gt;sparkling-water-examples_2.11&lt;/artifactId&gt;<br/>    &lt;version&gt;2.2.6&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;ai.h2o&lt;/groupId&gt;<br/>    &lt;artifactId&gt;h2o-core&lt;/artifactId&gt;<br/>    &lt;version&gt;${h2o.version}&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;ai.h2o&lt;/groupId&gt;<br/>    &lt;artifactId&gt;h2o-scala_2.11&lt;/artifactId&gt;<br/>    &lt;version&gt;${h2o.version}&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;ai.h2o&lt;/groupId&gt;<br/>    &lt;artifactId&gt;h2o-algos&lt;/artifactId&gt;<br/>    &lt;version&gt;${h2o.version}&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;ai.h2o&lt;/groupId&gt;<br/>    &lt;artifactId&gt;h2o-app&lt;/artifactId&gt;<br/>    &lt;version&gt;${h2o.version}&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;ai.h2o&lt;/groupId&gt;<br/>    &lt;artifactId&gt;h2o-persist-hdfs&lt;/artifactId&gt;<br/>    &lt;version&gt;${h2o.version}&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;ai.h2o&lt;/groupId&gt;<br/>    &lt;artifactId&gt;google-analytics-java&lt;/artifactId&gt;<br/>    &lt;version&gt;1.1.2-H2O-CUSTOM&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<p>当我在Windows机器上尝试时，我还必须安装<kbd>joda-time</kbd>依赖项。让我们来做(但是根据您的平台，可能不需要):</p>
<pre>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.bdgenomics.adam&lt;/groupId&gt;<br/>    &lt;artifactId&gt;adam-core_2.11&lt;/artifactId&gt;<br/>    &lt;version&gt;0.23.0&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<p>一旦您在Eclipse中创建了一个Maven项目(从IDE中手动创建或者使用<kbd>$ mvn install)</kbd>，所有需要的依赖项都将被下载！我们现在准备编码了！</p>
<pre>&lt;dependency&gt;<br/>    &lt;groupId&gt;joda-time&lt;/groupId&gt;<br/>    &lt;artifactId&gt;joda-time&lt;/artifactId&gt;<br/>    &lt;version&gt;2.9.9&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<p>等等！在浏览器上看到H2O的用户界面怎么样？为此，我们必须手动将H2O JAR下载到我们计算机的某个地方，并将其作为常规的<kbd>.jar</kbd>文件运行。简而言之，这是一个三方过程:</p>
<p>从<a href="https://www.h2o.ai/download/" target="_blank">https://www.h2o.ai/download/</a>下载<strong>最新稳定版本</strong> H <sub> 2 </sub> O。然后拉开拉链；它包含您开始所需的一切。</p>
<ul>
<li>在您的终端/命令提示符下，使用<kbd>java -jar h2o.jar</kbd>运行<kbd>.jar</kbd>。</li>
<li>将浏览器指向<kbd>http://localhost:54321</kbd>:</li>
<li>图11:H2O流程的用户界面</li>
</ul>
<div><img height="373" width="397" src="img/bbe1f494-edb7-4737-980a-1d485ebda72a.png"/></div>
<p>这显示了h2o最新版本(即2018年1月19日的h2o-3.16.0.4)的可用功能。然而，我不打算在这里解释一切，所以让我们停止探索，因为我相信目前这些关于H2O和火花水的知识就足够了。</p>
<p>数据预处理和特征工程</p>


            

            
        
    






    
        <title>Data pre-processing and feature engineering</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">我已经说过，所有24个VCF文件贡献了820 GB的数据。因此，我决定只使用Y染色体的遗传变异体一个两个使论证更清楚。大小约为160 MB，这并不意味着会带来巨大的计算挑战。你可以从ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/<a href="ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/">下载所有的VCF文件以及面板文件。</a></h1>
                
            
            
                
<p>让我们开始吧。我们从创建Spark应用程序的网关<kbd>SparkSession</kbd>开始:</p>
<p>然后让我们向Spark展示VCF和面板文件的路径:</p>
<pre><strong>val</strong> spark:SparkSession = SparkSession<br/>    .builder()<br/>    .appName("PopStrat")<br/>    .master("local[*]")<br/>    .config("spark.sql.warehouse.dir", "C:/Exp/")<br/>    .getOrCreate()</pre>
<p>我们使用Spark处理面板文件，以访问目标人群数据并识别人群。我们首先创建一组我们想要预测的人口:</p>
<pre><strong>val</strong> genotypeFile = "&lt;path&gt;/ALL.chrY.phase3_integrated_v2a.20130502.genotypes.vcf"<br/><strong>val</strong> panelFile = "&lt;path&gt;/integrated_call_samples_v3.20130502.ALL.panel "</pre>
<p>然后我们需要创建一个样本ID →人群的映射，这样我们就可以过滤掉我们不感兴趣的样本:</p>
<pre><strong>val</strong> populations = Set("FIN", "GBR", "ASW", "CHB", "CLM")</pre>
<p>请注意，面板文件生成所有个人、人口组、种族、超级人口组和性别的样本ID，如下所示:</p>
<pre><strong>def</strong> extract(file: String,<br/>filter: (String, String) =&gt; Boolean): Map[String, String] = {<br/>Source<br/>    .fromFile(file)<br/>    .getLines()<br/>    .map(line =&gt; {<br/><strong>val</strong> tokens = line.split(Array('t', ' ')).toList<br/>tokens(0) -&gt; tokens(1)<br/>}).toMap.filter(tuple =&gt; filter(tuple._1, tuple._2))<br/>}<br/><br/><strong>val</strong> panel: Map[String, String] = extract(<br/>panelFile,<br/>(sampleID: String, pop: String) =&gt; populations.contains(pop))</pre>
<p>图12:样本面板文件的内容</p>
<div><img height="74" width="478" src="img/ed1be322-c2e8-4982-a8f6-50c9546d37ef.png"/></div>
<p>然后加载ADAM基因型并过滤基因型，这样我们就只剩下那些我们感兴趣的群体:</p>
<p>下一项工作是将<kbd>Genotype</kbd>对象转换成我们自己的<kbd>SampleVariant</kbd>对象，以节省内存。然后，<kbd>genotype</kbd>对象被转换成一个<kbd>SampleVariant</kbd>对象，其中只包含我们进一步处理所需的数据:样本ID，它惟一地标识了一个特定的样本；变体ID，其唯一地识别特定的遗传变体；和交替等位基因的计数(仅当样品不同于参考基因组时)。</p>
<pre><strong>val</strong> allGenotypes: RDD[Genotype] = sc.loadGenotypes(genotypeFile).rdd<br/><strong>val</strong> genotypes: RDD[Genotype] = allGenotypes.filter(genotype =&gt; {<br/>    panel.contains(genotype.getSampleId)<br/>    })</pre>
<p>这里给出了准备样本变体的签名；它需要<kbd>sampleID</kbd>、<kbd>variationId</kbd>和<kbd>alternateCount</kbd>:</p>
<p>好吧！让我们从<kbd>genotype</kbd>文件中找到<kbd>variantID</kbd>。一个<kbd>varitantId</kbd>是一个<kbd>String</kbd>类型，由染色体中的名称、开始和结束位置组成:</p>
<pre><strong>case </strong><strong>class</strong> SampleVariant(sampleId: String,<br/>        variantId: Int,<br/>        alternateCount: Int)</pre>
<p>一旦我们有了<kbd>variantID</kbd>，我们应该寻找备用计数。在<kbd>genotype</kbd>文件中，没有等位基因参考的对象大致是基因替代物:</p>
<pre><strong>def</strong> variantId(genotype: Genotype): String = {<br/><strong>    val</strong> name = genotype.getVariant.getContigName<br/><strong>    val</strong> start = genotype.getVariant.getStart<br/><strong>    val</strong> end = genotype.getVariant.getEnd<br/>s"$name:$start:$end"<br/>}</pre>
<p>最后，我们构造了一个简单的变体对象。为此，我们需要实习样品id，因为它们会在VCF文件中重复出现:</p>
<pre><strong>def</strong> alternateCount(genotype: Genotype): Int = {<br/>      genotype.getAlleles.asScala.count(_ != GenotypeAllele.REF)<br/>   }</pre>
<p>太棒了。我们已经能够构建简单的变体。现在，下一个具有挑战性的任务是在我们能够创建<kbd>variantsBySampleId</kbd> RDD之前准备好<kbd>variantsRDD</kbd>:</p>
<pre><strong>def</strong> toVariant(genotype: Genotype): SampleVariant = {<br/><strong>    new</strong> SampleVariant(genotype.getSampleId.intern(),<br/>            variantId(genotype).hashCode(),<br/>            alternateCount(genotype))<br/>        }</pre>
<p>然后，我们必须按样本ID对变量进行分组，以便我们可以逐个样本地处理变量。在此之后，我们可以获得样本总数，用于查找某些样本中缺失的变体。最后，我们必须按变体ID对变体进行分组，并过滤掉一些样本中缺失的变体:</p>
<pre><strong>val</strong> variantsRDD: RDD[SampleVariant] = genotypes.map(toVariant)</pre>
<p>现在，让我们制作一个变量ID →样本计数的映射，其中交替计数大于零。然后我们过滤掉那些不在我们期望的频率范围内的变体。这里的目标只是减少数据集中的维数，以便更容易地训练模型:</p>
<pre><strong>val</strong> variantsBySampleId: RDD[(String, Iterable[SampleVariant])] =<br/>variantsRDD.groupBy(_.sampleId)<br/><br/><strong>val</strong> sampleCount: Long = variantsBySampleId.count()<br/>println("Found " + sampleCount + " samples")<br/><br/><strong>val</strong> variantsByVariantId: RDD[(Int, Iterable[SampleVariant])] =<br/>variantsRDD.groupBy(_.variantId).filter {<br/><strong>        case</strong> (_, sampleVariants) =&gt; sampleVariants.size == sampleCount<br/>    }</pre>
<p>样本(或个体)的总数已经确定，然后根据它们的变体id对它们进行分组，并过滤掉没有样本支持的变体，以简化数据预处理并更好地处理非常大量的变体(总共8440万)。</p>
<pre><strong>val</strong> variantFrequencies: collection.Map[Int, Int] = variantsByVariantId<br/>.map {<br/><strong>    case</strong> (variantId, sampleVariants) =&gt;<br/>        (variantId, sampleVariants.count(_.alternateCount &gt; 0))<br/>        }.collectAsMap()</pre>
<p><em>图13 </em>显示了1000基因组项目中基因型变体集合的概念视图，并展示了从相同数据中提取特征的过程，以训练我们的K均值和MLP模型:</p>
<p>图13:1000基因组项目中基因型变体集合的概念视图</p>
<div><img height="302" width="431" src="img/5006da23-8d31-45d6-a5a6-dfeb88d14337.png"/></div>
<p>指定的范围是任意的，选择它是因为它包括合理数量的变量，但不会太多。更具体地说，对于每个变体，已经计算了交替等位基因的频率，并且排除了具有少于12个交替等位基因的变体，在分析中留下大约300万个变体(对于23个染色体文件):</p>
<p>一旦我们有了<kbd>filteredVariantsBySampleId</kbd>，下一个任务就是对每个样本ID的变量进行排序。每个样本现在应该有相同数量的排序变量:</p>
<pre><strong>val</strong> permittedRange = inclusive(11, 11)<br/><strong>val</strong> filteredVariantsBySampleId: RDD[(String, Iterable[SampleVariant])] =<br/>    variantsBySampleId.map {<br/><strong>        case</strong> (sampleId, sampleVariants) =&gt;<br/><strong>        val</strong> filteredSampleVariants = sampleVariants.filter(<br/>        variant =&gt;<br/>        permittedRange.contains(<br/>        variantFrequencies.getOrElse(variant.variantId, -1)))<br/>    (sampleId, filteredSampleVariants)<br/>    }</pre>
<p>RDD中的所有项目现在应该在相同的订单中具有相同的变体。最后一个任务是使用<kbd>sortedVariantsBySampleId</kbd>构建一个包含地区和备用计数的<kbd>Row</kbd>的RDD:</p>
<pre><strong>val</strong> sortedVariantsBySampleId: RDD[(String, Array[SampleVariant])] =<br/>    filteredVariantsBySampleId.map {<br/><strong>        case</strong> (sampleId, variants) =&gt;<br/>        (sampleId, variants.toArray.sortBy(_.variantId))<br/>        }<br/>    println(s"Sorted by Sample ID RDD: " + sortedVariantsBySampleId.first())</pre>
<p>因此，我们可以只使用第一个来构造训练数据帧的报头:</p>
<pre><strong>val</strong> rowRDD: RDD[Row] = sortedVariantsBySampleId.map {<br/><strong>    case</strong> (sampleId, sortedVariants) =&gt;<br/><strong>        val</strong> region: Array[String] = Array(panel.getOrElse(sampleId, "Unknown"))<br/><strong>        val</strong> alternateCounts: Array[Int] = sortedVariants.map(_.alternateCount)<br/>        Row.fromSeq(region ++ alternateCounts)<br/>        }</pre>
<p>干得好！到目前为止，我们有我们的RDD和头球<kbd>StructType</kbd>。所以现在，我们可以通过最小的调整/转换来玩H2O和Spark deep/机器学习算法。下图显示了这个端到端项目的整体流程:</p>
<pre><strong>val</strong> header = StructType(<br/>        Seq(StructField("Region", StringType)) ++<br/>        sortedVariantsBySampleId<br/>            .first()<br/>            ._2<br/>            .map(variant =&gt; {<br/>                StructField(variant.variantId.toString, IntegerType)<br/>        }))</pre>
<p>图14:整体方法的管道</p>
<div><img height="276" width="682" src="img/47f8c70a-7a52-495c-bbc2-afbab44c1ab1.png"/></div>
<p>模型训练和超参数调整</p>


            

            
        
    






    
        <title>Model training and hyperparameter tuning</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">一旦我们有了<kbd>rowRDD</kbd>和头，下一个任务就是使用头和<kbd>rowRDD</kbd>从变量中构造我们的模式数据帧的行:</h1>
                
            
            
                
<p>图15:包含特征和标签(即区域)列的训练数据集的快照</p>
<pre><strong>val</strong> sqlContext = spark.sqlContext<br/><strong>val</strong> schemaDF = sqlContext.createDataFrame(rowRDD, header)<br/>schemaDF.printSchema()<br/>schemaDF.show(10)<br/>&gt;&gt;&gt;</pre>
<div><img height="161" width="380" src="img/9e459b56-504f-4465-a5c5-ebd78266d610.png"/></div>
<p>在前面的数据帧中，只显示了几列，包括标签，以便适合页面。</p>
<p>基于Spark的K-means聚类算法</p>


            

            
        
    






    
        <title>Spark-based K-means for population-scale clustering</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">在前面的章节中，我们已经看到了K-means是如何工作的。所以我们可以直接进入实现。由于训练将是无人监督的，我们需要删除标签列(即<kbd>Region</kbd>):</h1>
                
            
            
                
<p>图16:没有标签(即区域)的K-means的训练数据集的快照</p>
<pre><strong>val</strong> sqlContext = sparkSession.sqlContext<br/><strong>val</strong> schemaDF = sqlContext.createDataFrame(rowRDD, header).drop("Region")<br/>schemaDF.printSchema()<br/>schemaDF.show(10)<br/>&gt;&gt;&gt;</pre>
<div><img src="img/34b50280-9728-4f15-8048-2b5e56d2f801.png"/></div>
<p>现在，我们已经在<a href="4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml" target="_blank">第1章</a>、<em>分析保险严重索赔</em>和<a href="4e196881-40c8-4eb9-b2b3-e332a49adc1a.xhtml" target="_blank">第2章</a>、<em>分析和预测电信流失</em>中看到，Spark希望有两列(即特征和标签)用于监督训练，而对于非监督训练，它希望只有一列包含特征。由于我们删除了标签列，现在我们需要将整个变量列合并成一个单独的<kbd>features</kbd>列。为此，我们将再次使用<kbd>VectorAssembler()</kbd>变压器。首先，让我们选择要嵌入向量空间的列:</p>
<p>然后我们实例化<kbd>VectorAssembler()</kbd>转换器，指定输入列和输出列:</p>
<pre><strong>val</strong> featureCols = schemaDF.columns</pre>
<p>现在让我们看看它是什么样子的:</p>
<pre><strong>val</strong> assembler = <br/><strong>new</strong> VectorAssembler()<br/>    .setInputCols(featureCols)<br/>    .setOutputCol("features")<br/><strong>val</strong> assembleDF = assembler.transform(schemaDF).select("features")</pre>
<p>图17:K均值的特征向量快照</p>
<pre>assembleDF.show()<br/>&gt;&gt;&gt;</pre>
<div><img height="282" width="120" src="img/a26cbeac-f5c1-423c-984d-1b5417fda8b5.png"/></div>
<p>由于我们的数据集非常高维，我们可以使用一些维数算法，如PCA。因此，让我们通过实例化一个<kbd>PCA()</kbd>转换器来实现，如下所示:</p>
<p>然后我们对组装的数据帧(即组装的)和前50个主成分进行变换。不过你可以调整这个数字。最后，为了避免歧义，我们将<kbd>pcaFeatures</kbd>列重命名为<kbd>features</kbd>:</p>
<pre><strong>val</strong> pca = <br/><strong>new</strong> PCA()<br/>    .setInputCol("features")<br/>    .setOutputCol("pcaFeatures")<br/>    .setK(50)<br/>    .fit(assembleDF)</pre>
<p>图18:作为最重要特征的前50个主要成分的快照</p>
<pre><strong>val</strong> pcaDF = pca.transform(assembleDF)<br/>            .select("pcaFeatures")<br/>            .withColumnRenamed("pcaFeatures", "features")<br/>pcaDF.show()<br/>&gt;&gt;&gt;</pre>
<div><img height="331" width="145" src="img/6b822396-5778-4322-9013-44a867bc6a2f.png"/></div>
<p>太棒了。一切都很顺利。最后，我们准备训练K-means算法:</p>
<p>因此，让我们通过计算<strong>组内误差平方和</strong> ( <strong> WSSSE </strong>)来评估集群:</p>
<pre><strong>val</strong> kmeans = <br/><strong>new</strong> KMeans().setK(5).setSeed(12345L)<br/><strong>val</strong> model = kmeans.fit(pcaDF)</pre>
<p>确定最佳聚类的数量</p>
<pre><strong>val</strong> WSSSE = model.computeCost(pcaDF)<br/>println("Within-Cluster Sum of Squares for k = 5 is" + WSSSE)<br/>&gt;&gt;&gt;</pre>


            

            
        
    






    
        <title>Determining the number of optimal clusters</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">聚类算法(如K-means)的优点在于，它们对具有无限数量特征的数据进行聚类。当您拥有原始数据并希望了解数据中的模式时，它们是非常有用的工具。然而，在做实验之前决定簇的数量可能不成功，并且有时可能导致过拟合或欠拟合问题。</h1>
                
            
            
                
<p>另一方面，所有三种算法(即K-means、平分K-means和高斯混合)的一个共同点是，必须预先确定聚类的数量，并将其作为参数提供给算法。因此，非正式地，确定集群的数量是要解决的单独的优化问题。</p>
<p>现在我们将使用基于肘方法的启发式方法。我们从K = 2个聚类开始，然后通过增加K并观察成本函数WCSS的值，对同一数据集运行K均值算法:</p>
<p>在某一点上，可以观察到成本函数的大幅下降，但随后随着<kbd>k</kbd>值的增加，改善变得微不足道。正如聚类分析文献中所建议的，我们可以挑选WCSS最后一次大跌后的<kbd>k</kbd>作为最优。现在，让我们看看2到20个不同数量的分类的WCSS值，例如:</p>
<pre><strong>val</strong> iterations = 20<br/><strong>for</strong> (i &lt;- 2 to iterations) {<br/><strong>        val</strong> kmeans = <strong>new</strong> KMeans().setK(i).setSeed(12345L)<br/><strong>        val</strong> model = kmeans.fit(pcaDF)<br/><strong>        val</strong> WSSSE = model.computeCost(pcaDF)<br/>        println("Within-Cluster Sum of Squares for k = " + i + " is " +<br/>                WSSSE)<br/>    }</pre>
<p>现在让我们讨论如何利用肘方法来确定集群的数量。如下图所示，我们计算了成本函数WCSS，作为应用于所选人群中Y染色体遗传变异的K-means算法的聚类数的函数。</p>
<pre>Within-Cluster Sum of Squares for k = 2 is 453.161838161838<br/>Within-Cluster Sum of Squares for k = 3 is 438.2392344497606<br/>Within-Cluster Sum of Squares for k = 4 is 390.2278787878787<br/>Within-Cluster Sum of Squares for k = 5 is 397.72112098427874<br/>Within-Cluster Sum of Squares for k = 6 is 367.8890909090908<br/>Within-Cluster Sum of Squares for k = 7 is 362.3360347662672<br/>Within-Cluster Sum of Squares for k = 8 is 347.49306362861336<br/>Within-Cluster Sum of Squares for k = 9 is 327.5002901103624<br/>Within-Cluster Sum of Squares for k = 10 is 327.29376873556436<br/>Within-Cluster Sum of Squares for k = 11 is 315.2954156954155<br/>Within-Cluster Sum of Squares for k = 12 is 320.2478696814693<br/>Within-Cluster Sum of Squares for k = 13 is 308.7674242424241<br/>Within-Cluster Sum of Squares for k = 14 is 314.64784054938576<br/>Within-Cluster Sum of Squares for k = 15 is 297.38523698523704<br/>Within-Cluster Sum of Squares for k = 16 is 294.26114718614707<br/>Within-Cluster Sum of Squares for k = 17 is 284.34890572390555<br/>Within-Cluster Sum of Squares for k = 18 is 280.35662525879917<br/>Within-Cluster Sum of Squares for k = 19 is 272.765762015762<br/>Within-Cluster Sum of Squares for k = 20 is 272.05702362771336</pre>
<p>可以观察到，当<kbd>k = 9</kbd>时<strong>出现稍微大的下降</strong>(虽然不是急剧下降)。因此，我们选择簇的数量为10，如图<em>图10 </em>所示:</p>
<p>图19:作为WCSS函数的集群数量</p>
<div><img height="330" width="440" src="img/4260d5c6-46f4-4302-9948-ab8cbd4190b8.png"/></div>
<p>使用H2O进行种族预测</p>


            

            
        
    






    
        <title>Using H2O for ethnicity prediction</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">到目前为止，我们已经看到了如何对基因变异进行聚类。我们还使用了肘方法，并找到了最佳的数字<kbd>k</kbd>，暂定数字集群。现在我们应该探索我们在开始时计划的另一项任务——即种族预测。</h1>
                
            
            
                
<p>在前面的K-means部分，我们准备了一个名为<kbd>schemaDF</kbd>的Spark数据帧。那个不能和H2O一起用。然而，额外的转换是必要的。我们使用<kbd>asH2OFrame()</kbd>方法将火花数据帧转换成H2O帧:</p>
<p>现在，在使用H2O时，您应该记住的一件重要事情是，如果您不将标签列转换为分类列，它会将分类任务视为回归。为了摆脱这一点，我们可以使用来自H2O的<kbd>toCategoricalVec()</kbd>方法。由于H2O框架具有弹性，我们可以进一步更新同一框架:</p>
<pre><strong>val</strong> dataFrame = h2oContext.asH2OFrame(schemaDF)</pre>
<p>现在，我们的H2O框架已经准备好训练一个基于H2O的DL模型(这是DNN，或者更具体地说，一个深度MLP)。然而，在我们开始训练之前，让我们使用H2O内置的<kbd>FrameSplitter()</kbd>方法将数据帧随机分成60%的训练数据、20%的测试数据和20%的验证数据:</p>
<pre>dataFrame.replace(dataFrame.find("Region"),<br/>dataFrame.vec("Region").toCategoricalVec()).remove()<br/>dataFrame.update()</pre>
<p>太棒了。我们的训练、测试和验证集已经准备好了，所以让我们为DL模型设置参数:</p>
<pre><strong>val</strong> frameSplitter = <strong>new</strong> FrameSplitter(<br/>        dataFrame, Array(.8, .1), Array("training", "test", "validation")<br/>        .map(Key.make[Frame]),<strong>null</strong>)<br/><br/>water.H2O.submitTask(frameSplitter)<br/><strong>val</strong> splits = frameSplitter.getResult<br/><strong>val</strong> training = splits(0)<br/><strong>val</strong> test = splits(1)<br/><strong>val</strong> validation = splits(2)</pre>
<p>在前面的设置中，我们指定了一个MLP，它有三个隐藏层，分别有128、256和512个神经元。所以总共有五层，包括输入层和输出层。训练将迭代到200个纪元。由于我们在隐藏层中使用了太多的神经元，我们应该使用dropout来避免过度拟合。为了避免实现更好的正则化，我们使用了l1正则化。</p>
<pre>// Set the parameters for our deep learning model.<br/><strong>val</strong> deepLearningParameters = <strong>new</strong> DeepLearningParameters()<br/>        deepLearningParameters._train = training<br/>        deepLearningParameters._valid = validation<br/>        deepLearningParameters._response_column = "Region"<br/>        deepLearningParameters._epochs = 200<br/>        deepLearningParameters._l1 = 0.01<br/>        deepLearningParameters._seed = 1234567<br/>        deepLearningParameters._activation = Activation.RectifierWithDropout<br/>        deepLearningParameters._hidden = Array[Int](128, 256, 512)</pre>
<p>前面的设置还表明，我们将使用训练集来训练模型，此外，验证集将用于验证训练。最后，回复栏是<kbd>Region</kbd>。另一方面，种子用于确保再现性。</p>
<p>一切就绪！现在我们来训练DL模型:</p>
<p>根据您的硬件配置，这可能需要一段时间。因此，也许是时候休息一下，喝点咖啡了！一旦我们有了训练好的模型，我们就可以看到训练错误:</p>
<pre><strong>val</strong> deepLearning = <strong>new</strong> DeepLearning(deepLearningParameters)<br/><strong>val</strong> deepLearningTrained = deepLearning.trainModel<br/><strong>val</strong> trainedModel = deepLearningTrained.get</pre>
<p>不幸的是，训练并没有那么好！然而，我们应该尝试不同的超参数组合。虽然结果显示误差很大，但是让我们不要太担心，评估模型，计算一些模型度量，并评估模型质量:</p>
<pre><strong>val</strong> error = trainedModel.classification_error()<br/>println("Training Error: " + error)<br/>&gt;&gt;&gt;<br/>Training Error: 0.5238095238095238</pre>
<p>没那么高的准确率！但是，您应该尝试使用其他VCF文件，并通过调整超参数。例如，在减少隐藏层中的神经元并使用l2正则化和100个时期后，我有大约20%的改进:</p>
<pre><strong>val</strong> trainMetrics = ModelMetricsSupport.modelMetrics[ModelMetricsMultinomial](trainedModel, test)<br/><strong>val</strong> met = trainMetrics.cm()<br/><br/>println("Accuracy: "+ met.accuracy())<br/>println("MSE: "+ trainMetrics.mse)<br/>println("RMSE: "+ trainMetrics.rmse)<br/>println("R2: " + trainMetrics.r2)<br/>&gt;&gt;&gt;<br/>Accuracy: 0.42105263157894735<br/>MSE: 0.49369297490740655<br/>RMSE: 0.7026328877211816<br/>R2: 0.6091597281983032</pre>
<p>另一个改进线索在这里。除了这些超参数之外，使用基于H2O的DL算法的另一个优点是我们可以采用相对变量/特征重要性。在前面的章节中，我们已经看到，在Spark中使用随机森林算法时，也可以计算变量重要性。</p>
<pre><strong>val</strong> deepLearningParameters = <strong>new</strong> DeepLearningParameters()<br/>        deepLearningParameters._train = training<br/>        deepLearningParameters._valid = validation<br/>        deepLearningParameters._response_column = "Region"<br/>        deepLearningParameters._epochs = 100<br/>        deepLearningParameters._l2 = 0.01<br/>        deepLearningParameters._seed = 1234567<br/>        deepLearningParameters._activation = Activation.RectifierWithDropout<br/>        deepLearningParameters._hidden = Array[Int](32, 64, 128)<br/>&gt;&gt;&gt;<br/>Training Error: 0.47619047619047616<br/>Accuracy: 0.5263157894736843<br/>MSE: 0.39112548936806274<br/>RMSE: 0.6254002633258662<br/>R2: 0.690358987583617</pre>
<p>因此，我们的想法是，如果您的模型表现不佳，那么丢弃不太重要的特性并重新进行训练是值得的。现在，有可能在监督训练期间发现特征重要性。我观察到了这个特性的重要性:</p>
<p>图20:使用H2O的相对特征重要性</p>
<div><img height="268" width="466" src="img/ba65fb17-4d3b-43ad-ae89-0b2780157865.png"/></div>
<p>现在的问题是，你为什么不扔掉它们，再次尝试训练，观察准确度是否有所提高？好吧，我把它留给读者。</p>
<p>使用随机森林进行种族预测</p>


            

            
        
    






    
        <title>Using random forest for ethnicity prediction</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">在上一节中，我们已经了解了如何使用H2O进行种族预测。然而，我们无法实现更好的预测准确性。因此，H2O还不够成熟，无法计算所有必要的性能指标。</h1>
                
            
            
                
<p>那么我们为什么不试试随机森林或GBTs等基于Spark的树集成技术呢？因为我们已经看到，在大多数情况下，RF显示出更好的预测准确性，所以让我们尝试一下。</p>
<p>在K-means部分，我们已经准备了名为<kbd>schemaDF</kbd>的Spark数据帧。因此，我们可以简单地将变量转换成我们之前描述的特征向量。然而，为此，我们需要排除标签列。我们可以使用如下的<kbd>drop()</kbd>方法:</p>
<p>此时，您可以使用PCA或任何其他特征选择器算法进一步降低维度并提取最主要的成分。然而，我将把它留给你。因为Spark希望标签列是数字，所以我们必须将种族名称转换成数字。为此，我们可以使用<kbd>StringIndexer()</kbd>。很简单:</p>
<pre><strong>val</strong> featureCols = schemaDF.columns.drop(1)<br/><strong>val</strong> assembler = <br/><strong>new</strong> VectorAssembler()<br/>    .setInputCols(featureCols)<br/>    .setOutputCol("features")<br/><strong>val</strong> assembleDF = assembler.transform(schemaDF).select("features", "Region")<br/>assembleDF.show()</pre>
<p>然后我们随机分割数据集进行训练和测试。在我们的例子中，让我们将75%用于培训，其余的用于测试:</p>
<pre><strong>val</strong> indexer = <br/><strong>new</strong> StringIndexer()<br/>    .setInputCol("Region")<br/>    .setOutputCol("label")<br/><br/><strong>val</strong> indexedDF =  indexer.fit(assembleDF)<br/>                .transform(assembleDF)<br/>                .select("features", "label") </pre>
<p>由于这是一个小数据集，考虑到这一事实，我们可以<kbd>cache</kbd>训练集和测试集以获得更快的访问:</p>
<pre><strong>val</strong> seed = 12345L<br/><strong>val</strong> splits = indexedDF.randomSplit(Array(0.75, 0.25), seed)<br/><strong>val</strong> (trainDF, testDF) = (splits(0), splits(1))</pre>
<p>现在让我们创建一个<kbd>paramGrid</kbd>,通过决策树的<kbd>maxDepth</kbd>参数搜索最佳模型:</p>
<pre>trainDF.cache<br/>testDF.cache<br/><strong>val</strong> rf = <strong>new</strong> RandomForestClassifier()<br/>    .setLabelCol("label")<br/>    .setFeaturesCol("features")<br/>    .setSeed(1234567L)</pre>
<p>然后，我们为优化和稳定的模型设置了10重交叉验证。这将减少过度拟合的机会:</p>
<pre><strong>val</strong> paramGrid =<br/><strong>new</strong> ParamGridBuilder()<br/>    .addGrid(rf.maxDepth, 3 :: 5 :: 15 :: 20 :: 25 :: 30 :: Nil)<br/>    .addGrid(rf.featureSubsetStrategy, "auto" :: "all" :: Nil)<br/>    .addGrid(rf.impurity, "gini" :: "entropy" :: Nil)<br/>    .addGrid(rf.maxBins, 3 :: 5 :: 10 :: 15 :: 25 :: 35 :: 45 :: Nil)<br/>    .addGrid(rf.numTrees, 5 :: 10 :: 15 :: 20 :: 30 :: Nil)<br/>    .build()<br/><br/><strong>val</strong> evaluator = <strong>new</strong> MulticlassClassificationEvaluator()<br/>    .setLabelCol("label")<br/>    .setPredictionCol("prediction")</pre>
<p>好了，现在我们准备好训练了。因此，让我们使用最佳超参数设置来训练随机森林模型:</p>
<pre><strong>val</strong> numFolds = 10<br/><strong>val</strong> crossval = <br/><strong>new</strong> CrossValidator()<br/>    .setEstimator(rf)<br/>    .setEvaluator(evaluator)<br/>    .setEstimatorParamMaps(paramGrid)<br/>    .setNumFolds(numFolds)</pre>
<p>既然我们有了交叉验证和最佳模型，为什么不使用测试集来评估模型呢？为什么不呢？首先，我们计算每个实例的预测数据帧。然后我们使用<kbd>MulticlassClassificationEvaluator()</kbd>来评估性能，因为这是一个多类分类问题。</p>
<pre><strong>val</strong> cvModel = crossval.fit(trainDF)</pre>
<p>此外，我们还计算绩效指标，如<kbd>accuracy</kbd>、<kbd>precision</kbd>、<kbd>recall</kbd>和<kbd>f1</kbd>测量。注意，使用RF分类器，我们可以得到<kbd>weightedPrecision</kbd>和<kbd>weightedRecall</kbd>:</p>
<p>图21:原始预测概率、真实标签和使用随机森林的预测标签</p>
<pre><strong>val</strong> predictions = cvModel.transform(testDF)<br/>predictions.show(10)<br/>&gt;&gt;&gt;</pre>
<div><img height="203" width="489" src="img/40c0900b-2076-47ed-beb4-0bdc16c67382.png"/></div>
<p>现在让我们计算测试数据的分类<kbd>accuracy</kbd>、<kbd>precision</kbd>、<kbd>recall</kbd>、<kbd>f1</kbd>测量和误差:</p>
<pre><strong>val</strong> metric = <br/><strong>new</strong> MulticlassClassificationEvaluator()<br/>    .setLabelCol("label")<br/>    .setPredictionCol("prediction")<br/><br/><strong>val</strong> evaluator1 = metric.setMetricName("accuracy")<br/><strong>val</strong> evaluator2 = metric.setMetricName("weightedPrecision")<br/><strong>val</strong> evaluator3 = metric.setMetricName("weightedRecall")<br/><strong>val</strong> evaluator4 = metric.setMetricName("f1")</pre>
<p>最后，我们打印性能指标:</p>
<pre><strong>val</strong> accuracy = evaluator1.evaluate(predictions)<br/><strong>val</strong> precision = evaluator2.evaluate(predictions)<br/><strong>val</strong> recall = evaluator3.evaluate(predictions)<br/><strong>val</strong> f1 = evaluator4.evaluate(predictions)</pre>
<p>是的，事实证明它是一个更好的表演者。这有点出乎意料，因为我们希望从DL模型中获得更好的预测准确性，但我们没有。正如我已经说过的，我们仍然可以尝试H2O的其他参数。无论如何，我们现在可以看到使用随机森林大约有25%的改进。不过，大概，还是可以改进的。</p>
<pre>println("Accuracy = " + accuracy);<br/>println("Precision = " + precision)<br/>println("Recall = " + recall)<br/>println("F1 = " + f1)<br/>println(s"Test Error = ${1 - accuracy}")<br/>&gt;&gt;&gt;<br/>Accuracy = 0.7196470196470195<br/>Precision = 0.7196470196470195<br/>Recall = 0.7196470196470195<br/>F1 = 0.7196470196470195<br/>Test Error = 0.28035298035298046</pre>
<p>摘要</p>


            

            
        
    






    
        <title>Summary</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">在本章中，我们看到了如何与Spark、H2O和ADAM等一些大数据工具进行互操作，以处理大规模基因组数据集。我们将基于Spark的K-means算法应用于来自1000个基因组项目分析的遗传变异数据，目的是在人群规模上聚类基因型变异。</h1>
                
            
            
                
<p>然后，我们应用基于H2O的DL算法和基于Spark的随机森林模型来预测地理种族。此外，我们还学习了如何为DL安装和配置H2O。这些知识将在后面的章节中用到。最后，也是最重要的，我们学习了如何使用H2O来计算变量重要性，以便在训练集中选择最重要的特征。</p>
<p>在下一章中，我们将看到如何有效地使用<strong>潜在狄利克雷分配</strong> ( <strong> LDA </strong>)算法在数据中寻找有用的模式。我们将比较其他主题建模算法和LDA的可伸缩性。此外，我们将利用<strong>自然语言处理</strong> ( <strong> NLP </strong>)库，如斯坦福NLP。</p>
<p>In the next chapter, we will see how effectively we can use the <strong>Latent Dirichlet Allocation</strong> (<strong>LDA</strong>) algorithm for finding useful patterns in data. We will compare other topic modeling algorithms and the scalability power of LDA. In addition, we will utilize <strong>Natural Language Processing</strong> (<strong>NLP</strong>) libraries such as Stanford NLP.</p>


            

            
        
    


</body></html>